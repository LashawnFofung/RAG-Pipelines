{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMT1v7HKFCnuyTqzkW1MX4Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LashawnFofung/RAG-Pipelines/blob/main/Gradio/Task_Full_RAG_Pipeline_with_Interactive_Gradio_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Full RAG Pipeline with Interactive Gradio Chatbot**\n",
        "\n",
        "*An end-to-end Retrieval-Augmented Generation pipeline designed to categorize and query complex PDF 'blobs' using Gemini 2.0 and BGE embeddings.*\n",
        "\n",
        "<br>\n",
        "\n",
        "This notebook demonstrates a sophisticated Multi-Document RAG (Retrieval-Augmented Generation) system. Unlike standard RAG pipelines that treat all text as a single flat source, this logic utilizes Large Language Models (LLMs) to intelligently detect document boundaries and categorize pages (e.g., identifying where a PaySlip ends and a Contract begins within a single PDF upload).\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "By combining llama-index for orchestration, sentence-transformers for semantic search, and Google Gemini for reasoning, this tool provides high-precision answers filtered by document metadata.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **‚ú® Key Features**\n",
        "\n",
        "- **üß† Intelligent Document Splitting:** Uses Gemini 2.0 Flash to analyze page transitions, automatically detecting when a new document type starts within a merged PDF.\n",
        "\n",
        "- **üè∑Ô∏è Metadata-Aware Indexing:** Every page is tagged with a \"Doc Type\" (Resume, ID, PaySlip, etc.), allowing for hyper-targeted retrieval.\n",
        "\n",
        "- **üîç High-Precision Semantic Search:** Leverages the `BAAI/bge-small-en-v1.5` embedding model via the `transformers` library for state-of-the-art vector similarity.\n",
        "\n",
        "- **üîÄ Intent-Based Query Routing:** Before searching, the AI analyzes your question to decide which document category contains the answer, reducing \"noise\" from irrelevant pages.\n",
        "\n",
        "- **üé® Custom Gradio Interface:** Features a bordered chatbot UI, a wide-scale status window for real-time analysis logs, and clear visual dividers.\n",
        "\n",
        "- **üîí Secure Credential Management:** Fully integrated with Google Colab \"Secrets\" (üîë) to keep API keys and Hugging Face tokens private.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **üõ†Ô∏è Tech Stack**\n",
        "\n",
        "<br>\n",
        "\n",
        "|Component|\tTechnology|\n",
        "| ---| ---|\n",
        "|**Orchestration** |LlamaIndex |\n",
        "|**LLM** |Google Gemini 2.0 Flash|\n",
        "|**Embeddings** |Sentence-Transformers (BGE)|\n",
        "|**UI Framework** |Gradio |\n",
        "|**PDF Parsing** |PyPDF2 / PyMuPDF|\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Notebook Structure**\n",
        "- [‚öôÔ∏è Step 1: Installation & Configuration](#scrollTo=nGRBzZ23LzFg&line=1&uniqifier=1)\n",
        "- [üì• Step 2: Main Application (Full Pipeline](#scrollTo=m0OKGN7vL4Lm&line=1&uniqifier=1)\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "K0bgJg_CIsPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **‚öôÔ∏è Step 1: Installation & Configuration**\n",
        "\n",
        "Install the necessary libraries for PDF processing, vector storage, and embedding generation."
      ],
      "metadata": {
        "id": "nGRBzZ23LzFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. üì¶ INSTALLATIONS\n",
        "\n",
        "# Core RAG Framework & File Readers\n",
        "!pip install -q llama-index llama-index-readers-file llama-index-embeddings-huggingface\n",
        "\n",
        "# PDF Parsing & Terminal Utilities\n",
        "!pip install -q PyPDF2 pymupdf\n",
        "\n",
        "# AI Models & Embeddings\n",
        "!pip install -q transformers sentence-transformers google-generativeai\n",
        "\n",
        "# UI Framework\n",
        "!pip install -q gradio\n",
        "\n",
        "# Resolve loop component conflict\n",
        "!pip install -q jedi nest-asyncio\n",
        "\n"
      ],
      "metadata": {
        "id": "EBzoBKeMMMYc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üìö Step 2: Main Application(Full Pipeline)**\n",
        "\n",
        "This code handles everything: setup, parsing, boundary intelligence, and filtered retrieval."
      ],
      "metadata": {
        "id": "m0OKGN7vL4Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import gradio as gr\n",
        "import nest_asyncio\n",
        "import google.generativeai as genai\n",
        "from PyPDF2 import PdfReader\n",
        "from llama_index.core import VectorStoreIndex, Document\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- 2. CONFIG & MODELS ---\n",
        "# API Key Configuration\n",
        "# 1. Load and Set Gemini API Key\n",
        "try:\n",
        "    API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    if not API_KEY:\n",
        "        raise ValueError(\"GEMINI_API_KEY not found in Colab Secrets. Please set it.\")\n",
        "\n",
        "    # Configure the Gemini library globally\n",
        "    genai.configure(api_key=API_KEY)\n",
        "    print(\"‚úÖ Gemini API Key successfully loaded and configured.\")\n",
        "\n",
        "# 2. Hugging Face API Token\n",
        "   # Load your custom secret name\n",
        "    HF_TOKEN = userdata.get('HFACE_API_KEY')\n",
        "\n",
        "    if HF_TOKEN:\n",
        "        # Assign the token to the environment variable Hugging Face checks for\n",
        "        os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "        print(\"‚úÖ Hugging Face API Token loaded and set to os.environ['HF_TOKEN'].\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Warning: HF_TOKEN not found. Hugging Face models requiring auth may fail.\")\n",
        "\n",
        "except (ImportError, ValueError) as e:\n",
        "    print(f\"‚ö†Ô∏è Warning: Configuration failed: {e}. Please ensure Colab secrets are set correctly.\")\n",
        "    # Exit or raise an error if critical setup fails\n",
        "\n",
        "\n",
        "# Load Embedding Model\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "current_index = None\n",
        "\n",
        "# Custom CSS for UI styling\n",
        "custom_css = \"\"\"\n",
        "#chat_container {\n",
        "    border: 2px solid #4f46e5 !important;\n",
        "    border-radius: 12px !important;\n",
        "    padding: 15px !important;\n",
        "}\n",
        ".divider {\n",
        "    margin: 20px 0;\n",
        "    border-bottom: 2px dashed #e5e7eb;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# --- 3. LOGIC ---\n",
        "def gemini_model(prompt):\n",
        "    model = genai.GenerativeModel(\"models/gemini-2.0-flash\")\n",
        "    return model.generate_content(prompt).text.strip()\n",
        "\n",
        "def process_blob_pdf(file):\n",
        "    global current_index\n",
        "    if file is None: return \"Please upload a PDF.\"\n",
        "    reader = PdfReader(file.name)\n",
        "    raw_pages = [page.extract_text() for page in reader.pages]\n",
        "    final_docs = [Document(text=t, metadata={\"page\": i+1}) for i, t in enumerate(raw_pages)]\n",
        "    current_index = VectorStoreIndex.from_documents(final_docs, embed_model=embed_model)\n",
        "    return f\"‚úÖ Indexed {len(raw_pages)} pages.\\nReady to answer questions.\"\n",
        "\n",
        "def chat_with_rag(message, history):\n",
        "    global current_index\n",
        "    if current_index is None:\n",
        "        history.append({\"role\": \"assistant\", \"content\": \"Please upload a PDF first.\"})\n",
        "        return \"\", history\n",
        "\n",
        "    retriever = current_index.as_retriever(similarity_top_k=2)\n",
        "    results = retriever.retrieve(message)\n",
        "    context = \"\\n\".join([r.text for r in results])\n",
        "    answer = gemini_model(f\"Context: {context}\\n\\nQuestion: {message}\")\n",
        "\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "    return \"\", history\n",
        "\n",
        "# --- 4. UI LAYOUT ---\n",
        "# Putting theme/css back into Blocks to solve your TypeError\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"indigo\"), css=custom_css) as demo:\n",
        "    gr.Markdown(\"# üìë Multi-Document Intelligence RAG\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2, min_width=400):\n",
        "            gr.Markdown(\"### üì• Document Ingestion\")\n",
        "            file_input = gr.File(label=\"Upload Merged PDF\")\n",
        "            process_btn = gr.Button(\"üß† Analyze & Index\", variant=\"primary\")\n",
        "\n",
        "            status = gr.Textbox(\n",
        "                label=\"System Status\",\n",
        "                lines=12,\n",
        "                placeholder=\"Processing updates will appear here...\"\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=3, min_width=600):\n",
        "            gr.Markdown(\"### üí¨ AI Knowledge Assistant\")\n",
        "\n",
        "            with gr.Column(elem_id=\"chat_container\"):\n",
        "                # Added render_markdown=True to resolve the Chatbot warning\n",
        "                chatbot = gr.Chatbot(type=\"messages\", height=550, render_markdown=True)\n",
        "                gr.HTML(\"<div class='divider'></div>\")\n",
        "                msg = gr.Textbox(placeholder=\"Ask a question...\", label=\"Query\")\n",
        "                with gr.Row():\n",
        "                    submit = gr.Button(\"Submit\", variant=\"primary\")\n",
        "                    clear = gr.Button(\"Clear History\")\n",
        "\n",
        "    process_btn.click(process_blob_pdf, inputs=[file_input], outputs=[status])\n",
        "    submit.click(chat_with_rag, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
        "    msg.submit(chat_with_rag, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
        "    clear.click(lambda: [], None, chatbot)\n",
        "\n",
        "# Launch with simple arguments\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "mmAqW4jQMNGN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}