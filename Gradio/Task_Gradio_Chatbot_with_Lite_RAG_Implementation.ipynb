{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOXJb3I3ZGQlJo18ZDFa/JN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LashawnFofung/RAG-Pipelines/blob/main/Gradio/Task_Gradio_Chatbot_with_Lite_RAG_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradio Chatbot with Lite RAG Implementation**\n",
        "<br>\n",
        "\n",
        "### **üìë Multi-PDF Research Buddy**\n",
        "\n",
        "*Streamlined Document Intelligence with Gradio & PyMuPDF*\n",
        "\n",
        "<br>\n",
        "\n",
        "This notebook provides a lightweight, local environment to upload, index, and query multiple PDF documents simultaneously. It uses a Gradio-powered UI to create a seamless research workspace directly inside Google Colab.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **‚öôÔ∏è Key Features**\n",
        "\n",
        "- **Multi-File Support:** Upload several research papers or reports at once.\n",
        "\n",
        "- **Instant Indexing:** Uses `PyMuPDF` (fitz) for high-speed text extraction.\n",
        "\n",
        "- **Intelligent Modes:** * Standard Q&A: Keyword-based sentence retrieval (RAG foundation).\n",
        "\n",
        "  - **Summary:** Quick overview of document contents.\n",
        "\n",
        "  - **Key Takeaways**: Automated theme identification through text analysis.\n",
        "\n",
        "- **Minimalist UI:** A clean, slate-themed monochrome interface for focused work.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **üß† How My Retrieval System Works**\n",
        "\n",
        "I've designed this notebook to use what is known as a \"Lite\" or \"Naive\" RAG (Retrieval-Augmented Generation) architecture. Instead of just relying on a model's pre-existing knowledge, I am forcing the system to look at the specific documents you've uploaded.\n",
        "\n",
        "<br>\n",
        "\n",
        "Here is how I've broken down the logic:\n",
        "\n",
        "- **Retrieval:** When you ask a question, I don't just guess. I use `re.split` to break your PDFs into individual sentences and run a keyword search `(search_query in s.lower())` to find the exact lines where your topic is mentioned.\n",
        "\n",
        "- **Augmentation:** Once I find those relevant sentences, I \"augment\" the system's knowledge by pulling them into a dedicated `context` variable. This acts as the short-term memory for your specific query.\n",
        "\n",
        "- **Generation:** In this current version, I am \"generating\" a response by returning those exact text snippets to you. This ensures 100% accuracy to the source text without any \"hallucinations.\"\n",
        "\n",
        "- **[!TIP] Taking it to the next level:** To turn this into a **\"Full RAG\"** pipeline, I would simply take that `context` variable and feed it into a generative LLM (like Gemini). Instead of showing you raw snippets, the AI would read those snippets and write a natural, cohesive answer for you.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### **How to Use**\n",
        "\n",
        "- **Installation:** Run the first cell to install the necessary libraries (`PyMuPDF`, Gradio, etc.).\n",
        "\n",
        "- **Launch:** Run the Main Application cell. Click the **public URL** or use the **inline frame** to view the app.\n",
        "\n",
        "- **Analyze:** Upload your PDFs, click **\"Process All Documents\"**, and start chatting with your data!"
      ],
      "metadata": {
        "id": "6TS4lKCyUXcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Setup & Installation**"
      ],
      "metadata": {
        "id": "kSMsPBvhWKwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install both UI and Document Intelligence libraries\n",
        "!pip install -q pymupdf gradio llama-index llama-index-readers-file google-generativeai jedi"
      ],
      "metadata": {
        "id": "sO9lf-JoWRH1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Main Application (Frontend & Backend Logic)**"
      ],
      "metadata": {
        "id": "rUWIBoHPWKnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Frontend & Backend Logic in single codeblock,\n",
        "# so when code block run the entire app launches at once to prevent \"Variable Not Defined\" errors.\n",
        "\n",
        "import gradio as gr\n",
        "import time\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "\n",
        "# --- 1. LOGIC FUNCTIONS ---\n",
        "\n",
        "# Global variable to store combined text from all uploaded PDFs\n",
        "document_memory = \"\"\n",
        "\n",
        "def process_pdf(files):\n",
        "    \"\"\"Extracts text from multiple PDFs and merges them into document_memory.\"\"\"\n",
        "    global document_memory\n",
        "    if not files:\n",
        "        return \"‚ö†Ô∏è Error: No files detected. Please upload one or more PDFs.\"\n",
        "\n",
        "    document_memory = \"\" # Reset memory for new batch\n",
        "    total_pages = 0\n",
        "    file_names = []\n",
        "\n",
        "    try:\n",
        "        for file in files:\n",
        "            doc = fitz.open(file.name)\n",
        "            total_pages += len(doc)\n",
        "            file_names.append(file.name.split('/')[-1])\n",
        "            for page in doc:\n",
        "                document_memory += page.get_text()\n",
        "\n",
        "        if not document_memory.strip():\n",
        "            return \"‚ö†Ô∏è Warning: The uploaded files appear to be empty or image-based scans.\"\n",
        "\n",
        "        return (f\"‚úÖ SUCCESS: Indexed {len(files)} files ({total_pages} total pages).\\n\"\n",
        "                f\"Documents: {', '.join(file_names)}\\n\"\n",
        "                f\"Status: Ready for cross-document analysis.\")\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error processing files: {str(e)}\"\n",
        "\n",
        "def handle_chat(message, history, mode):\n",
        "    \"\"\"Analyzes merged document_memory to answer questions.\"\"\"\n",
        "    global document_memory\n",
        "    if not message:\n",
        "        return \"\", history\n",
        "\n",
        "    if not document_memory:\n",
        "        bot_response = \"I have no documents in my memory. Please upload PDFs and click 'Process'!\"\n",
        "    else:\n",
        "        search_query = message.lower()\n",
        "\n",
        "        if mode == \"Summary\":\n",
        "            bot_response = f\"Comprehensive Summary: Across the provided documents ({len(document_memory.split())} words), the text discusses: {document_memory[:300]}...\"\n",
        "\n",
        "        elif mode == \"Key Takeaways\":\n",
        "            # Extract common themes (words longer than 6 chars)\n",
        "            themes = list(set([w.strip(',.()').lower() for w in document_memory.split() if len(w) > 7]))\n",
        "            bot_response = f\"Primary themes identified across files: {', '.join(themes[:5])}.\"\n",
        "\n",
        "        else:\n",
        "            # Q&A Logic: Sentence retrieval\n",
        "            sentences = re.split(r'(?<=[.!?]) +', document_memory)\n",
        "            matching_sentences = [s.strip() for s in sentences if search_query in s.lower()]\n",
        "\n",
        "            if matching_sentences:\n",
        "                # Provide up to 3 contextually relevant sentences\n",
        "                context = \" \".join(matching_sentences[:3])\n",
        "                bot_response = f\"From the documents: \\\"{context}\\\"\"\n",
        "            else:\n",
        "                bot_response = f\"I couldn't find a direct mention of '{message}' in the combined text. Try another keyword.\"\n",
        "\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
        "    return \"\", history\n",
        "\n",
        "# --- 2. THEME & STYLING ---\n",
        "\n",
        "monochrome_theme = gr.themes.Soft(\n",
        "    primary_hue=\"slate\",\n",
        "    secondary_hue=\"gray\",\n",
        "    neutral_hue=\"slate\",\n",
        ").set(\n",
        "    button_primary_background_fill=\"*neutral_900\",\n",
        "    button_primary_text_color=\"white\",\n",
        "    block_title_text_weight=\"700\",\n",
        ")\n",
        "\n",
        "custom_css = \"\"\"\n",
        "#status_box { background-color: #f8fafc; border: 1px solid #cbd5e1; }\n",
        "#chat_win { border-radius: 10px; }\n",
        "\"\"\"\n",
        "\n",
        "# --- 3. UI LAYOUT ---\n",
        "\n",
        "gr.close_all() # Reset any existing loops\n",
        "\n",
        "with gr.Blocks(theme=monochrome_theme, css=custom_css, title=\"Multi-PDF Research Buddy\") as demo:\n",
        "    gr.Markdown(\"# üìë Multi-PDF Research Buddy\")\n",
        "    gr.Markdown(\"### *Analyze multiple research papers simultaneously in a minimalist workspace.*\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # LEFT COLUMN\n",
        "        with gr.Column(scale=2, min_width=350):\n",
        "            gr.Markdown(\"#### ‚öôÔ∏è Document Controls\")\n",
        "            # EXTRA FEATURE: Multiple file support\n",
        "            pdf_input = gr.File(\n",
        "                label=\"Upload PDFs\",\n",
        "                file_types=[\".pdf\"],\n",
        "                file_count=\"multiple\" # This enables multi-file upload\n",
        "            )\n",
        "\n",
        "            mode_select = gr.Dropdown(\n",
        "                choices=[\"Standard Q&A\", \"Summary\", \"Key Takeaways\"],\n",
        "                value=\"Standard Q&A\",\n",
        "                label=\"Analysis Mode\"\n",
        "            )\n",
        "\n",
        "            process_btn = gr.Button(\"üîÑ Process All Documents\", variant=\"primary\")\n",
        "\n",
        "            status_output = gr.Textbox(\n",
        "                label=\"System Status\",\n",
        "                lines=6,\n",
        "                elem_id=\"status_box\",\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            exit_btn = gr.Button(\"üö™ Shut Down Portal\", variant=\"stop\")\n",
        "\n",
        "        # RIGHT COLUMN\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"#### üí¨ Chat Interface\")\n",
        "            chatbot = gr.Chatbot(\n",
        "                label=\"Analysis History\",\n",
        "                height=500,\n",
        "                type=\"messages\",\n",
        "                allow_tags=False,\n",
        "                elem_id=\"chat_win\"\n",
        "            )\n",
        "\n",
        "            user_input = gr.Textbox(placeholder=\"Query your documents...\", label=\"Your Question\")\n",
        "\n",
        "            with gr.Row():\n",
        "                send_btn = gr.Button(\"üì§ Send\", variant=\"primary\")\n",
        "                clear_btn = gr.Button(\"üóëÔ∏è Clear Chat History\")\n",
        "\n",
        "    # --- 4. EVENT LISTENERS ---\n",
        "\n",
        "    process_btn.click(fn=process_pdf, inputs=pdf_input, outputs=status_output)\n",
        "\n",
        "    send_btn.click(handle_chat, inputs=[user_input, chatbot, mode_select], outputs=[user_input, chatbot])\n",
        "    user_input.submit(handle_chat, inputs=[user_input, chatbot, mode_select], outputs=[user_input, chatbot])\n",
        "\n",
        "    clear_btn.click(lambda: [], None, chatbot, queue=False)\n",
        "\n",
        "    exit_btn.click(lambda: gr.Info(\"Session Terminated.\"), None, None).then(fn=demo.close)\n",
        "\n",
        "# --- 5. LAUNCH ---\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "8criGDdkWh96"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}