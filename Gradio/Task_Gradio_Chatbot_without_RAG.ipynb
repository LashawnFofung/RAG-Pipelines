{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPk0gzwKlqHScO6bJ3tWRAh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LashawnFofung/RAG-Pipelines/blob/main/Gradio/Task_Gradio_Chatbot_without_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradio Chatbot**\n",
        "\n",
        "**GOAL:** A document Q&A interface with a sleek monochrome theme.\n",
        "\n",
        "<br>\n",
        "\n",
        "**PDF Research Buddy: Project Submission**\n",
        "\n",
        "This notebook contains a Gradio-based chatbot designed to analyze PDF documents using a monochrome theme and a custom \"Analysis Mode\" feature.\n",
        "\n",
        "<br>\n",
        "\n",
        "**FEATURES:**\n",
        "1. Mode Selection (Dropdown) to switch between Q&A and Summarization.\n",
        "2. Visual Loading Status via a dedicated status box and progress bars."
      ],
      "metadata": {
        "id": "6TS4lKCyUXcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Setup & Installation**"
      ],
      "metadata": {
        "id": "kSMsPBvhWKwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install both UI and Document Intelligence libraries\n",
        "!pip install -q pymupdf gradio llama-index llama-index-readers-file google-generativeai jedi"
      ],
      "metadata": {
        "id": "sO9lf-JoWRH1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Main Application (Frontend & Backend Logic)**"
      ],
      "metadata": {
        "id": "rUWIBoHPWKnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Frontend & Backend Logic in single codeblock,\n",
        "# so when code block run the entire app launches at once to prevent \"Variable Not Defined\" errors.\n",
        "\n",
        "# PRE-REQUISITE: Run this in a cell first: !pip install pymupdf\n",
        "\n",
        "import gradio as gr\n",
        "import time\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "\n",
        "# --- 1. LOGIC FUNCTIONS ---\n",
        "\n",
        "# Global variable to store combined text from all uploaded PDFs\n",
        "document_memory = \"\"\n",
        "\n",
        "def process_pdf(files):\n",
        "    \"\"\"Extracts text from multiple PDFs and merges them into document_memory.\"\"\"\n",
        "    global document_memory\n",
        "    if not files:\n",
        "        return \"âš ï¸ Error: No files detected. Please upload one or more PDFs.\"\n",
        "\n",
        "    document_memory = \"\" # Reset memory for new batch\n",
        "    total_pages = 0\n",
        "    file_names = []\n",
        "\n",
        "    try:\n",
        "        for file in files:\n",
        "            doc = fitz.open(file.name)\n",
        "            total_pages += len(doc)\n",
        "            file_names.append(file.name.split('/')[-1])\n",
        "            for page in doc:\n",
        "                document_memory += page.get_text()\n",
        "\n",
        "        if not document_memory.strip():\n",
        "            return \"âš ï¸ Warning: The uploaded files appear to be empty or image-based scans.\"\n",
        "\n",
        "        return (f\"âœ… SUCCESS: Indexed {len(files)} files ({total_pages} total pages).\\n\"\n",
        "                f\"Documents: {', '.join(file_names)}\\n\"\n",
        "                f\"Status: Ready for cross-document analysis.\")\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error processing files: {str(e)}\"\n",
        "\n",
        "def handle_chat(message, history, mode):\n",
        "    \"\"\"Analyzes merged document_memory to answer questions.\"\"\"\n",
        "    global document_memory\n",
        "    if not message:\n",
        "        return \"\", history\n",
        "\n",
        "    if not document_memory:\n",
        "        bot_response = \"I have no documents in my memory. Please upload PDFs and click 'Process'!\"\n",
        "    else:\n",
        "        search_query = message.lower()\n",
        "\n",
        "        if mode == \"Summary\":\n",
        "            bot_response = f\"Comprehensive Summary: Across the provided documents ({len(document_memory.split())} words), the text discusses: {document_memory[:300]}...\"\n",
        "\n",
        "        elif mode == \"Key Takeaways\":\n",
        "            # Extract common themes (words longer than 6 chars)\n",
        "            themes = list(set([w.strip(',.()').lower() for w in document_memory.split() if len(w) > 7]))\n",
        "            bot_response = f\"Primary themes identified across files: {', '.join(themes[:5])}.\"\n",
        "\n",
        "        else:\n",
        "            # Q&A Logic: Sentence retrieval\n",
        "            sentences = re.split(r'(?<=[.!?]) +', document_memory)\n",
        "            matching_sentences = [s.strip() for s in sentences if search_query in s.lower()]\n",
        "\n",
        "            if matching_sentences:\n",
        "                # Provide up to 3 contextually relevant sentences\n",
        "                context = \" \".join(matching_sentences[:3])\n",
        "                bot_response = f\"From the documents: \\\"{context}\\\"\"\n",
        "            else:\n",
        "                bot_response = f\"I couldn't find a direct mention of '{message}' in the combined text. Try another keyword.\"\n",
        "\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
        "    return \"\", history\n",
        "\n",
        "# --- 2. THEME & STYLING ---\n",
        "\n",
        "monochrome_theme = gr.themes.Soft(\n",
        "    primary_hue=\"slate\",\n",
        "    secondary_hue=\"gray\",\n",
        "    neutral_hue=\"slate\",\n",
        ").set(\n",
        "    button_primary_background_fill=\"*neutral_900\",\n",
        "    button_primary_text_color=\"white\",\n",
        "    block_title_text_weight=\"700\",\n",
        ")\n",
        "\n",
        "custom_css = \"\"\"\n",
        "#status_box { background-color: #f8fafc; border: 1px solid #cbd5e1; }\n",
        "#chat_win { border-radius: 10px; }\n",
        "\"\"\"\n",
        "\n",
        "# --- 3. UI LAYOUT ---\n",
        "\n",
        "gr.close_all() # Reset any existing loops\n",
        "\n",
        "with gr.Blocks(theme=monochrome_theme, css=custom_css, title=\"Multi-PDF Research Buddy\") as demo:\n",
        "    gr.Markdown(\"# ðŸ“‘ Multi-PDF Research Buddy\")\n",
        "    gr.Markdown(\"### *Analyze multiple research papers simultaneously in a minimalist workspace.*\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # LEFT COLUMN\n",
        "        with gr.Column(scale=2, min_width=350):\n",
        "            gr.Markdown(\"#### âš™ï¸ Document Controls\")\n",
        "            # EXTRA FEATURE: Multiple file support\n",
        "            pdf_input = gr.File(\n",
        "                label=\"Upload PDFs\",\n",
        "                file_types=[\".pdf\"],\n",
        "                file_count=\"multiple\" # This enables multi-file upload\n",
        "            )\n",
        "\n",
        "            mode_select = gr.Dropdown(\n",
        "                choices=[\"Standard Q&A\", \"Summary\", \"Key Takeaways\"],\n",
        "                value=\"Standard Q&A\",\n",
        "                label=\"Analysis Mode\"\n",
        "            )\n",
        "\n",
        "            process_btn = gr.Button(\"ðŸ”„ Process All Documents\", variant=\"primary\")\n",
        "\n",
        "            status_output = gr.Textbox(\n",
        "                label=\"System Status\",\n",
        "                lines=6,\n",
        "                elem_id=\"status_box\",\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            exit_btn = gr.Button(\"ðŸšª Shut Down Portal\", variant=\"stop\")\n",
        "\n",
        "        # RIGHT COLUMN\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"#### ðŸ’¬ Chat Interface\")\n",
        "            chatbot = gr.Chatbot(\n",
        "                label=\"Analysis History\",\n",
        "                height=500,\n",
        "                type=\"messages\",\n",
        "                allow_tags=False,\n",
        "                elem_id=\"chat_win\"\n",
        "            )\n",
        "\n",
        "            user_input = gr.Textbox(placeholder=\"Query your documents...\", label=\"Your Question\")\n",
        "\n",
        "            with gr.Row():\n",
        "                send_btn = gr.Button(\"ðŸ“¤ Send\", variant=\"primary\")\n",
        "                clear_btn = gr.Button(\"ðŸ—‘ï¸ Clear Chat History\")\n",
        "\n",
        "    # --- 4. EVENT LISTENERS ---\n",
        "\n",
        "    process_btn.click(fn=process_pdf, inputs=pdf_input, outputs=status_output)\n",
        "\n",
        "    send_btn.click(handle_chat, inputs=[user_input, chatbot, mode_select], outputs=[user_input, chatbot])\n",
        "    user_input.submit(handle_chat, inputs=[user_input, chatbot, mode_select], outputs=[user_input, chatbot])\n",
        "\n",
        "    clear_btn.click(lambda: [], None, chatbot, queue=False)\n",
        "\n",
        "    exit_btn.click(lambda: gr.Info(\"Session Terminated.\"), None, None).then(fn=demo.close)\n",
        "\n",
        "# --- 5. LAUNCH ---\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "8criGDdkWh96",
        "outputId": "fc926945-2db2-4d78-dd24-1211511cf2eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1995458098.py:99: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=monochrome_theme, css=custom_css, title=\"Multi-PDF Research Buddy\") as demo:\n",
            "/tmp/ipython-input-1995458098.py:99: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=monochrome_theme, css=custom_css, title=\"Multi-PDF Research Buddy\") as demo:\n",
            "/tmp/ipython-input-1995458098.py:134: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://5954586a0ae01895a8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5954586a0ae01895a8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://5954586a0ae01895a8.gradio.live\n"
          ]
        }
      ]
    }
  ]
}