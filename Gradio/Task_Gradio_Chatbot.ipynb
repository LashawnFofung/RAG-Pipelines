{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNdhBZYCSIxFku8Yz+9mlYz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LashawnFofung/RAG-Pipelines/blob/main/Gradio/Task_Gradio_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradio Chatbot**\n",
        "\n",
        "**GOAL:** A document Q&A interface with a sleek monochrome theme.\n",
        "\n",
        "<br>\n",
        "\n",
        "**PDF Research Buddy: Project Submission**\n",
        "\n",
        "This notebook contains a Gradio-based chatbot designed to analyze PDF documents using a monochrome theme and a custom \"Analysis Mode\" feature.\n",
        "\n",
        "<br>\n",
        "\n",
        "**FEATURES:**\n",
        "1. Mode Selection (Dropdown) to switch between Q&A and Summarization.\n",
        "2. Visual Loading Status via a dedicated status box and progress bars."
      ],
      "metadata": {
        "id": "6TS4lKCyUXcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Setup & Installation**"
      ],
      "metadata": {
        "id": "kSMsPBvhWKwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "sO9lf-JoWRH1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Main Application (Frontend & Backend Logic)**"
      ],
      "metadata": {
        "id": "rUWIBoHPWKnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Frontend & Backend Logic in single codeblock,\n",
        "# so when code block run the entire app launches at once to prevent \"Variable Not Defined\" errors.\n",
        "\n",
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "# --- 1. LOGIC FUNCTIONS ---\n",
        "\n",
        "def process_pdf(file):\n",
        "    if file is None:\n",
        "        return \"‚ö†Ô∏è Error: Please upload a PDF file.\"\n",
        "    time.sleep(1.5)\n",
        "    return f\"COMPLETED\\n---\\nDocument: {file.name.split('/')[-1]}\\nStatus: Ready.\"\n",
        "\n",
        "def handle_chat(message, history, mode):\n",
        "    if not message:\n",
        "        return \"\", history\n",
        "\n",
        "    responses = {\n",
        "        \"Summary\": \"The document provides an overview of technical implementation strategies...\",\n",
        "        \"Key Takeaways\": \"1. Efficiency is prioritized.\\n2. User design is critical.\\n3. Scalability is the goal.\",\n",
        "        \"Standard Q&A\": f\"Analyzing the text for '{message}'... The data indicates a strong positive trend.\"\n",
        "    }\n",
        "\n",
        "    bot_response = responses.get(mode, \"Mode not recognized.\")\n",
        "\n",
        "    # Using the modern 'messages' format\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
        "\n",
        "    return \"\", history\n",
        "\n",
        "# --- 2. THEME & STYLING ---\n",
        "\n",
        "monochrome_theme = gr.themes.Soft(\n",
        "    primary_hue=\"slate\",\n",
        "    secondary_hue=\"gray\",\n",
        "    neutral_hue=\"slate\",\n",
        ").set(\n",
        "    button_primary_background_fill=\"*neutral_900\",\n",
        "    button_primary_text_color=\"white\",\n",
        "    block_title_text_weight=\"700\",\n",
        ")\n",
        "\n",
        "# Defined here to be used in gr.Blocks\n",
        "custom_css = \"#status_box { background-color: #f8fafc; border: 1px solid #cbd5e1; }\"\n",
        "\n",
        "# --- 3. UI LAYOUT ---\n",
        "\n",
        "# Use a clean approach here to avoid loop conflicts\n",
        "gr.close_all()\n",
        "\n",
        "#  Both theme and css here to avoid the TypeError in launch()\n",
        "with gr.Blocks(theme=monochrome_theme, css=custom_css, title=\"AI Research Buddy\") as demo:\n",
        "    gr.Markdown(\"# üìë PDF Research Buddy\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2, min_width=400):\n",
        "            gr.Markdown(\"#### ‚öôÔ∏è Input & Settings\")\n",
        "            pdf_input = gr.File(label=\"Upload Document\", file_types=[\".pdf\"])\n",
        "\n",
        "            mode_select = gr.Dropdown(\n",
        "                choices=[\"Standard Q&A\", \"Summary\", \"Key Takeaways\"],\n",
        "                value=\"Standard Q&A\",\n",
        "                label=\"Analysis Mode\"\n",
        "            )\n",
        "\n",
        "            process_btn = gr.Button(\"üîÑ Process Document\", variant=\"primary\")\n",
        "\n",
        "            status_output = gr.Textbox(\n",
        "                label=\"System Status & Metadata\",\n",
        "                interactive=False,\n",
        "                lines=6,\n",
        "                elem_id=\"status_box\",\n",
        "                placeholder=\"System idle...\"\n",
        "            )\n",
        "\n",
        "            exit_btn = gr.Button(\"üö™ Exit & Shut Down\", variant=\"stop\")\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"#### üí¨ Document Chat\")\n",
        "            # Added allow_tags=False to resolve the final warning\n",
        "            chatbot = gr.Chatbot(\n",
        "                label=\"Conversation History\",\n",
        "                height=500,\n",
        "                type=\"messages\",\n",
        "                allow_tags=False\n",
        "            )\n",
        "\n",
        "            user_input = gr.Textbox(\n",
        "                placeholder=\"Ask a question...\",\n",
        "                label=\"User Query\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                send_btn = gr.Button(\"üì§ Send\", variant=\"primary\")\n",
        "                clear_btn = gr.Button(\"üóëÔ∏è Clear Chat\")\n",
        "\n",
        "    # --- 4. EVENT LISTENERS ---\n",
        "\n",
        "    process_btn.click(fn=process_pdf, inputs=pdf_input, outputs=status_output)\n",
        "\n",
        "    send_btn.click(handle_chat, inputs=[user_input, chatbot, mode_select], outputs=[user_input, chatbot])\n",
        "    user_input.submit(handle_chat, inputs=[user_input, chatbot, mode_select], outputs=[user_input, chatbot])\n",
        "\n",
        "    clear_btn.click(lambda: [], None, chatbot, queue=False)\n",
        "\n",
        "    exit_btn.click(lambda: gr.Info(\"Shutting down...\"), None, None).then(fn=demo.close)\n",
        "\n",
        "# --- 5. LAUNCH ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Removed theme and css from here because your version doesn't support them in launch()\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8criGDdkWh96",
        "outputId": "e66fd1e4-daae-48fb-e55e-cf5d8cd54468"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1434651072.py:54: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=monochrome_theme, css=custom_css, title=\"AI Research Buddy\") as demo:\n",
            "/tmp/ipython-input-1434651072.py:54: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=monochrome_theme, css=custom_css, title=\"AI Research Buddy\") as demo:\n",
            "/tmp/ipython-input-1434651072.py:83: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://2b82f41959c9ba9f81.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2b82f41959c9ba9f81.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1133, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 123, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 109, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 387, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x7eb6b56dbb60 [unset]> is bound to a different event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://2b82f41959c9ba9f81.gradio.live\n"
          ]
        }
      ]
    }
  ]
}