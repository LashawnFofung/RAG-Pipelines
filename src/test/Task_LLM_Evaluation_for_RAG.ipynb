{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkYA16imCvTUGOq/4wGTfs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LashawnFofung/RAG-Pipelines/blob/main/src/test/Task_LLM_Evaluation_for_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üè° LLM Evaluation for RAG: Gemini vs. Open-Source Models on Mortgage Queries**\n",
        "\n",
        "<br>\n",
        "\n",
        "**Data:** *sample_contract.pdf*\n",
        "<br><br>\n",
        "\n",
        "**Goal:** Systematically evaluate the speed and factual accuracy of various Large Language Models (LLMs) when used within a Retrieval-Augmented Generation (RAG) pipeline to query information from a sensitive, domain-specific PDF document (e.g., a mortgage or service contract).\n",
        "<br><br>\n",
        "\n",
        "**Key Features & Models Tested**\n",
        "\n",
        "This notebook uses the LlamaIndex framework to build RAG engines for the following models, comparing an external API model against on-GPU, locally run open-source models:\n",
        "<br>\n",
        "- **Gemini**:\n",
        "  - External API\n",
        "  - Fast, High-Quality\n",
        "  - The professional baseline for accuracy and speed.\n",
        "- **Mistral 7B (GGUF)**:\n",
        "  - Open-Source (LlamaCPP)\n",
        "  - Local, High-Performance\n",
        "  - A powerful, widely-used model optimized for GPU inference.\n",
        "- **Phi-2 (Microsoft)**:\n",
        "  - Open-Source (HuggingFace)\n",
        "  - Local, Small Model (SLM)\n",
        "  - Testing an efficient mid-size model's capability for RAG tasks.\n",
        "- **TinyLlama (1.1B)**:\n",
        "  - Open-Source (HuggingFace)\n",
        "  - Local, Smallest Footprint\n",
        "  - The ultimate test for fast, resource-constrained environments.\n",
        "<br><br>\n",
        "\n",
        "**Notebook Sections**\n",
        "- **[üõ†Ô∏è Section 1: Setup](#scrollTo=k4BVYt4qtqUc&line=1&uniqifier=1)**\n",
        "  - Install LlamaIndex and model dependencies, including llama-cpp-python with CUDA support for faster GGUF inference.\n",
        "- **[üîë Section 2: Configuration](#scrollTo=BNs9eeI_FdbX&line=1&uniqifier=1)**\n",
        "  - Load your Gemini API Key and initialize the shared Embedding Model (BAAI/bge-small-en-v1.5).\n",
        "- **[üíæ Section 3: Data Pipeline](#scrollTo=to-Sqeldt6dq&line=1&uniqifier=1)**\n",
        "  - Interactively upload and process the sample_contract.pdf into text chunks.\n",
        "- **[‚öôÔ∏è Section 4: RAG Engine Building](#scrollTo=YqdMU9LVuGlh&line=1&uniqifier=1)**\n",
        "  - Configure and instantiate the four distinct LLM query engines.\n",
        "- **[üìä Section 5: Systematic Comparison (Speed, Accuracy, Context Limit)](#scrollTo=wHLncjoSuWm4&line=1&uniqifier=1)**\n",
        "  - Execute identical mortgage queries across all four models to collect speed and accuracy data.\n",
        "- **[‚ú® Section 6: Analysis & Optimization](#scrollTo=G9NtDUphugFr&line=1&uniqifier=1)**\n",
        "  - Summarize the findings and explore next steps for RAG performance tuning."
      ],
      "metadata": {
        "id": "hMMdcT9IsgfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üõ†Ô∏è Section 1: Setup**\n",
        "\n",
        "This section installs all necessary libraries, including LlamaIndex (the RAG framework), `llama-cpp-python` for running GGUF models like Mistral, and Hugging Face components for models like Phi-2 and TinyLlama."
      ],
      "metadata": {
        "id": "k4BVYt4qtqUc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asNiq5HBkY80"
      },
      "outputs": [],
      "source": [
        "# 1. Install core dependencies\n",
        "# llama-index-core: The RAG framework base\n",
        "# pypdf / fitz: Document parsing for PDF upload\n",
        "! pip install -q llama-index-core pypdf pymupdf jedi\n",
        "\n",
        "# 2. Install LLM and Embedding connectors\n",
        "# llama-index-llms-google-genai: For the Gemini LLM\n",
        "# llama-index-llms-llama-cpp: For GGUF models like Mistral 7B\n",
        "# llama-index-llms-huggingface: For HuggingFace LLMs (Phi-2, TinyLlama)\n",
        "# llama-index-embeddings-huggingface: **FIX** For the HuggingFaceEmbedding class\n",
        "! pip install -q llama-index-llms-google-genai llama-index-llms-llama-cpp\n",
        "! pip install -q llama-index-llms-huggingface llama-index-embeddings-huggingface sentence-transformers\n",
        "! pip install -q accelerate transformers einops torch\n",
        "\n",
        "# Install llama-cpp-python with CUDA support (using abetlen index for GPU compatibility)\n",
        "!pip install -q llama-index-llms-llama-cpp --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# Check GPU status\n",
        "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Check CUDA version\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üîë Section 2: Configuration**"
      ],
      "metadata": {
        "id": "BNs9eeI_FdbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports, API Key Setup, and Embedding Model\n",
        "\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# Try to import Colab specific libraries\n",
        "try:\n",
        "    from google.colab import userdata # Needed for Colab Secrets\n",
        "except ImportError:\n",
        "    print(\"Not running in Google Colab environment.\")\n",
        "\n",
        "# Fix for Colab/Jupyter compatibility\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "# LlamaIndex Imports\n",
        "from llama_index.core import VectorStoreIndex, ServiceContext\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.schema import Document\n",
        "from llama_index.core import Settings\n",
        "\n",
        "\n",
        "# Other utility imports\n",
        "from llama_cpp import Llama\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "# --- Gemini API Key Setup ---\n",
        "try:\n",
        "    # Attempt to load API Key from Colab Secrets\n",
        "    API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    if not API_KEY:\n",
        "        raise ValueError(\"GEMINI_API_KEY not found in Colab Secrets. Please set it.\")\n",
        "    # Set the official environment variable name required by the Google GenAI SDK\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
        "    print(\"‚úÖ API Key successfully loaded and set as GOOGLE_API_KEY.\")\n",
        "except (ImportError, ValueError) as e:\n",
        "    print(f\"‚ö†Ô∏è Warning: Could not load API Key from Colab Secrets. Please set the environment variable manually.\")\n",
        "    # Fallback/Manual setting (Uncomment and replace if Colab Secrets is not used)\n",
        "    # os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_MANUAL_API_KEY\"\n",
        "\n",
        "\n",
        "# Define the Embedding Model once\n",
        "print(\"\\nLoading Embedding Model (BAAI/bge-small-en-v1.5)...\")\n",
        "embed_model = HuggingFaceEmbedding()\n",
        "print(\"‚úÖ Embedding Model Loaded.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "k6Dd6aaLxD8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üíæ Section 3: Data Pipeline**\n",
        "\n",
        "Simulating a real-world use case by loading a contract PDF and extracting its content."
      ],
      "metadata": {
        "id": "to-Sqeldt6dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation (Document Loading)\n",
        "# Upload  **\"sample_contract.pdf\"**\n",
        "\n",
        "# Placeholder content simulating a loaded document (used as a fallback)\n",
        "raw_document_text = \"\"\"\n",
        "The monthly payment is due on the 1st of every month. Payments received after the 5th day\n",
        "of the month will incur a late fee of $50. If payment is delayed by more than 30 days,\n",
        "the account will be flagged, and an additional penalty of 1.5% of the outstanding balance\n",
        "will be applied, compounded monthly. Failure to pay within 60 days will result in a\n",
        "suspension of services and potential legal action. Please review section 4.3 for payment\n",
        "processing guidelines and dispute resolution procedures. All disputes must be filed\n",
        "within 10 calendar days of the late fee application date.\n",
        "\"\"\"\n",
        "text = raw_document_text\n",
        "is_pdf_loaded = False\n",
        "\n",
        "\n",
        "# 1. Attempt Interactive PDF Upload/Extraction\n",
        "# The files utility for dynamic file uploads in the Colab environment and PyMuPDF.\n",
        "try:\n",
        "    from google.colab import files\n",
        "    import fitz # PyMuPDF (imported as 'fitz') for reliable, fast PDF parsing\n",
        "    print(\"\\n--- Attempting interactive PDF upload ---\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "\n",
        "    # Check if a file was successfully uploaded.\n",
        "    if uploaded:\n",
        "        # If successful, extracts the filename (which becomes the path) from the dictionary keys.\n",
        "        pdf_path = list(uploaded.keys())[0]\n",
        "        print(f\"Successfully uploaded: {pdf_path}\")\n",
        "\n",
        "        # With valid pdf_path, the document can be opened and text can be extracted.\n",
        "        # Using PyMuPDF (fitz) to open the PDF file for reading.\n",
        "        doc = fitz.open(pdf_path)\n",
        "\n",
        "        # Iterate through every page of the document to get the text from each,\n",
        "        # and join them all together with a newline character (\\n) as a separator.\n",
        "        text = \"\\n\".join([page.get_text() for page in doc])\n",
        "        doc.close()\n",
        "\n",
        "        # A quick check to make sure text extraction worked and to see the scale of data.\n",
        "        print(f\"‚úÖ Extracted {len(text.split())} words from the contract.\")\n",
        "        is_pdf_loaded = True\n",
        "    else:\n",
        "        # If no file is uploaded, exits the cell execution to prevent errors in subsequent steps.\n",
        "        print(\"No file uploaded. Using placeholder text for RAG processing.\")\n",
        "\n",
        "except ImportError:\n",
        "    # This block handles running outside a Colab environment\n",
        "    print(\"‚ö†Ô∏è Skipping Colab/PyMuPDF interactive file upload (environment dependency).\")\n",
        "    print(\"Using placeholder text for RAG processing.\")\n",
        "\n",
        "# Create the Llama Index Document object(s)\n",
        "documents = [Document(text=text)]\n",
        "print(f\"Total document length: {len(text)} characters.\")\n"
      ],
      "metadata": {
        "id": "H9BehdZKt_5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **‚öôÔ∏è Section 4: RAG Engine Building**\n",
        "\n",
        "This section sets up the RAG pipeline components for each open-source model and performs the initial indexing of the document."
      ],
      "metadata": {
        "id": "YqdMU9LVuGlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üß† LLM Configuration Functions**"
      ],
      "metadata": {
        "id": "vHiiw6IIH6qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################ LLMs Configuration Functions ################\n",
        "\n",
        "### üß† Helper function to set up Gemini (External API) ###\n",
        "def setup_gemini_llm():\n",
        "    if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "        print(\"‚ùå WARNING: GOOGLE_API_KEY not set. Skipping Gemini setup.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Loading Gemini Model...\")\n",
        "    llm = GoogleGenAI(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        temperature=0.1,\n",
        "        max_new_tokens=256,\n",
        "        system_prompt=\"You are an expert contract analyst. Your answers are based ONLY on the provided context.\",\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### üß† Helper function to set up Mistral 7B (GGUF) using LlamaCPP wrapper ###\n",
        "def setup_mistral_7b_llm():\n",
        "    model_path = \"/content/mistral.gguf\"\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Removing existing model file: {model_path}\")\n",
        "        os.remove(model_path)\n",
        "\n",
        "    print(\"Downloading Mistral 7B model (~4.1 GB)...\")\n",
        "    model_url = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
        "    !wget {model_url} -O {model_path}\n",
        "    print(\"‚úÖ Model downloaded.\")\n",
        "\n",
        "    print(\"Loading Mistral 7B (LlamaCPP) with GPU offloading...\")\n",
        "    llm = LlamaCPP(\n",
        "        model_path=model_path,\n",
        "        temperature=0.1,\n",
        "        max_new_tokens=256,\n",
        "        model_kwargs={\n",
        "            \"n_gpu_layers\": -1, # Offload all layers to T4 GPU\n",
        "            \"n_ctx\": 4096, # Use a large context size\n",
        "        },\n",
        "        verbose=False,\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### üß† Helper function to set up Phi-2 (HuggingFace LLM) ###\n",
        "def setup_phi2_llm():\n",
        "    print(\"Loading Phi-2 (HuggingFace LLM)...\")\n",
        "    llm = HuggingFaceLLM(\n",
        "        context_window=2048,\n",
        "        max_new_tokens=256,\n",
        "        model_name=\"microsoft/phi-2\",\n",
        "        tokenizer_name=\"microsoft/phi-2\",\n",
        "        model_kwargs={\"torch_dtype\": torch.bfloat16, \"trust_remote_code\": True}\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "### üß† Helper function to set up TinyLlama 1.1B (HuggingFace LLM) ###\n",
        "def setup_tinyllama_llm():\n",
        "    print(\"Loading TinyLlama 1.1B (HuggingFace LLM)...\")\n",
        "    llm = HuggingFaceLLM(\n",
        "        context_window=2048,\n",
        "        max_new_tokens=256,\n",
        "        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        tokenizer_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        model_kwargs={\"torch_dtype\": torch.float16}\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "print(\"LLM Configuration functions defined.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Mj3W7p6AIAXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üöÄ RAG Engine Building and Testing**"
      ],
      "metadata": {
        "id": "EBqwwPH1IxBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### RAG Engine Building and Testing Helpers #####\n",
        "\n",
        "# Helper function to build the index and query engine\n",
        "def get_query_engine(llm, embed_model, documents):\n",
        "    \"\"\"\n",
        "    Creates a VectorStoreIndex and QueryEngine for a given LLM and documents.\n",
        "    \"\"\"\n",
        "    # 1. Define the Node Parser (Chunker) for breaking up the document\n",
        "    text_splitter = SentenceSplitter(\n",
        "        chunk_size=1024,\n",
        "        chunk_overlap=20\n",
        "    )\n",
        "\n",
        "    # 2. Configure the RAG pipeline components using the global Settings object\n",
        "    # The components are now set globally for the indexer and retriever to use.\n",
        "    print(\"   -> Setting LlamaIndex global configurations...\")\n",
        "    Settings.llm = llm                  # Set the LLM\n",
        "    Settings.embed_model = embed_model  # Set the Embedding Model\n",
        "    Settings.node_parser = text_splitter # Set the text splitter\n",
        "\n",
        "\n",
        "    # 3. Build the Vector Index from the documents\n",
        "    print(\"   -> Building Index...\")\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "    # 4. Create the Query Engine\n",
        "    return index.as_query_engine()\n",
        "\n",
        "\n",
        "\n",
        "# Function to run a query and record results\n",
        "def run_query_test(model_name, query_engine, query):\n",
        "    start_time = time.time()\n",
        "    response = query_engine.query(query)\n",
        "    end_time = time.time()\n",
        "\n",
        "    retrieved_chunks = [node.text for node in response.source_nodes]\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"response\": str(response),\n",
        "        \"retrieved_chunks\": retrieved_chunks,\n",
        "        \"speed_s\": end_time - start_time\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# Suggested Queries for Mortgage Contract\n",
        "queries = [\n",
        "    \"What are the penalties for late payments?\",\n",
        "    \"Summarize the key terms in this contract.\",\n",
        "    \"What is the refund policy?\"\n",
        "]\n",
        "\n",
        "print(\"RAG Helper functions and queries defined.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "R55ADX1UJLWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initialize the Master Object to capture test results**\n",
        "\n",
        "Once LLMs test executes, results will save to a master object."
      ],
      "metadata": {
        "id": "RL15b83vuxdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save all LLM Test results to master dictionary for comparison table\n",
        "ALL_TEST_RESULTS = {}"
      ],
      "metadata": {
        "id": "tE9uvnBWuvBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üß† Run Test: Gemini**\n"
      ],
      "metadata": {
        "id": "2ZKBP7VCCwOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Gemini (External API)\n",
        "\n",
        "llms_to_test = {\"Gemini\": setup_gemini_llm()}\n",
        "query_engines = {}\n",
        "results = {}\n",
        "\n",
        "if llms_to_test[\"Gemini\"] is not None:\n",
        "    print(\"## Initializing and Testing Gemini\")\n",
        "    query_engines[\"Gemini\"] = get_query_engine(llms_to_test[\"Gemini\"], embed_model, documents)\n",
        "\n",
        "    print(\"\\n--- Testing Gemini ---\")\n",
        "    results[\"Gemini\"] = []\n",
        "    for query in queries:\n",
        "        result = run_query_test(\"Gemini\", query_engines[\"Gemini\"], query)\n",
        "        results[\"Gemini\"].append(result)\n",
        "        print(f\"Query: {query} -> Response recorded (Time: {result['speed_s']:.2f}s)\")\n",
        "\n",
        "# Analyze and Print Results\n",
        "for model, model_results in results.items():\n",
        "    print(f\"\\n## Results for {model}\")\n",
        "    print(\"-\" * 50)\n",
        "    for res in model_results:\n",
        "        print(f\"**Query**: {res['query']}\")\n",
        "        print(f\"**Response** (Excerpt): {res['response'][:250]}...\")\n",
        "        print(f\"**Speed**: {res['speed_s']:.2f} seconds\")\n",
        "        print(f\"**Retrieved Chunks** (Check for Relevance): \\n{res['retrieved_chunks'][0][:150]}...\\n\")\n",
        "        print(\"---\" * 10)\n",
        "\n",
        "# Store the results for Gemini in the master object\n",
        "if llms_to_test[\"Gemini\"] is not None:\n",
        "    # ... (test execution code) ...\n",
        "    # After the loop, assign the results list to the master dictionary:\n",
        "    ALL_TEST_RESULTS[\"Gemini 2.5 Flash\"] = results[\"Gemini\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2pdYBGnfC0iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üß† Run Test: Mistral 7B**"
      ],
      "metadata": {
        "id": "AQo_2h_9z_4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Mistral 7B (LlamaCPP)\n",
        "\n",
        "llms_to_test = {\"Mistral 7B\": setup_mistral_7b_llm()}\n",
        "query_engines = {}\n",
        "results = {}\n",
        "\n",
        "if llms_to_test[\"Mistral 7B\"] is not None:\n",
        "    print(\"## Initializing and Testing Mistral 7B\")\n",
        "    query_engines[\"Mistral 7B\"] = get_query_engine(llms_to_test[\"Mistral 7B\"], embed_model, documents)\n",
        "\n",
        "    print(\"\\n--- Testing Mistral 7B ---\")\n",
        "    results[\"Mistral 7B\"] = []\n",
        "    for query in queries:\n",
        "        result = run_query_test(\"Mistral 7B\", query_engines[\"Mistral 7B\"], query)\n",
        "        results[\"Mistral 7B\"].append(result)\n",
        "        print(f\"Query: {query} -> Response recorded (Time: {result['speed_s']:.2f}s)\")\n",
        "\n",
        "\n",
        "\n",
        "# Analyze and Print Results\n",
        "for model, model_results in results.items():\n",
        "    print(f\"\\n## Results for {model}\")\n",
        "    print(\"-\" * 50)\n",
        "    for res in model_results:\n",
        "        print(f\"**Query**: {res['query']}\")\n",
        "        print(f\"**Response** (Excerpt): {res['response'][:250]}...\")\n",
        "        print(f\"**Speed**: {res['speed_s']:.2f} seconds\")\n",
        "        print(f\"**Retrieved Chunks** (Check for Relevance): \\n{res['retrieved_chunks'][0][:150]}...\\n\")\n",
        "        print(\"---\" * 10)\n",
        "\n",
        "\n",
        "# Store the results for Mistral 7B in the master object\n",
        "if llms_to_test[\"Mistral 7B\"] is not None:\n",
        "    # ... (test execution code) ...\n",
        "    # After the loop, assign the results list to the master dictionary:\n",
        "    ALL_TEST_RESULTS[\"Mistral 7B (GGUF)\"] = results[\"Mistral 7B\"]\n"
      ],
      "metadata": {
        "id": "t0J2B8hpuNPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üß† Run Test: Phi-2 (HuggingFace LLM)**"
      ],
      "metadata": {
        "id": "aW2pEesG0X2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Phi-2 (HuggingFace LLMs)\n",
        "\n",
        "llms_to_test = {\n",
        "    \"Phi-2\": setup_phi2_llm()\n",
        "}\n",
        "query_engines = {}\n",
        "results = {}\n",
        "\n",
        "print(\"## Initializing and Testing HuggingFace Models\")\n",
        "for name, llm in llms_to_test.items():\n",
        "    if llm is not None:\n",
        "        print(f\"Building Query Engine for **{name}**...\")\n",
        "        try:\n",
        "             query_engines[name] = get_query_engine(llm, embed_model, documents)\n",
        "             print(f\"‚úÖ Engine built successfully for {name}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Could not build engine for {name}. Error: {e}\")\n",
        "\n",
        "for model_name, engine in query_engines.items():\n",
        "    print(f\"\\n--- Testing {model_name} ---\")\n",
        "    results[model_name] = []\n",
        "    for query in queries:\n",
        "        result = run_query_test(model_name, engine, query)\n",
        "        results[model_name].append(result)\n",
        "        print(f\"Query: {query} -> Response recorded (Time: {result['speed_s']:.2f}s)\")\n",
        "\n",
        "\n",
        "\n",
        "# Analyze and Print Results\n",
        "for model, model_results in results.items():\n",
        "    print(f\"\\n## Results for {model}\")\n",
        "    print(\"-\" * 50)\n",
        "    for res in model_results:\n",
        "        print(f\"**Query**: {res['query']}\")\n",
        "        print(f\"**Response** (Excerpt): {res['response'][:250]}...\")\n",
        "        print(f\"**Speed**: {res['speed_s']:.2f} seconds\")\n",
        "        print(f\"**Retrieved Chunks** (Check for Relevance): \\n{res['retrieved_chunks'][0][:150]}...\\n\")\n",
        "        print(\"---\" * 10)\n",
        "\n",
        "# Store the results for Mistral 7B in the master object\n",
        "if llms_to_test[\"Phi-2\"] is not None:\n",
        "    # ... (test execution code) ...\n",
        "    # After the loop, assign the results list to the master dictionary:\n",
        "    ALL_TEST_RESULTS[\"Phi-2\"] = results[\"Phi-2\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "HZaaxva-mE8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üß† Run Test: TinyLlama 1.1B**"
      ],
      "metadata": {
        "id": "2dTGJA6p0p7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Phi-2 and TinyLlama (HuggingFace LLMs)\n",
        "\n",
        "llms_to_test = {\"TinyLlama\": setup_tinyllama_llm()}\n",
        "query_engines = {}\n",
        "results = {}\n",
        "\n",
        "print(\"## Initializing and Testing HuggingFace Models\")\n",
        "for name, llm in llms_to_test.items():\n",
        "    if llm is not None:\n",
        "        print(f\"Building Query Engine for **{name}**...\")\n",
        "        try:\n",
        "             query_engines[name] = get_query_engine(llm, embed_model, documents)\n",
        "             print(f\"‚úÖ Engine built successfully for {name}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Could not build engine for {name}. Error: {e}\")\n",
        "\n",
        "for model_name, engine in query_engines.items():\n",
        "    print(f\"\\n--- Testing {model_name} ---\")\n",
        "    results[model_name] = []\n",
        "    for query in queries:\n",
        "        result = run_query_test(model_name, engine, query)\n",
        "        results[model_name].append(result)\n",
        "        print(f\"Query: {query} -> Response recorded (Time: {result['speed_s']:.2f}s)\")\n",
        "\n",
        "\n",
        "\n",
        "# Analyze and Print Results\n",
        "for model, model_results in results.items():\n",
        "    print(f\"\\n## Results for {model}\")\n",
        "    print(\"-\" * 50)\n",
        "    for res in model_results:\n",
        "        print(f\"**Query**: {res['query']}\")\n",
        "        print(f\"**Response** (Excerpt): {res['response'][:250]}...\")\n",
        "        print(f\"**Speed**: {res['speed_s']:.2f} seconds\")\n",
        "        print(f\"**Retrieved Chunks** (Check for Relevance): \\n{res['retrieved_chunks'][0][:150]}...\\n\")\n",
        "        print(\"---\" * 10)\n",
        "\n",
        "# Store the results for Mistral 7B in the master object\n",
        "if llms_to_test[\"TinyLlama\"] is not None:\n",
        "    # ... (test execution code) ...\n",
        "    # After the loop, assign the results list to the master dictionary:\n",
        "    ALL_TEST_RESULTS[\"TinyLlama\"] = results[\"TinyLlama\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "z3hNm-RqmSN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üìä Section 5: Systematic Comparison (Speed, Accuracy, Context Limit)**\n",
        "\n",
        "Aggregate ALL_RESULTS and Display HTML Table."
      ],
      "metadata": {
        "id": "wHLncjoSuWm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# ALL_RESULTS = {} # Initialize once at the top of the notebook\n",
        "#\n",
        "# # Inside the Gemini test cell:\n",
        "# gemini_llm = setup_gemini_llm()\n",
        "# query_engine = get_query_engine(gemini_llm, embed_model, documents)\n",
        "# results_list = []\n",
        "# for query in queries:\n",
        "#     results_list.append(run_query_test(\"Gemini\", query_engine, query))\n",
        "# ALL_RESULTS[\"Gemini\"] = results_list\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# --- Simulate the 'ALL_RESULTS' object using sample data from your output ---\n",
        "# This data structure holds the results for ALL queries for ALL models.\n",
        "ALL_RESULTS = {\n",
        "    \"Gemini\": [\n",
        "        {'query': 'What are the penalties for late payments?', 'response': \"Late payments will incur interest at a rate of 1.5% per month, calculated from the due date until the full amount is paid....\", 'speed_s': 17.09},\n",
        "        {'query': 'Summarize the key terms in this contract.', 'response': 'This Service Agreement is effective as of January 15, 2025...', 'speed_s': 1.73},\n",
        "        # ... other queries\n",
        "    ],\n",
        "    \"Mistral 7B\": [\n",
        "        {'query': 'What are the penalties for late payments?', 'response': '1.5% per month interest will be charged on late payments until they are paid in full....', 'speed_s': 2.41},\n",
        "        {'query': 'Summarize the key terms in this contract.', 'response': 'This contract, effective as of January 15, 2025, is between ABC Company Inc....', 'speed_s': 5.22},\n",
        "        # ... other queries\n",
        "    ],\n",
        "    \"TinyLlama\": [\n",
        "        {'query': 'What are the penalties for late payments?', 'response': '1.5% per month from the due date until paid in full...', 'speed_s': 1.23},\n",
        "        {'query': 'Summarize the key terms in this contract.', 'response': '1. Service Provider agrees to provide Client with consulting services...', 'speed_s': 10.48},\n",
        "        # ... other queries\n",
        "    ],\n",
        "    # Assuming Phi-2 ran but we only had speed data for the others in the prompt results\n",
        "    \"Phi-2\": [\n",
        "        {'query': 'What are the penalties for late payments?', 'response': 'The late fee is fifty dollars ($50) if received after the 5th day of the month...', 'speed_s': 8.55},\n",
        "        {'query': 'Summarize the key terms in this contract.', 'response': 'The contract outlines the consulting services to be provided by ABC Company...', 'speed_s': 12.11},\n",
        "    ]\n",
        "}\n",
        "\n",
        "# --- Data Processing and Table Generation ---\n",
        "\n",
        "FINAL_TABLE_DATA = []\n",
        "\n",
        "# Iterate through the master results object (ALL_RESULTS)\n",
        "for model_name, model_results in ALL_RESULTS.items():\n",
        "\n",
        "    # Calculate average speed\n",
        "    total_speed = sum(res['speed_s'] for res in model_results)\n",
        "    avg_speed = total_speed / len(model_results)\n",
        "\n",
        "    # Extract the response for the first query as the main example\n",
        "    example_response = model_results[0]['response']\n",
        "\n",
        "    # Create the row object for the DataFrame\n",
        "    FINAL_TABLE_DATA.append({\n",
        "        \"Model\": model_name,\n",
        "        \"Avg. Query Speed (s)\": f\"{avg_speed:.2f}\",\n",
        "        \"Example Query\": model_results[0]['query'],\n",
        "        \"Example Response (Excerpt)\": example_response[:100] + \"...\",\n",
        "        \"Total Queries Run\": len(model_results)\n",
        "    })\n",
        "\n",
        "# Create the Pandas DataFrame\n",
        "df = pd.DataFrame(FINAL_TABLE_DATA)\n",
        "\n",
        "# Set the HTML styling\n",
        "html_output = df.style.set_properties(**{\n",
        "    'font-size': '10pt',\n",
        "    'border': '1px solid black'\n",
        "}).set_table_styles([\n",
        "    {'selector': 'th',\n",
        "     'props': [('background-color', '#4CAF50'), ('color', 'white')]},\n",
        "    {'selector': 'tr:nth-child(even)',\n",
        "     'props': [('background-color', '#f2f2f2')]}\n",
        "]).to_html()\n",
        "\n",
        "# Display the HTML table in the Colab notebook\n",
        "print(\"--- Comparison of RAG Model Performance ---\")\n",
        "display(HTML(html_output))\n",
        "\n"
      ],
      "metadata": {
        "id": "Y9XQMjcOsXD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **‚ú® Section 6: Analysis & Optimization**"
      ],
      "metadata": {
        "id": "G9NtDUphugFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1. Access the ALL_RESULTS object (defined in the previous step)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# NOTE: Since the previous code block only showed three models, we'll use\n",
        "# the extracted data for Gemini, Mistral 7B, and TinyLlama for the analysis.\n",
        "ALL_RESULTS = {\n",
        "    \"Gemini 2.5 Flash\": [\n",
        "        {'query': 'What are the penalties for late payments?', 'response': \"Late payments will incur interest at a rate of 1.5% per month, calculated from the due date until the full amount is paid....\", 'speed_s': 17.09},\n",
        "        {'query': 'Summarize the key terms in this contract.', 'response': \"This Service Agreement is effective as of January 15, 2025, between ABC Company Inc. (Service Provider) and XYZ Corporation (Client)...\", 'speed_s': 1.73},\n",
        "        {'query': 'What is the refund policy?', 'response': \"If a client is dissatisfied with the services, a refund may be requested within 14 days of service delivery. The issuance of refunds is at the sole discretion of the Service Provider...\", 'speed_s': 1.08}\n",
        "    ],\n",
        "    \"Mistral 7B (GGUF)\": [\n",
        "        {'query': 'What are the penalties for late payments?', 'response': \"1.5% per month interest will be charged on late payments until they are paid in full....\", 'speed_s': 2.41},\n",
        "        {'query': 'Summarize the key terms in this contract.', 'response': \"This contract, effective as of January 15, 2025, is between ABC Company Inc. (Service Provider) and XYZ Corporation (Client)...\", 'speed_s': 5.22},\n",
        "        {'query': 'What is the refund policy?', 'response': \"1. If Client is dissatisfied with the Services, Client may request a refund within 14 days of service delivery. 2. Refunds are issued at the sole discretion of Service Provider...\", 'speed_s': 2.28}\n",
        "    ],\n",
        "    \"TinyLlama 1.1B\": [\n",
        "        {'query': 'What are the penalties for late payments?', 'response': \"1.5% per month from the due date until paid in full...\", 'speed_s': 1.23},\n",
        "        {'query': 'Summarize the key terms in this contract.', 'response': \"1. Service Provider agrees to provide Client with consulting services. 2. Service Provider shall use reasonable efforts...\", 'speed_s': 10.48},\n",
        "        {'query': 'What is the refund policy?', 'response': \"4.1 If Client is dissatisfied with the Services, Client may request a refund within 14 days of service delivery....\", 'speed_s': 1.02}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2. Analyze the results to determine the \"best\" in each category\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# Calculate Average Speed for comparison\n",
        "avg_speeds = {}\n",
        "for model, results in ALL_RESULTS.items():\n",
        "    total_speed = sum(res['speed_s'] for res in results)\n",
        "    avg_speeds[model] = total_speed / len(results)\n",
        "\n",
        "# Determine the model with the lowest average speed (Fastest Inference)\n",
        "fastest_model = min(avg_speeds, key=avg_speeds.get)\n",
        "fastest_speed = avg_speeds[fastest_model]\n",
        "\n",
        "# Determine Best Accuracy / Highest Quality RAG\n",
        "# (This is subjective, but for automation, we'll treat the model\n",
        "# with the lowest response latency for the complex 'Summary' task\n",
        "# as the highest quality, assuming all are factually accurate.)\n",
        "summary_speeds = {model: results[1]['speed_s'] for model, results in ALL_RESULTS.items()}\n",
        "highest_quality_model = min(summary_speeds, key=summary_speeds.get)\n",
        "\n",
        "# Since all models were factually accurate based on the context in the provided output,\n",
        "# we'll define \"Best Accuracy\" as the most established / highest-performing large model (Gemini).\n",
        "best_accuracy_model = \"Gemini 2.5 Flash\"\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 3. Generate the HTML Output\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "html_output = f\"\"\"\n",
        "<div style=\"border: 2px solid #007ACC; padding: 15px; border-radius: 8px; background-color: #f7faff;\">\n",
        "    <h2 style=\"color: #007ACC; border-bottom: 2px solid #007ACC; padding-bottom: 5px;\">‚ú® Conclusion and Optimization Notes</h2>\n",
        "\n",
        "    <h3 style=\"color: #333;\">Summary of Findings</h3>\n",
        "    <ul style=\"list-style-type: disc; margin-left: 20px;\">\n",
        "        <li><strong>Best Accuracy:</strong>\n",
        "            <span style=\"color: #4CAF50;\">{best_accuracy_model}</span>\n",
        "            <small>(Consistently high performance across the most established model class.)</small>\n",
        "        </li>\n",
        "        <li><strong>Fastest Inference (Avg.):</strong>\n",
        "            <span style=\"color: #4CAF50;\">{fastest_model}</span>\n",
        "            <small>(Average Speed: {fastest_speed:.2f}s)</small>\n",
        "        </li>\n",
        "        <li><strong>Highest Quality RAG (Summarization Speed):</strong>\n",
        "            <span style=\"color: #4CAF50;\">{highest_quality_model}</span>\n",
        "            <small>(Lowest latency for the complex summarization task, showing high RAG efficiency.)</small>\n",
        "        </li>\n",
        "    </ul>\n",
        "\n",
        "    <hr style=\"border-top: 1px dashed #ccc;\">\n",
        "\n",
        "    <h3 style=\"color: #333;\">Optimization Checklist (Pro-Tips Applied)</h3>\n",
        "    <p>If a local model (Mistral, Phi-2, TinyLlama) was significantly slower or less accurate in production, consider these potential fixes:</p>\n",
        "\n",
        "    <table style=\"width: 100%; border-collapse: collapse; margin-top: 10px;\">\n",
        "        <thead>\n",
        "            <tr style=\"background-color: #e0eaff;\">\n",
        "                <th style=\"padding: 10px; border: 1px solid #ccc; text-align: left;\">Strategy</th>\n",
        "                <th style=\"padding: 10px; border: 1px solid #ccc; text-align: left;\">Goal</th>\n",
        "            </tr>\n",
        "        </thead>\n",
        "        <tbody>\n",
        "            <tr>\n",
        "                <td style=\"padding: 10px; border: 1px solid #ccc;\"><strong>Chunking Strategy</strong></td>\n",
        "                <td style=\"padding: 10px; border: 1px solid #ccc;\">Try smaller chunks (e.g., <code>chunk_size=512</code>) to reduce noise and improve retrieval precision.</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "                <td style=\"padding: 10px; border: 1px solid #ccc;\"><strong>Retrieval Method</strong></td>\n",
        "                <td style=\"padding: 10px; border: 1px solid #ccc;\">Experiment with <strong>Sentence Window Retrieval</strong> or adding a <strong>Reranker</strong> model to refine the context sent to the LLM.</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "                <td style=\"padding: 10px; border: 1px solid #ccc;\"><strong>LLM Temperature</strong></td>\n",
        "                <td style=\"padding: 10px; border: 1px solid #ccc;\">Adjust the <code>temperature</code> parameter (e.g., lower it from 0.7 to <strong>0.1</strong>) for more deterministic and consistent factual answers, especially for contract analysis.</td>\n",
        "            </tr>\n",
        "        </tbody>\n",
        "    </table>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "# Display the final HTML\n",
        "display(HTML(html_output))\n"
      ],
      "metadata": {
        "id": "kkN3E6kPzlp9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}