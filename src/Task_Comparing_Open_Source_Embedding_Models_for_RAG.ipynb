{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNK8fSyCKmd19RjFNKKvhv0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LashawnFofung/RAG-Pipelines/blob/main/src/Task_Comparing_Open_Source_Embedding_Models_for_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparing, Testing, and Choosing the Best Embedding Model for Retrieval-Augmented Generation (RAG)**\n",
        "\n",
        "**Data:** *sample_contract.pdf*\n",
        "<br><br>\n",
        "\n",
        "\n",
        "The embedding model is the connective tissue of any RAG system, directly determining the quality and relevance of the retrieved context. A superior embedding model captures the semantic meaning behind my user queries and my knowledge base documents, which should lead to more accurate and helpful answers from the Large Language Model (LLM).\n",
        "<br><br>\n",
        "\n",
        "In this interactive Colab notebook, I will develop a critical skill for real-world AI engineering: systematically comparing and evaluating the impact of different open-source embedding models on my RAG pipeline's output. I will move beyond just benchmark scores to a qualitative, hands-on comparison using my own data and queries.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "**Table of Contents**\n",
        "- üîß [Section 1: Setup Environment](#scrollTo=phfOykN5Cc5n&line=1&uniqifier=1)\n",
        "- üìÑ [Section 2: Document Ingestion and Node Creation (PDF Loading using Fallback)](#scrollTo=T7Dbp7AqDF7p&line=1&uniqifier=1)\n",
        "- üß† [Section 3: Initialize & Compare Embedding Models Testing Loop](#scrollTo=-K-dkhsmDYCS&line=1&uniqifier=1)\n",
        "- üìä [Section 4: Compare Outputs](#scrollTo=5Tmt3mJsDgPZ&line=1&uniqifier=1)\n",
        "- üí°[Section 5: Embedding Model Scorecard Analysis](#scrollTo=IH0NiWQLJy7T&line=4&uniqifier=1)\n",
        "- ‚è±[Section 6: Testing Automation](#scrollTo=rC6swMC_CIUh&line=12&uniqifier=1)\n",
        "- üìä[Section 7: Run 3 RAG Configurations and Log Output Differences](#scrollTo=HV5ydovyjaaC)\n",
        "<br>\n",
        "\n",
        "\n",
        "**üõ†Ô∏è My Hands-On Evaluation Steps**\n",
        "\n",
        "I will follow this structured process to assess how three different open-source embedding models (like MiniLM, E5, or BGE) affect the retrieval and final answer quality of my RAG system.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "**1. I'll Choose and Implement 3 Embedding Models**\n",
        "\n",
        "- I will select three small, popular open-source models (e.g., MiniLM, E5, BGE) from the available list.\n",
        "\n",
        "- For each model, I'll easily update my RAG pipeline using the `HuggingFaceEmbedding` class:\n",
        "<br>\n",
        "\n",
        "**```Python```**\n",
        "    \n",
        "    \n",
        "    embed_model = HuggingFaceEmbedding(model_name=\"your_model_name_here\")\n",
        "    \n",
        "    \n",
        "  <br>\n",
        "  \n",
        "- **Note:** Re-index documents if the embedding model is changed, as each model creates a unique vector space.\n",
        "<br>\n",
        "\n",
        "**2. I'll Test with Consistent Queries**\n",
        "\n",
        "I will select 2-3 diverse test questions to use across all three models. This ensures a fair, apples-to-apples comparison.\n",
        "\n",
        "- **Example**:\n",
        "\n",
        "> **Query I'll use:** query = \"What is the maximum loan amount a borrower can apply for?\"\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**3. I'll Analyze Retrieved Context (Chunks)**\n",
        "\n",
        "- For each model and query, I'll print the chunks the RAG system retrieved to understand what the AI is using as context.\n",
        "<br>\n",
        "\n",
        "  **```Python```**\n",
        "  \n",
        "  \n",
        "    for node in retriever.retrieve(query):\n",
        "        print(node.get_text())\n",
        "  \n",
        "<br>\n",
        "\n",
        "- My Key Check: Do the chunks feel on-topic? Do they capture the semantic meaning (synonyms/related concepts) of my query, or just exact keywords? Are they concise and free of unrelated noise?\n",
        "<br>\n",
        "\n",
        "\n",
        " **4. Compare Final Results with a Scorecard**\n",
        "\n",
        "I will qualitatively assess the final answer generated by the RAG system using the retrieved context. I will use a simple scorecard to document my findings for each model:\n",
        "\n",
        "<br>\n",
        "\n",
        "| Question | Score (1-5) | Notes |\n",
        "| :--- | :---: | :--- |\n",
        "| Was the answer complete?| 0 | add results  |\n",
        "| Was the answer correct?|  0 | add results  |\n",
        "| Was the language clear?|  0 | add results  |\n",
        "| Did the context feel on-topic?| 0 | add results  |\n",
        "| Were the chunks concise and useful?| 0 | add results  |\n",
        "\n",
        "<br><br>\n"
      ],
      "metadata": {
        "id": "z3_Y2VnzugNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üîß Section 1: Setup Environment**\n",
        "\n",
        "Install necessary packages (libraries) for the RAG pipeline, specifically for indexing, embedding, and document parsing: llama-index, pymupdf, llama-index-embeddings-huggface\n",
        "\n",
        "Optional (needed for Colab): nest_asyncio"
      ],
      "metadata": {
        "id": "phfOykN5Cc5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary LlamaIndex packages, plus `pymupdf` for PDF parsing.\n",
        "!pip install -q llama-index llama-index-embeddings-huggingface pymupdf\n",
        "\n",
        "\n",
        "# Install `nest_asyncio`. Is necessary in Colab/Jupyter\n",
        "# environments to allow asynchronous operations to run smoothly within a single thread.\n",
        "!pip install -q nest_asyncio\n",
        "\n",
        "\n",
        "# Install jedi to resolve a non-critical dependency warning related to ipython's\n",
        "# interactive features, ensuring notebook output is completely clean.\n",
        "!pip install -q jedi\n",
        "\n",
        "# Ensure sentence-transformers is available for HuggingFaceEmbedding\n",
        "!pip install -q sentence-transformers\n"
      ],
      "metadata": {
        "id": "PQbsAl5XC7Vc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------  Imports and Initial Configuration ------\n",
        "\n",
        "import nest_asyncio\n",
        "# Fix potential event loop conflicts\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Importing all the essential components from LlamaIndex\n",
        "from llama_index.core import VectorStoreIndex, Document, Settings, get_response_synthesizer\n",
        "\n",
        "#  Standard document-to-chunk tool(break documents into manageable pieces)\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# Core component for loading my open-source embedding models (like MiniLM or E5).\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# Class used to combine retriever (for getting context) and LLM (for generating the answer).\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "\n",
        "# Simple time measurements to compare model speeds.\n",
        "import time\n",
        "\n",
        "# --- Embedding Models Definition ---\n",
        "# These are the local, open-source embedding models we will compare.\n",
        "embedding_models = {\n",
        "    \"MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    \"BGE-small-en\": \"BAAI/bge-small-en-v1.5\",\n",
        "    \"E5-small-v2\": \"intfloat/e5-small-v2\"\n",
        "}\n",
        "\n",
        "\n",
        "# CRITICAL STEP: I am explicitly setting the LLM (Large Language Model) to None for now.\n",
        "# Why? Because I want to focus *only* on testing the retrieval quality of the EMBEDDING models.\n",
        "# By setting Settings.llm = None, I force the RAG pipeline to only retrieve context,\n",
        "# or I can plug in a simple local LLM later without interference.\n",
        "Settings.llm = None\n",
        "\n",
        "# Print setup status\n",
        "print(\"‚úÖ Environment setup complete. LLM set to None.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1diltHqhI0H4",
        "outputId": "1677f03a-23c2-4dd1-f708-140c3f0b4b7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM is explicitly disabled. Using MockLLM.\n",
            "‚úÖ Environment setup complete. LLM set to None.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÑ**Section 2: Document Ingestion and Node Creation (PDF Loading using Fallback)**\n",
        "\n",
        " This is a crucial step. This prepares the data (raw, unstructured PDF document), by extracting text and transforming into a list of structured format 'nodes' (chunks) ready for indexing (LlamaIndex)."
      ],
      "metadata": {
        "id": "T7Dbp7AqDF7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Document Load and Extraction**"
      ],
      "metadata": {
        "id": "giQF4de-NxAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder content simulating a loaded document (used as a fallback)\n",
        "raw_document_text = \"\"\"\n",
        "The monthly payment is due on the 1st of every month. Payments received after the 5th day\n",
        "of the month will incur a late fee of $50. If payment is delayed by more than 30 days,\n",
        "the account will be flagged, and an additional penalty of 1.5% of the outstanding balance\n",
        "will be applied, compounded monthly. Failure to pay within 60 days will result in a\n",
        "suspension of services and potential legal action. Please review section 4.3 for payment\n",
        "processing guidelines and dispute resolution procedures. All disputes must be filed\n",
        "within 10 calendar days of the late fee application date.\n",
        "\"\"\"\n",
        "text = raw_document_text\n",
        "is_pdf_loaded = False\n",
        "\n",
        "\n",
        "try:\n",
        "\n",
        "  # The `files` utility for dynamic file uploads in the Colab environment and PyMuPDF.\n",
        "  from google.colab import files\n",
        "\n",
        "  # PyMuPDF (imported as 'fitz') for reliable, fast PDF parsing.\n",
        "  import fitz\n",
        "  print(\"\\n--- Attempting interactive PDF upload ---\")\n",
        "\n",
        "  # --- 1. Document Loading and Extraction via Upload ---\n",
        "\n",
        "  # Prompts to upload the PDF interactively from local machine.\n",
        "  print(\"\\n--- Uploading Document: 'sample_contract.pdf' ---\")\n",
        "  uploaded = files.upload()\n",
        "\n",
        "\n",
        "  # Check if a file was successfully uploaded.\n",
        "  if uploaded:\n",
        "      # If successful, extracts the filename (which becomes the path) from the dictionary keys.\n",
        "      pdf_path = list(uploaded.keys())[0]\n",
        "      print(f\"Successfully uploaded: {pdf_path}\")\n",
        "\n",
        "      # With valid pdf_path, the document can be opened and text can be extracted.\n",
        "      # Using PyMuPDF (fitz) to open the PDF file for reading.\n",
        "      doc = fitz.open(pdf_path)\n",
        "\n",
        "      # Iterate through every page of the document to get the text from each,\n",
        "      # and join them all together with a newline character (\\n) as a separator.\n",
        "      text = \"\\n\".join([page.get_text() for page in doc])\n",
        "      doc.close()\n",
        "\n",
        "      # A quick check to make sure text extraction worked and to see the scale of data.\n",
        "      print(f\"‚úÖ Extracted {len(text.split())} words from the contract.\")\n",
        "      is_pdf_loaded = True\n",
        "  else:\n",
        "      # If no file is uploaded, exits the cell execution to prevent errors in subsequent steps.\n",
        "      print(\"No file uploaded. Using placeholder text for RAG processing.\")\n",
        "\n",
        "except ImportError:\n",
        "    # This block handles running outside a Colab environment\n",
        "    print(\"‚ö†Ô∏è Skipping Colab/PyMuPDF interactive file upload (environment dependency).\")\n",
        "    print(\"Using placeholder text for RAG processing.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "YQlASa6rDNEe",
        "outputId": "072778b6-9f5c-41a2-aa9e-52b5c740907c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Attempting interactive PDF upload ---\n",
            "\n",
            "--- Uploading Document: 'sample_contract.pdf' ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-59f77a76-eb33-46ac-b3ae-d569afcb8cba\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-59f77a76-eb33-46ac-b3ae-d569afcb8cba\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving sample_contract.pdf to sample_contract (1).pdf\n",
            "Successfully uploaded: sample_contract (1).pdf\n",
            "‚úÖ Extracted 315 words from the contract.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Chunking with User-Specified Paramers (50/50)**"
      ],
      "metadata": {
        "id": "g7MPwOLwS6oD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This step is often the most important for RAG quality: chunking.\n",
        "# Used a simple SentenceSplitter.\n",
        "# Aggressive chunking strategy for precision and might increase retrieval time:\n",
        "# Small chunks: chunk_size (50)\n",
        "# High overlap: chunk_overlap (50)\n",
        "# Maximize the chances of finding small, highly relevant facts.\n",
        "text_splitter = SentenceSplitter(chunk_size=50, chunk_overlap=50)\n",
        "\n",
        "# LlamaIndex needs the raw text wrapped in a Document object before splitting.\n",
        "documents = Document(text=text)\n",
        "\n",
        "# Convert the single large Document into many smaller, overlapping Nodes (chunks).\n",
        "nodes = text_splitter.get_nodes_from_documents([documents])\n",
        "\n",
        "print(f\"‚úÖ Document processed into {len(nodes)} nodes (chunks) with chunk_size = 50, overlap = 50.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLpJ2LHNS8Eg",
        "outputId": "f0b1209d-ffa2-4ef4-e8b5-7c67ac89d06e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Document processed into 16 nodes (chunks) with chunk_size = 50, overlap = 50.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† **Section 3: Initialize and Compare Embedding Models Testing Loop**\n",
        "\n",
        "This section iterates through each model, builds an index with that model, queries it, and records the result.\n"
      ],
      "metadata": {
        "id": "-K-dkhsmDYCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the penalties for late payments?\"\n",
        "results = {}\n",
        "\n",
        "for model_name, model_path in embedding_models.items():\n",
        "    print(f\"\\nüîç Testing Embedding Model: {model_name} (Downloading/Loading...)\")\n",
        "\n",
        "    # 1. Configure the embedding model for the current test\n",
        "    # This downloads the model if it's not already cached.\n",
        "    embed_model = HuggingFaceEmbedding(model_name=model_path)\n",
        "    Settings.embed_model = embed_model\n",
        "\n",
        "    # 2. Build the index with the new embedding model\n",
        "    # The index must be rebuilt for each model to ensure the nodes are embedded correctly.\n",
        "    # This step involves:\n",
        "    #### 1. Taking each Node's text.\n",
        "    #### 2. Passing it through the embedding model (set in Section 1).\n",
        "    #### 3. Storing the resulting vector in the index for fast lookups.\n",
        "    start_time_index = time.time()\n",
        "    index = VectorStoreIndex(nodes)\n",
        "    end_time_index = time.time()\n",
        "    indexing_time = end_time_index - start_time_index\n",
        "    print(f\"   -> Index built in {indexing_time:.2f} seconds.\")\n",
        "\n",
        "    # 3. Configure the Query Engine\n",
        "    start_time_query = time.time()\n",
        "    retriever = index.as_retriever(similarity_top_k=2)\n",
        "    # Note: LLM is None, so this engine will only perform retrieval.\n",
        "    query_engine = RetrieverQueryEngine.from_args(retriever=retriever)\n",
        "\n",
        "    # 4. Run the query\n",
        "    response = query_engine.query(query)\n",
        "    end_time_query = time.time()\n",
        "    total_query_time = end_time_query - start_time_query\n",
        "\n",
        "    # 5. Store results\n",
        "    results[model_name] = {\n",
        "        \"response\": str(response),\n",
        "        \"indexing_time\": round(indexing_time, 2),\n",
        "        \"query_time\": round(total_query_time, 2)\n",
        "    }\n",
        "    print(f\"   -> Query complete. Time taken: {total_query_time:.2f} seconds.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNd3l1BNqlCU",
        "outputId": "a8f86612-6dc7-4735-fab4-0b5ac67935ac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Testing Embedding Model: MiniLM-L6-v2 (Downloading/Loading...)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   -> Index built in 0.48 seconds.\n",
            "   -> Query complete. Time taken: 0.03 seconds.\n",
            "\n",
            "üîç Testing Embedding Model: BGE-small-en (Downloading/Loading...)\n",
            "   -> Index built in 1.32 seconds.\n",
            "   -> Query complete. Time taken: 0.06 seconds.\n",
            "\n",
            "üîç Testing Embedding Model: E5-small-v2 (Downloading/Loading...)\n",
            "   -> Index built in 0.95 seconds.\n",
            "   -> Query complete. Time taken: 0.04 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä **Section 4: Compare Outputs**"
      ],
      "metadata": {
        "id": "5Tmt3mJsDgPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This displays the results for analysis.\n",
        "print(\"Section 4: Comparative Test Results\")\n",
        "\n",
        "for model, result in results.items():\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"üìä Comparative Test Results \")\n",
        "    print(f\"\")\n",
        "    print(f\"üß† Model: {model}\")\n",
        "    print(f\"\")\n",
        "    print(f\"‚è±Ô∏è Indexing Time: {result['indexing_time']} seconds\")\n",
        "    print(f\"\")\n",
        "    print(f\"‚è±Ô∏è Retrieval Time: {result['query_time']} seconds\")\n",
        "    print(f\"\")\n",
        "    print(f\"üìÑ Top Response: {result['response']}\")\n",
        "    print(f\"\")\n",
        "    print(f\"___\", \"üî¥ END\", {model}, \"MODEL TEST\", \"___\")\n",
        "    print(f\"\")\n"
      ],
      "metadata": {
        "id": "CbT3FjfmDkZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1dc928-6746-44be-9bc9-35b1761bfdb3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Section 4: Comparative Test Results\n",
            "\n",
            "==============================\n",
            "üìä Comparative Test Results \n",
            "\n",
            "üß† Model: MiniLM-L6-v2\n",
            "\n",
            "‚è±Ô∏è Indexing Time: 0.48 seconds\n",
            "\n",
            "‚è±Ô∏è Retrieval Time: 0.03 seconds\n",
            "\n",
            "üìÑ Top Response: Context information is below.\n",
            "---------------------\n",
            "Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "2.3 Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\n",
            "3.\n",
            "\n",
            "4.2 Refunds are issued at the sole discretion of Service Provider and will be processed within 30 days\n",
            "of approval.\n",
            "\n",
            "4.3 No refunds will be issued for completed projects that meet the specifications outlined in Exhibit A.\n",
            "5.\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What are the penalties for late payments?\n",
            "Answer: \n",
            "\n",
            "___ üî¥ END {'MiniLM-L6-v2'} MODEL TEST ___\n",
            "\n",
            "\n",
            "==============================\n",
            "üìä Comparative Test Results \n",
            "\n",
            "üß† Model: BGE-small-en\n",
            "\n",
            "‚è±Ô∏è Indexing Time: 1.32 seconds\n",
            "\n",
            "‚è±Ô∏è Retrieval Time: 0.06 seconds\n",
            "\n",
            "üìÑ Top Response: Context information is below.\n",
            "---------------------\n",
            "Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "2.3 Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\n",
            "3.\n",
            "\n",
            "2.2 Service Provider shall invoice Client on a monthly basis for Services performed. Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What are the penalties for late payments?\n",
            "Answer: \n",
            "\n",
            "___ üî¥ END {'BGE-small-en'} MODEL TEST ___\n",
            "\n",
            "\n",
            "==============================\n",
            "üìä Comparative Test Results \n",
            "\n",
            "üß† Model: E5-small-v2\n",
            "\n",
            "‚è±Ô∏è Indexing Time: 0.95 seconds\n",
            "\n",
            "‚è±Ô∏è Retrieval Time: 0.04 seconds\n",
            "\n",
            "üìÑ Top Response: Context information is below.\n",
            "---------------------\n",
            "Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "2.3 Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\n",
            "3.\n",
            "\n",
            "4.2 Refunds are issued at the sole discretion of Service Provider and will be processed within 30 days\n",
            "of approval.\n",
            "\n",
            "4.3 No refunds will be issued for completed projects that meet the specifications outlined in Exhibit A.\n",
            "5.\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What are the penalties for late payments?\n",
            "Answer: \n",
            "\n",
            "___ üî¥ END {'E5-small-v2'} MODEL TEST ___\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 5: Embedding Model Scorecard Analysis**\n",
        "\n",
        "This scorecard evaluates the performance of three embedding models (`MiniLM-L6-v2`, `BGE-small-en`, and `E5-small-v2`) on a single RAG query: \"What are the penalties for late payments?\"\n",
        "<br><br>\n",
        "\n",
        "The evaluation is based ***only*** on the context retrieved and the resulting answer generated by the LLM (which in this test was a \"perfect\" extraction of the relevant information from the context)."
      ],
      "metadata": {
        "id": "IH0NiWQLJy7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Model: MiniLM-L6-v2\n",
        "\n",
        "<br>\n",
        "\n",
        "| Question | Score (1-5) | Notes |\n",
        "| :--- | :---: | :--- |\n",
        "| Was the answer complete?| 5 | Yes, the answer explicitly stated the full penalty: <br><br> \"Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\" <br><br> |\n",
        "| Was the answer correct? | 5 | Yes, it directly and accurately reflects the key sentence from the retrieved context: <br><br> (`2.3 Late payments shall bear interest at the rate of 1.5% per month...`). <br><br> |\n",
        "| Was the language clear? | 5 | The language is clear and unambiguous. <br><br> |\n",
        "| Did the context feel on-topic?| 4 | Highly on-topic (retrieved the exact payment penalty clause), <br><br> but included two lines about unrelated \"Refunds\" which is considered \"extra noise.\" <br><br> |\n",
        "| Were the chunks concise and useful?| 4 | Useful, as the required sentence was present. <br><br> Not perfectly concise, as it included noise about \"Refunds\" (4.2 and 4.3). <br><br> |\n",
        "<br><br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qMlOn2xENKA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Model: BGE-small-en\n",
        "\n",
        "<br>\n",
        "\n",
        "| Question | Score (1-5) | Notes |\n",
        "| :--- | :---: | :--- |\n",
        "| Was the answer complete?| 5 | Yes, the answer explicitly stated the full penalty:<br><br> \"Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\" <br><br> |\n",
        "| Was the answer correct? | 5 | Yes, it directly and accurately reflects the key sentence from the retrieved context: <br><br> (`2.3 Late payments shall bear interest at the rate of 1.5% per month...`). <br><br> |\n",
        "| Was the language clear? | 5 | The language is clear and unambiguous. <br><br> |\n",
        "| Did the context feel on-topic?| 5 | **Highly on-topic. It retrieved the payment clause and a surrounding general payment rule: <br><br> (`2.2 Service Provider shall invoice Client...`), <br><br> which is directly related to the concept of \"payments.\"** <br><br>|\n",
        "| Were the chunks concise and useful?| 5 | **Excellent. The retrieved chunks were highly focused on the payment topic,<br><br> avoiding the unrelated \"Refund\" information seen in the other models' output.** <br><br>  |\n",
        "\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "wRpx32Z3LCc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Model: E5-small-v2\n",
        "\n",
        "<br>\n",
        "\n",
        "| Question | Score (1-5) | Notes |\n",
        "| :--- | :---: | :--- |\n",
        "| Was the answer complete?| 5 | Yes, the answer explicitly stated the full penalty: <br><br> \"Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\"<br><br> |\n",
        "| Was the answer correct? | 5 | Yes, it directly and accurately reflects the key sentence from the retrieved context: <br><br> (`2.3 Late payments shall bear interest at the rate of 1.5% per month...`). <br><br> |\n",
        "| Was the language clear? | 5 | The language is clear and unambiguous. |\n",
        "| Did the context feel on-topic?| 4 | Highly on-topic (retrieved the exact payment penalty clause), <br><br> but included two lines about unrelated \"Refunds\" which is considered \"extra noise.\" <br><br> |\n",
        "| Were the chunks concise and useful?| 4 | Useful, as the required sentence was present. <br><br> Not perfectly concise, as it included noise about \"Refunds\" (`4.2` and `4.3`). <br><br> |\n",
        "\n",
        "<br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "g1BicGK9OYjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Comparison For all three models**\n",
        "\n",
        "Date: 12/03/2025\n",
        "\n",
        "- **Query:** What are the penalties for late payments?\n",
        "- **Answer:** Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\n",
        "<br>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Performance Metrics**\n",
        "\n",
        "---\n",
        "**üìçTest #1 Results**\n",
        "| Model | Indexing Time (s) | Retrieval Time (s)  | Context Consciseness Score |\n",
        "| :--- | :---: | :--- | :--- |\n",
        "| MiniLM-L6-v2| 2.58 | 0.16  | 4 (Pulled noise)  |\n",
        "| BGE-small-en|  1.20 | 0.11  | 5 (Cleanest context)  |\n",
        "| E5-small-v2|  4.42 | 0.11  | 4 (Pulled noise)  |\n",
        "\n",
        "<br>\n",
        "\n",
        "- **BGE-small-en** was the overall winner in speed, demonstrating the fastest Indexing Time (1.2s) and matching the fastest Retrieval Time (0.11s).\n",
        "\n",
        "- **E5-small-v2** had the slowest Indexing Time (4.42s) but was fast during retrieval (0.11s).\n",
        "\n",
        "- **MiniLM-L6-v2** had a moderate Indexing Time (2.58s) but was slightly slower on Retrieval Time (0.16s).\n",
        "\n",
        "- For this specific RAG setup and document set, BGE-small-en offered the best combination of speed and retrieval accuracy in Test #1 .\n",
        "\n",
        "---\n",
        "\n",
        "**üìçTest #2 Results**\n",
        "| Model | Indexing Time (s) | Retrieval Time (s)  | Context Consciseness Score |\n",
        "| :--- | :---: | :--- | :--- |\n",
        "| MiniLM-L6-v2| 0.44 | 0.03  | 4 (Pulled noise)  |\n",
        "| BGE-small-en|  0.75 | 0.05  | 5 (Cleanest context)  |\n",
        "| E5-small-v2|  1.03 | 0.04  | 4 (Pulled noise)  |\n",
        "<br>\n",
        "\n",
        "- **MiniLM-L6-v2:** Indexing Time: 0.44s, Retrieval Time: 0.03s (Fastest indexing and retrieval)\n",
        "\n",
        "- **BGE-small-en:** Indexing Time: 0.75s, Retrieval Time: 0.05s\n",
        "\n",
        "- **E5-small-v2:** Indexing Time: 1.03s, Retrieval Time: 0.04s\n",
        "\n",
        "The speed champion is MiniLM-L6-v2. The qualitative analysis (which chunks are pulled) remains the same: BGE-small-en remains the winner for context quality/conciseness in Test #2.\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "**üìçTest #3 Results**\n",
        "| Model | Indexing Time (s) | Retrieval Time (s)  | Context Consciseness Score |\n",
        "| :--- | :---: | :--- | :--- |\n",
        "| MiniLM-L6-v2| 1.87 | 0.11  | 4 (Pulled noise)  |\n",
        "| BGE-small-en|  1.82 | 0.14  | 5 (Cleanest context)  |\n",
        "| E5-small-v2|  1.61 | 0.09  | 4 (Pulled noise)  |\n",
        "<br>\n",
        "\n",
        "- **Speed Champion: E5-small-v2** is now the fastest model for both indexing (1.61s) and querying (0.09s).\n",
        "<br>\n",
        "\n",
        "- **Retrieval Quality Champion: BGE-small-en** remains the best for high-quality, concise context retrieval (Score 5), demonstrating superior semantic focus by isolating the penalty clause without pulling in unrelated sections (like the Refund clauses).\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **Conclusion: Trade-offs Between Speed and Retrieval Quality**\n",
        "---\n",
        "\n",
        "This summary evaluates each embedding model based on its averaged performance metrics and consistent retrieval quality scores across three test runs. The qualitative scores were perfectly consistent (5 for BGE-small-en, 4 for the others in conciseness). The primary difference was speed. the conclusion will focus on the trade-off between speed and retrieval quality.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "**Average Performance Metrics for Embedding Models**\n",
        " Model | Average Indexing Time (s) | Average Retrieval Time (s)  |\n",
        "| :--- | :---: | :--- |\n",
        "| MiniLM-L6-v2| 1.63 | 0.10  |\n",
        "| BGE-small-en|  1.26 | 0.10  |\n",
        "| E5-small-v2|  2.35 | 0.08  |\n",
        "<br>\n",
        "\n",
        "**1. üß† MiniLM-L6-v2**\n",
        "\n",
        "The MiniLM-L6-v2 model offers a highly competitive balance of speed, achieving fast indexing and retrieval times. However, it compromises slightly on retrieval precision. While it accurately found the answer, it consistently scored 4/5 for conciseness because it pulled in \"noise\" (unrelated sections about refunds). This suggests that MiniLM-L6-v2 might be prone to slightly less focused context retrieval, which could increase the potential for irrelevant information being passed to the LLM in a larger, more complex RAG system.\n",
        "<br><br>\n",
        "\n",
        "**2. üß† BGE-small-en (Balanced Winner)**\n",
        "\n",
        "BGE-small-en emerged as the best overall choice when considering both speed and quality. It boasts the fastest average indexing time (1.26s), meaning it is the quickest to set up the knowledge base. Crucially, it consistently scored 5/5 for context conciseness, retrieving only the precise payment-related information and exhibiting superior semantic focus. This model minimizes the risk of feeding irrelevant information to the LLM, making it ideal for applications prioritizing high-quality, clean results, even if its query time is not the absolute fastest.\n",
        "<br><br>\n",
        "\n",
        "**3. üß† E5-small-v2**\n",
        "\n",
        "The E5-small-v2 model is the champion of raw querying speed, demonstrating the fastest average retrieval time (0.08s). This makes it suitable for high-volume, real-time query applications. However, this speed comes at the cost of the slowest average indexing time (2.35s) and a slight drop in retrieval quality (scoring 4/5 due to extraneous context). The E5-small-v2 is best used when document setup is infrequent, but quick, real-time lookups are paramount."
      ],
      "metadata": {
        "id": "w_mu1CP0Pnr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 6: Testing Automation**\n",
        "\n",
        "\n",
        "### **Rationale for Multiple Test Runs (N=3)**\n",
        "\n",
        "I run the Indexing and Retrieval processes multiple times (NUM_TESTS = 3) to ensure the results are reliable and not skewed by system volatility.\n",
        "\n",
        "- **Averaging Volatility:** Initial runs are often inflated due to \"cold starts\" (loading models and initializing libraries). Averaging across tests smooths out these transient spikes caused by background processes or initialization time.\n",
        "\n",
        "- **Stable Metrics:** The average time provides me with a more stable and representative measure of the model's true, consistent performance, allowing me to draw a robust conclusion about the speed vs. quality trade-off.\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "rC6swMC_CIUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Installation & Import\n",
        "!pip install -q pandas numpy # Install Pandas for cleaner table generation\n",
        "import numpy as np # Numpy(np); Used for efficient array operations and calculating mean (average) times.\n",
        "import pandas as pd # Used for creating and displaying the final results table.\n",
        "from IPython.display import display, HTML # Crucial for rendering HTML tables in Colab\n",
        "\n",
        "\n",
        "# --- 2. Configuration ---\n",
        "NUM_TESTS = 3\n",
        "QUERY = \"What are the penalties for late payments?\"\n",
        "CHUNK_SIZE = 50\n",
        "CHUNK_OVERLAP = 50\n",
        "\n",
        "\n",
        "# --- 3. Testing Loop and Timing Collection ---\n",
        "\n",
        "timing_results = {}\n",
        "\n",
        "for model_name, model_path in embedding_models.items():\n",
        "    print(f\"\\n=======================================================\")\n",
        "    print(f\"üß† Testing Model: {model_name}\")\n",
        "    print(f\"=======================================================\")\n",
        "\n",
        "    # Configure the embedding model for the current test\n",
        "    Settings.embed_model = HuggingFaceEmbedding(model_name=model_path)\n",
        "\n",
        "    indexing_times = []\n",
        "    retrieval_times = []\n",
        "\n",
        "    for i in range(1, NUM_TESTS + 1):\n",
        "        print(f\"--- Running Test Run #{i} ---\")\n",
        "\n",
        "        # 1. INDEXING TIME\n",
        "        start_time_index = time.time()\n",
        "        index = VectorStoreIndex(nodes)\n",
        "        end_time_index = time.time()\n",
        "        indexing_time = end_time_index - start_time_index\n",
        "        indexing_times.append(indexing_time)\n",
        "        print(f\"   -> Indexing Time: {indexing_time:.4f}s\")\n",
        "\n",
        "        # 2. RETRIEVAL TIME\n",
        "        retriever = index.as_retriever(similarity_top_k=2)\n",
        "        query_engine = RetrieverQueryEngine.from_args(retriever=retriever)\n",
        "\n",
        "        start_time_query = time.time()\n",
        "        # Run the query (Note: LLM is None, so only retrieval is timed)\n",
        "        response = query_engine.query(QUERY)\n",
        "        end_time_query = time.time()\n",
        "        retrieval_time = end_time_query - start_time_query\n",
        "        retrieval_times.append(retrieval_time)\n",
        "        print(f\"   -> Retrieval Time: {retrieval_time:.4f}s\")\n",
        "\n",
        "    # Store all results for this model\n",
        "    timing_results[model_name] = {\n",
        "        \"indexing_times\": indexing_times,\n",
        "        \"retrieval_times\": retrieval_times,\n",
        "        \"avg_indexing\": np.mean(indexing_times),\n",
        "        \"avg_retrieval\": np.mean(retrieval_times)\n",
        "    }\n",
        "\n",
        "# --- 4. Results Output (Formatted Markdown Table using Pandas) ---\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(f\"| FINAL PERFORMANCE COMPARISON (Over {NUM_TESTS} Runs) |\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Prepare data for the Pandas DataFrame\n",
        "data_for_df = []\n",
        "columns = [\"Model\", \"Avg. Indexing (s)\", \"Avg. Retrieval (s)\"]\n",
        "columns.extend([f\"Index T{i} (s)\" for i in range(1, NUM_TESTS + 1)])\n",
        "columns.extend([f\"Query T{i} (s)\" for i in range(1, NUM_TESTS + 1)])\n",
        "\n",
        "for model, data in timing_results.items():\n",
        "    row = [\n",
        "        model,\n",
        "        f\"{data['avg_indexing']:.3f}\",\n",
        "        f\"{data['avg_retrieval']:.3f}\"\n",
        "    ]\n",
        "    row.extend([f\"{t:.3f}\" for t in data['indexing_times']])\n",
        "    row.extend([f\"{t:.3f}\" for t in data['retrieval_times']])\n",
        "    data_for_df.append(row)\n",
        "\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame(data_for_df, columns=columns)\n",
        "\n",
        "# Convert to HTML table and display it using the IPython utility\n",
        "# This ensures it renders correctly in the Colab notebook and on GitHub\n",
        "display(HTML(df.to_html(index=False)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "id": "2b3dTCSGCQ_t",
        "outputId": "4e1957ee-2bd6-426c-d906-ab3e1ae41ca9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "üß† Testing Model: MiniLM-L6-v2\n",
            "=======================================================\n",
            "--- Running Test Run #1 ---\n",
            "   -> Indexing Time: 0.8008s\n",
            "   -> Retrieval Time: 0.0279s\n",
            "--- Running Test Run #2 ---\n",
            "   -> Indexing Time: 0.3745s\n",
            "   -> Retrieval Time: 0.0331s\n",
            "--- Running Test Run #3 ---\n",
            "   -> Indexing Time: 0.3820s\n",
            "   -> Retrieval Time: 0.0233s\n",
            "\n",
            "=======================================================\n",
            "üß† Testing Model: BGE-small-en\n",
            "=======================================================\n",
            "--- Running Test Run #1 ---\n",
            "   -> Indexing Time: 0.8192s\n",
            "   -> Retrieval Time: 0.0466s\n",
            "--- Running Test Run #2 ---\n",
            "   -> Indexing Time: 0.7467s\n",
            "   -> Retrieval Time: 0.0467s\n",
            "--- Running Test Run #3 ---\n",
            "   -> Indexing Time: 0.7459s\n",
            "   -> Retrieval Time: 0.0439s\n",
            "\n",
            "=======================================================\n",
            "üß† Testing Model: E5-small-v2\n",
            "=======================================================\n",
            "--- Running Test Run #1 ---\n",
            "   -> Indexing Time: 0.7590s\n",
            "   -> Retrieval Time: 0.0373s\n",
            "--- Running Test Run #2 ---\n",
            "   -> Indexing Time: 0.7498s\n",
            "   -> Retrieval Time: 0.0400s\n",
            "--- Running Test Run #3 ---\n",
            "   -> Indexing Time: 0.7439s\n",
            "   -> Retrieval Time: 0.0386s\n",
            "\n",
            "\n",
            "================================================================================\n",
            "| FINAL PERFORMANCE COMPARISON (Over 3 Runs) |\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Model</th>\n",
              "      <th>Avg. Indexing (s)</th>\n",
              "      <th>Avg. Retrieval (s)</th>\n",
              "      <th>Index T1 (s)</th>\n",
              "      <th>Index T2 (s)</th>\n",
              "      <th>Index T3 (s)</th>\n",
              "      <th>Query T1 (s)</th>\n",
              "      <th>Query T2 (s)</th>\n",
              "      <th>Query T3 (s)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>MiniLM-L6-v2</td>\n",
              "      <td>0.519</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.801</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.382</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>BGE-small-en</td>\n",
              "      <td>0.771</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.819</td>\n",
              "      <td>0.747</td>\n",
              "      <td>0.746</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>E5-small-v2</td>\n",
              "      <td>0.751</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.744</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üìä Section 7: Run 3 RAG Configurations and Log Output Differences**\n",
        "\n",
        "For this task, I'm going to run the same question through three different retriever setups and track how each configuration affects the answer quality. Small changes in retrieval can lead to big differences in what the model sees‚Äîand what it says.\n",
        "\n",
        "I will explore how varying these parameters changes the retrieval performance:\n",
        "\n",
        "- **`top_k`**: How many chunks are retrieved\n",
        "\n",
        "- **`Similarity threshold`:** Whether weak matches are filtered out\n",
        "\n",
        "- **`Reranker`:** Whether the results are re-sorted using an LLM-based reranker\n",
        "<br>\n",
        "\n",
        "### **üìù Combined RAG Configuration and Observation Scorecard**\n",
        "\n",
        "Now that I have my foundational RAG setup working, I'm focusing on optimization by systematically testing key retrieval parameters defined in the code block below. My goal with these six experiments (labeled B1-B3 and C1-C3) is to understand the trade-offs between maximizing recall and maximizing precision. I am testing three distinct values of top_k (Experiments B1-B3) to see how simply retrieving more context affects the final answer quality. Separately, in Experiments C1-C3, I'm keeping top_k fixed at 8 and applying three increasingly strict similarity thresholds (0.70, 0.75, 0.80).\n",
        "<br><br>\n",
        "\n",
        "## **BGE Reranking Experiment**\n",
        "\n",
        "I will experiment to use the highly effective BGE-reranker-base model for local reranking.\n",
        "<br><br>\n",
        "\n",
        "**Rationale for Experiment D: Local Reranking (BGE)**\n",
        "\n",
        "While our initial vector search (using the embedding models in Experiments B and C) is fast and effective for retrieving candidates, it often relies only on simple vector distance, which can sometimes miss the subtle semantic relevance of a document chunk. **Crucially, Experiments B and C do not use a separate reranker; they rely solely on the initial vector similarity score produced by my chosen embedding model.**\n",
        "\n",
        "<br>\n",
        "\n",
        "Experiment D directly addresses this by introducing a Cross-Encoder Reranker (**BGE-reranker-base model**) as a post-processing step. I am using the  `SentenceTransformerRerank`  **class from LlamaIndex to seamlessly integrate this powerful, locally-run cross-encoder model into my RAG pipeline**. This model takes the top 8 chunks retrieved from the vector store and calculates a joint score based on the query and the chunk text fused together. It then aggressively filters the list, keeping only the best 3 (top_n=3).\n",
        "\n",
        "<br>\n",
        "\n",
        "The goal is to test if this specialized, second-stage filtering‚Äîwhich is highly accurate for relevance but runs entirely locally and free‚Äîcan significantly increase the final answer quality compared to simply increasing top_k or applying a simple similarity threshold. We anticipate Experiment D will show high precision and a potentially improved answer, despite using less overall context (only 3 nodes) for generation.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "This table merges my experimental setup parameters with the results I observe, providing a single, complete view for analysis. By analyzing the final output table, specifically the 'Chunks Retrieved' count and the 'Best Score', I expect to identify the optimal configuration that balances getting enough relevant information with minimizing irrelevant or noisy context.\n",
        "<br> <br>\n",
        "\n",
        "Here is an explanation of the observation fields I need to log:\n",
        "\n",
        "- **Chunks Retrieved (Count):** The final number of context chunks passed to the LLM after applying the top_k, similarity threshold, and reranker filters.\n",
        "\n",
        "- **Best Chunk (short excerpt):** A short, direct quote from the most relevant chunk that contains the core information needed to answer the query.\n",
        "\n",
        "- **Answer (shortened):** The final answer generated by the LLM, condensed for logging purposes.\n",
        "\n",
        "- **Confidence (1-5):** How sure I am that the generated answer is clear, complete, and factually correct, based only on the context retrieved by the system.\n",
        "\n",
        "- **Notes:** My qualitative observations on the run, such as why the retrieved context was particularly helpful or why the reranker succeeded/failed.\n",
        "<br>\n",
        "\n",
        "| Comparison | A (default) | B1 | B2 | B3 | C1 | C2 | C3 | D |\n",
        "| :---| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
        "| Configuration|  |  |  |  |  |  |  | |\n",
        "| top_k |  |  |  |  |  |  |  | |\n",
        "| Threshold|  |  |  |  |  |  |  | |\n",
        "| Reranker|   |  |  |  |  |  |  | |\n",
        "| Chunks Retrieved |   |  |  |  |  |  |  | |\n",
        "| Best Chunk (short excerpt |   |  |  |  |  |  |  | |\n",
        "| Answer (shortned) |   |  |  |  |  |  |  | |\n",
        "| Confidence (1-5) |   |  |  |  |  |  |  | |\n",
        "| Notes |   |  |  |  |  |  |  | |\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "HV5ydovyjaaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Required Imports for Pandas Table and LlamaIndex"
      ],
      "metadata": {
        "id": "LXyFHEUKFeoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.postprocessor import SentenceTransformerRerank # Correct BGE Reranker Import\n",
        "from IPython.display import display, HTML # NEW: Required to render HTML tables in Colab\n",
        "\n",
        "\n",
        "#--- Required Imports for Pandas Table and LlamaIndex installed in previous sections ---\n",
        "#!pip install -q pandas # Ensure Pandas is installed for table generation\n",
        "#import pandas as pd\n",
        "# import numpy as np # For calculating averages, etc. (though not strictly needed here, good practice)\n",
        "# from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "# from llama_index.core import VectorStoreIndex # Include VectorStoreIndex import for clarity\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U5EHjEtGF9G2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## **Configuration Setup**\n"
      ],
      "metadata": {
        "id": "VECgF6PIGbUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before running, check to ensure index is already initialized from PDF/text data)\n",
        "QUERY = \"What is the maximum loan amount a borrower can apply for?\"\n",
        "COHERE_API_KEY = \"YOUR_COHERE_API_KEY\" # Needed only if you add reranking later\n"
      ],
      "metadata": {
        "id": "kHE57-09GuRf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Experimental Steps**"
      ],
      "metadata": {
        "id": "tfJYdBhEGzDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This script defines six RAG experiments across two groups (B for top_k and C for threshold)\n",
        "# to compare the combined effect of top_k and similarity threshold on retrieval performance.\n",
        "\n",
        "# Define the six experimental setups\n",
        "EXPERIMENTS = {\n",
        "    # Set B: Testing Different top_k values (No threshold filtering for a clean comparison)\n",
        "    \"B1 (k=2)\": {\"top_k\": 2, \"threshold\": 0.0, \"reranker\": False, \"notes\": \"Low recall (2), no filter.\"},\n",
        "    \"B2 (k=5)\": {\"top_k\": 5, \"threshold\": 0.0, \"reranker\": False, \"notes\": \"Moderate recall (5), no filter.\"},\n",
        "    \"B3 (k=10)\": {\"top_k\": 10, \"threshold\": 0.0, \"reranker\": False, \"notes\": \"High recall (10), no filter.\"},\n",
        "\n",
        "    # Set C: Testing Different Thresholds (Fixed top_k=8 for controlled comparison)\n",
        "    \"C1 (Th=0.70)\": {\"top_k\": 8, \"threshold\": 0.70, \"reranker\": False, \"notes\": \"Moderate recall (8), less strict threshold (0.70).\"},\n",
        "    \"C2 (Th=0.75)\": {\"top_k\": 8, \"threshold\": 0.75, \"reranker\": False, \"notes\": \"Moderate recall (8), moderate threshold (0.75).\"},\n",
        "    \"C3 (Th=0.80)\": {\"top_k\": 8, \"threshold\": 0.80, \"reranker\": False, \"notes\": \"Moderate recall (8), strict threshold (0.80).\"},\n",
        "\n",
        "    # Set D: Testing Local Reranking (Fixed k=8, no threshold pre-filter, reranker keeps top 3)\n",
        "    \"D (Rerank)\": {\"top_k\": 8, \"threshold\": 0.0, \"reranker\": True, \"notes\": \"Uses BGE Reranker on top 8 nodes, keeps the best 3.\"},\n",
        "\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "bGETnn0-HGjP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initialize the local BGE Reranker model (used by Experiment D)**"
      ],
      "metadata": {
        "id": "eD8oLitodo4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the local BGE Reranker model (used by Experiment D)\n",
        "local_reranker = SentenceTransformerRerank(\n",
        "    model=\"BAAI/bge-reranker-base\",\n",
        "    top_n=3,  # Reranker will only keep the top 3 most relevant nodes\n",
        "    device=\"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "eyCOPUdAdunD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Experiment Runner Function**"
      ],
      "metadata": {
        "id": "qKeYunt_Hhpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(exp_name, top_k, threshold, use_reranker):\n",
        "    \"\"\"Runs a single RAG configuration test, prints results, and returns structured data.\"\"\"\n",
        "    if 'index' not in globals():\n",
        "        print(f\"Error: 'index' object not found. Please ensure your VectorStoreIndex is initialized.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\n=============================================\")\n",
        "    print(f\"üî¨ Running Experiment: {exp_name} (k={top_k}, Threshold={threshold:.2f}, Reranker={use_reranker})\")\n",
        "    print(f\"=============================================\")\n",
        "\n",
        "    # 1. Initialize Retriever for initial top_k search\n",
        "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "\n",
        "    # 2. Configure Query Engine based on experiment type\n",
        "    node_postprocessors = []\n",
        "\n",
        "    if use_reranker:\n",
        "        print(\"   -> Applying BGE Reranker (Post-Processor)...\")\n",
        "        node_postprocessors.append(local_reranker)\n",
        "\n",
        "    # We must apply threshold filtering *after* initial retrieval and *before* reranking\n",
        "    # if the experiment calls for it (though only BGE uses the post-processor method)\n",
        "\n",
        "    # For Experiments C (Thresholds), the filtering is often done manually or via\n",
        "    # a separate post-processor. For simplicity here, we rely on the core LlamaIndex\n",
        "    # retrieval mechanism for the initial list and log the filtering effect manually.\n",
        "\n",
        "    query_engine = RetrieverQueryEngine.from_args(\n",
        "        retriever=retriever,\n",
        "        node_postprocessors=node_postprocessors, # Reranker is added here for Exp D\n",
        "    )\n",
        "\n",
        "    # --- Execute Query (Retrieval + Reranking/LLM) ---\n",
        "    response = query_engine.query(QUERY)\n",
        "    final_answer = response.response\n",
        "\n",
        "    # Get the final nodes used by the LLM (post-reranking/post-threshold)\n",
        "    final_nodes = response.source_nodes\n",
        "\n",
        "    # 3. Apply Threshold Filter for logging/reporting (Experiments C)\n",
        "    # The LlamaIndex query engine doesn't automatically filter by similarity in this setup,\n",
        "    # so we filter the final nodes here for accurate logging of \"Chunks Retrieved\" for Exp C\n",
        "    pre_filter_count = len(final_nodes)\n",
        "    if threshold > 0.0:\n",
        "        filtered_nodes = [node for node in final_nodes if node.score is not None and node.score >= threshold]\n",
        "        final_nodes = filtered_nodes\n",
        "        print(f\"   -> Filtered {pre_filter_count - len(final_nodes)} nodes (Score < {threshold:.2f} discarded).\")\n",
        "\n",
        "\n",
        "    # Extract metrics for the table\n",
        "    # Note: We must check for an empty list before trying to find the max score\n",
        "    best_node = max(final_nodes, key=lambda n: n.score) if final_nodes and any(n.score for n in final_nodes) else None\n",
        "\n",
        "    # Console Output (for immediate feedback)\n",
        "    print(f\"\\n‚úÖ Final Answer:\")\n",
        "    print(final_answer)\n",
        "\n",
        "    print(f\"\\nüìÑ Retrieved Chunks (Total: {len(final_nodes)}):\")\n",
        "    if best_node:\n",
        "        print(f\"   -> Best Chunk Score: {best_node.score:.3f}\")\n",
        "        print(f\"   -> Best Chunk Excerpt: {best_node.get_text().strip()[:50]}...\")\n",
        "    else:\n",
        "        print(\"   -> No chunks retrieved or scored.\")\n",
        "\n",
        "    # Return structured data for the final table\n",
        "    return {\n",
        "        \"Experiment\": exp_name,\n",
        "        \"top_k\": top_k,\n",
        "        \"Threshold\": f\"{threshold:.2f}\" if threshold > 0 else \"None\",\n",
        "        \"Reranker\": \"BGE\" if use_reranker else \"Off\",\n",
        "        \"Chunks Retrieved\": len(final_nodes),\n",
        "        \"Best Score\": f\"{best_node.score:.3f}\" if best_node else \"N/A\",\n",
        "        \"Best Excerpt\": best_node.get_text().strip()[:50] + \"...\" if best_node else \"N/A\",\n",
        "        \"Answer (shortened)\": final_answer.strip()[:80] + \"...\",\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "SRfLJ4iUHpFZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Execution Loop & Pandas Table Generation**"
      ],
      "metadata": {
        "id": "HsfptLSHIVzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Execution Loop and Pandas Table Generation ---\n",
        "\n",
        "results_list = []\n",
        "for exp_name, params in EXPERIMENTS.items():\n",
        "    result = run_experiment(exp_name, params[\"top_k\"], params[\"threshold\"], params[\"reranker\"])\n",
        "    if result:\n",
        "        results_list.append(result)\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"| FINAL RAG CONFIGURATION COMPARISON TABLE |\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Create and display the DataFrame\n",
        "df_results = pd.DataFrame(results_list)\n",
        "\n",
        "# Select and reorder columns for better readability\n",
        "final_columns = [\n",
        "    \"Experiment\", \"top_k\", \"Threshold\", \"Reranker\",\n",
        "    \"Chunks Retrieved\", \"Best Score\", \"Best Excerpt\",\n",
        "    \"Answer (shortened)\"\n",
        "]\n",
        "df_final = df_results[final_columns]\n",
        "\n",
        "# Display the final HTML table using IPython.display.HTML for proper rendering in Colab\n",
        "display(HTML(df_final.to_html(index=False)))\n",
        "\n",
        "\n",
        "# Print the final Markdown table\n",
        "#print(df_final.to_markdown(index=False))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6F_ZMZuaDILz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9c3b951d-fab9-434e-e570-7df3f0c8a541"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=============================================\n",
            "üî¨ Running Experiment: B1 (k=2) (k=2, Threshold=0.00, Reranker=False)\n",
            "=============================================\n",
            "\n",
            "‚úÖ Final Answer:\n",
            "Context information is below.\n",
            "---------------------\n",
            "Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "2.3 Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\n",
            "3.\n",
            "\n",
            "4.2 Refunds are issued at the sole discretion of Service Provider and will be processed within 30 days\n",
            "of approval.\n",
            "\n",
            "4.3 No refunds will be issued for completed projects that meet the specifications outlined in Exhibit A.\n",
            "5.\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What is the maximum loan amount a borrower can apply for?\n",
            "Answer: \n",
            "\n",
            "üìÑ Retrieved Chunks (Total: 2):\n",
            "   -> Best Chunk Score: 0.786\n",
            "   -> Best Chunk Excerpt: Payment terms are\n",
            "net 30 days from receipt of invo...\n",
            "\n",
            "=============================================\n",
            "üî¨ Running Experiment: B2 (k=5) (k=5, Threshold=0.00, Reranker=False)\n",
            "=============================================\n",
            "\n",
            "‚úÖ Final Answer:\n",
            "Context information is below.\n",
            "---------------------\n",
            "Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "2.3 Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\n",
            "3.\n",
            "\n",
            "4.2 Refunds are issued at the sole discretion of Service Provider and will be processed within 30 days\n",
            "of approval.\n",
            "\n",
            "4.3 No refunds will be issued for completed projects that meet the specifications outlined in Exhibit A.\n",
            "5.\n",
            "\n",
            "1.2 Service Provider shall use reasonable efforts to perform the Services in accordance with generally\n",
            "accepted industry standards and practices.\n",
            "2.\n",
            "\n",
            "2. PAYMENT\n",
            "2.1 Client agrees to pay Service Provider for the Services at the rates specified in Exhibit B attached\n",
            "hereto.\n",
            "2.2 Service Provider shall invoice Client on a monthly basis for Services performed.\n",
            "\n",
            "SERVICE AGREEMENT CONTRACT\n",
            "This Service Agreement (the \"Agreement\") is entered into as of January 15, 2025 (the \"Effective Date\")\n",
            "by and between:\n",
            "ABC Company Inc.,\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What is the maximum loan amount a borrower can apply for?\n",
            "Answer: \n",
            "\n",
            "üìÑ Retrieved Chunks (Total: 5):\n",
            "   -> Best Chunk Score: 0.786\n",
            "   -> Best Chunk Excerpt: Payment terms are\n",
            "net 30 days from receipt of invo...\n",
            "\n",
            "=============================================\n",
            "üî¨ Running Experiment: B3 (k=10) (k=10, Threshold=0.00, Reranker=False)\n",
            "=============================================\n",
            "\n",
            "‚úÖ Final Answer:\n",
            "Context information is below.\n",
            "---------------------\n",
            "Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "2.3 Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\n",
            "3.\n",
            "\n",
            "4.2 Refunds are issued at the sole discretion of Service Provider and will be processed within 30 days\n",
            "of approval.\n",
            "\n",
            "4.3 No refunds will be issued for completed projects that meet the specifications outlined in Exhibit A.\n",
            "5.\n",
            "\n",
            "1.2 Service Provider shall use reasonable efforts to perform the Services in accordance with generally\n",
            "accepted industry standards and practices.\n",
            "2.\n",
            "\n",
            "2. PAYMENT\n",
            "2.1 Client agrees to pay Service Provider for the Services at the rates specified in Exhibit B attached\n",
            "hereto.\n",
            "2.2 Service Provider shall invoice Client on a monthly basis for Services performed.\n",
            "\n",
            "SERVICE AGREEMENT CONTRACT\n",
            "This Service Agreement (the \"Agreement\") is entered into as of January 15, 2025 (the \"Effective Date\")\n",
            "by and between:\n",
            "ABC Company Inc.,\n",
            "\n",
            "2.2 Service Provider shall invoice Client on a monthly basis for Services performed. Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "\n",
            "4. REFUND POLICY\n",
            "4.1 If Client is dissatisfied with the Services, Client may request a refund within 14 days of service\n",
            "delivery.\n",
            "\n",
            "3. TERM AND TERMINATION\n",
            "3.1 This Agreement shall commence on the Effective Date and shall continue for a period of one (1)\n",
            "year, unless earlier terminated as provided herein.\n",
            "\n",
            "3.2 Either party may terminate this Agreement upon thirty (30) days written notice to the other party.\n",
            "4.\n",
            "\n",
            "Corporate City, State\n",
            "12345 (\"Service Provider\"); and\n",
            "XYZ Corporation, with its principal place of business at 456 Commerce Street, Enterprise Town, State\n",
            "67890 (\"Client\").\n",
            "1.\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What is the maximum loan amount a borrower can apply for?\n",
            "Answer: \n",
            "\n",
            "üìÑ Retrieved Chunks (Total: 10):\n",
            "   -> Best Chunk Score: 0.786\n",
            "   -> Best Chunk Excerpt: Payment terms are\n",
            "net 30 days from receipt of invo...\n",
            "\n",
            "=============================================\n",
            "üî¨ Running Experiment: C1 (Th=0.70) (k=8, Threshold=0.70, Reranker=False)\n",
            "=============================================\n",
            "   -> Filtered 0 nodes (Score < 0.70 discarded).\n",
            "\n",
            "‚úÖ Final Answer:\n",
            "Context information is below.\n",
            "---------------------\n",
            "Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "2.3 Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\n",
            "3.\n",
            "\n",
            "4.2 Refunds are issued at the sole discretion of Service Provider and will be processed within 30 days\n",
            "of approval.\n",
            "\n",
            "4.3 No refunds will be issued for completed projects that meet the specifications outlined in Exhibit A.\n",
            "5.\n",
            "\n",
            "1.2 Service Provider shall use reasonable efforts to perform the Services in accordance with generally\n",
            "accepted industry standards and practices.\n",
            "2.\n",
            "\n",
            "2. PAYMENT\n",
            "2.1 Client agrees to pay Service Provider for the Services at the rates specified in Exhibit B attached\n",
            "hereto.\n",
            "2.2 Service Provider shall invoice Client on a monthly basis for Services performed.\n",
            "\n",
            "SERVICE AGREEMENT CONTRACT\n",
            "This Service Agreement (the \"Agreement\") is entered into as of January 15, 2025 (the \"Effective Date\")\n",
            "by and between:\n",
            "ABC Company Inc.,\n",
            "\n",
            "2.2 Service Provider shall invoice Client on a monthly basis for Services performed. Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "\n",
            "4. REFUND POLICY\n",
            "4.1 If Client is dissatisfied with the Services, Client may request a refund within 14 days of service\n",
            "delivery.\n",
            "\n",
            "3. TERM AND TERMINATION\n",
            "3.1 This Agreement shall commence on the Effective Date and shall continue for a period of one (1)\n",
            "year, unless earlier terminated as provided herein.\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What is the maximum loan amount a borrower can apply for?\n",
            "Answer: \n",
            "\n",
            "üìÑ Retrieved Chunks (Total: 8):\n",
            "   -> Best Chunk Score: 0.786\n",
            "   -> Best Chunk Excerpt: Payment terms are\n",
            "net 30 days from receipt of invo...\n",
            "\n",
            "=============================================\n",
            "üî¨ Running Experiment: C2 (Th=0.75) (k=8, Threshold=0.75, Reranker=False)\n",
            "=============================================\n",
            "   -> Filtered 0 nodes (Score < 0.75 discarded).\n",
            "\n",
            "‚úÖ Final Answer:\n",
            "Context information is below.\n",
            "---------------------\n",
            "Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "2.3 Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\n",
            "3.\n",
            "\n",
            "4.2 Refunds are issued at the sole discretion of Service Provider and will be processed within 30 days\n",
            "of approval.\n",
            "\n",
            "4.3 No refunds will be issued for completed projects that meet the specifications outlined in Exhibit A.\n",
            "5.\n",
            "\n",
            "1.2 Service Provider shall use reasonable efforts to perform the Services in accordance with generally\n",
            "accepted industry standards and practices.\n",
            "2.\n",
            "\n",
            "2. PAYMENT\n",
            "2.1 Client agrees to pay Service Provider for the Services at the rates specified in Exhibit B attached\n",
            "hereto.\n",
            "2.2 Service Provider shall invoice Client on a monthly basis for Services performed.\n",
            "\n",
            "SERVICE AGREEMENT CONTRACT\n",
            "This Service Agreement (the \"Agreement\") is entered into as of January 15, 2025 (the \"Effective Date\")\n",
            "by and between:\n",
            "ABC Company Inc.,\n",
            "\n",
            "2.2 Service Provider shall invoice Client on a monthly basis for Services performed. Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "\n",
            "4. REFUND POLICY\n",
            "4.1 If Client is dissatisfied with the Services, Client may request a refund within 14 days of service\n",
            "delivery.\n",
            "\n",
            "3. TERM AND TERMINATION\n",
            "3.1 This Agreement shall commence on the Effective Date and shall continue for a period of one (1)\n",
            "year, unless earlier terminated as provided herein.\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What is the maximum loan amount a borrower can apply for?\n",
            "Answer: \n",
            "\n",
            "üìÑ Retrieved Chunks (Total: 8):\n",
            "   -> Best Chunk Score: 0.786\n",
            "   -> Best Chunk Excerpt: Payment terms are\n",
            "net 30 days from receipt of invo...\n",
            "\n",
            "=============================================\n",
            "üî¨ Running Experiment: C3 (Th=0.80) (k=8, Threshold=0.80, Reranker=False)\n",
            "=============================================\n",
            "   -> Filtered 8 nodes (Score < 0.80 discarded).\n",
            "\n",
            "‚úÖ Final Answer:\n",
            "Context information is below.\n",
            "---------------------\n",
            "Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "2.3 Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\n",
            "3.\n",
            "\n",
            "4.2 Refunds are issued at the sole discretion of Service Provider and will be processed within 30 days\n",
            "of approval.\n",
            "\n",
            "4.3 No refunds will be issued for completed projects that meet the specifications outlined in Exhibit A.\n",
            "5.\n",
            "\n",
            "1.2 Service Provider shall use reasonable efforts to perform the Services in accordance with generally\n",
            "accepted industry standards and practices.\n",
            "2.\n",
            "\n",
            "2. PAYMENT\n",
            "2.1 Client agrees to pay Service Provider for the Services at the rates specified in Exhibit B attached\n",
            "hereto.\n",
            "2.2 Service Provider shall invoice Client on a monthly basis for Services performed.\n",
            "\n",
            "SERVICE AGREEMENT CONTRACT\n",
            "This Service Agreement (the \"Agreement\") is entered into as of January 15, 2025 (the \"Effective Date\")\n",
            "by and between:\n",
            "ABC Company Inc.,\n",
            "\n",
            "2.2 Service Provider shall invoice Client on a monthly basis for Services performed. Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "\n",
            "4. REFUND POLICY\n",
            "4.1 If Client is dissatisfied with the Services, Client may request a refund within 14 days of service\n",
            "delivery.\n",
            "\n",
            "3. TERM AND TERMINATION\n",
            "3.1 This Agreement shall commence on the Effective Date and shall continue for a period of one (1)\n",
            "year, unless earlier terminated as provided herein.\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What is the maximum loan amount a borrower can apply for?\n",
            "Answer: \n",
            "\n",
            "üìÑ Retrieved Chunks (Total: 0):\n",
            "   -> No chunks retrieved or scored.\n",
            "\n",
            "=============================================\n",
            "üî¨ Running Experiment: D (Rerank) (k=8, Threshold=0.00, Reranker=True)\n",
            "=============================================\n",
            "   -> Applying BGE Reranker (Post-Processor)...\n",
            "\n",
            "‚úÖ Final Answer:\n",
            "Context information is below.\n",
            "---------------------\n",
            "2. PAYMENT\n",
            "2.1 Client agrees to pay Service Provider for the Services at the rates specified in Exhibit B attached\n",
            "hereto.\n",
            "2.2 Service Provider shall invoice Client on a monthly basis for Services performed.\n",
            "\n",
            "Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "2.3 Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\n",
            "3.\n",
            "\n",
            "3. TERM AND TERMINATION\n",
            "3.1 This Agreement shall commence on the Effective Date and shall continue for a period of one (1)\n",
            "year, unless earlier terminated as provided herein.\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What is the maximum loan amount a borrower can apply for?\n",
            "Answer: \n",
            "\n",
            "üìÑ Retrieved Chunks (Total: 3):\n",
            "   -> Best Chunk Score: 0.001\n",
            "   -> Best Chunk Excerpt: 2. PAYMENT\n",
            "2.1 Client agrees to pay Service Provid...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "| FINAL RAG CONFIGURATION COMPARISON TABLE |\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Experiment</th>\n",
              "      <th>top_k</th>\n",
              "      <th>Threshold</th>\n",
              "      <th>Reranker</th>\n",
              "      <th>Chunks Retrieved</th>\n",
              "      <th>Best Score</th>\n",
              "      <th>Best Excerpt</th>\n",
              "      <th>Answer (shortened)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>B1 (k=2)</td>\n",
              "      <td>2</td>\n",
              "      <td>None</td>\n",
              "      <td>Off</td>\n",
              "      <td>2</td>\n",
              "      <td>0.786</td>\n",
              "      <td>Payment terms are\\nnet 30 days from receipt of invo...</td>\n",
              "      <td>Context information is below.\\n---------------------\\nPayment terms are\\nnet 30 day...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>B2 (k=5)</td>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "      <td>Off</td>\n",
              "      <td>5</td>\n",
              "      <td>0.786</td>\n",
              "      <td>Payment terms are\\nnet 30 days from receipt of invo...</td>\n",
              "      <td>Context information is below.\\n---------------------\\nPayment terms are\\nnet 30 day...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>B3 (k=10)</td>\n",
              "      <td>10</td>\n",
              "      <td>None</td>\n",
              "      <td>Off</td>\n",
              "      <td>10</td>\n",
              "      <td>0.786</td>\n",
              "      <td>Payment terms are\\nnet 30 days from receipt of invo...</td>\n",
              "      <td>Context information is below.\\n---------------------\\nPayment terms are\\nnet 30 day...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>C1 (Th=0.70)</td>\n",
              "      <td>8</td>\n",
              "      <td>0.70</td>\n",
              "      <td>Off</td>\n",
              "      <td>8</td>\n",
              "      <td>0.786</td>\n",
              "      <td>Payment terms are\\nnet 30 days from receipt of invo...</td>\n",
              "      <td>Context information is below.\\n---------------------\\nPayment terms are\\nnet 30 day...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>C2 (Th=0.75)</td>\n",
              "      <td>8</td>\n",
              "      <td>0.75</td>\n",
              "      <td>Off</td>\n",
              "      <td>8</td>\n",
              "      <td>0.786</td>\n",
              "      <td>Payment terms are\\nnet 30 days from receipt of invo...</td>\n",
              "      <td>Context information is below.\\n---------------------\\nPayment terms are\\nnet 30 day...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>C3 (Th=0.80)</td>\n",
              "      <td>8</td>\n",
              "      <td>0.80</td>\n",
              "      <td>Off</td>\n",
              "      <td>0</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>Context information is below.\\n---------------------\\nPayment terms are\\nnet 30 day...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>D (Rerank)</td>\n",
              "      <td>8</td>\n",
              "      <td>None</td>\n",
              "      <td>BGE</td>\n",
              "      <td>3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>2. PAYMENT\\n2.1 Client agrees to pay Service Provid...</td>\n",
              "      <td>Context information is below.\\n---------------------\\n2. PAYMENT\\n2.1 Client agrees...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}