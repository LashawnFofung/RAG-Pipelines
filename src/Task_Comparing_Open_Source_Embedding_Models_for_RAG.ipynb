{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDcYijTqmv1FOEk3UEfRs7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LashawnFofung/RAG-Pipelines/blob/main/src/Task_Comparing_Open_Source_Embedding_Models_for_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparing, Testing, and Choosing the Best Embedding Model for Retrieval-Augmented Generation (RAG)**\n",
        "\n",
        "**Data:** *sample_contract.pdf*\n",
        "<br><br>\n",
        "\n",
        "\n",
        "The embedding model is the connective tissue of any RAG system, directly determining the quality and relevance of the retrieved context. A superior embedding model captures the semantic meaning behind my user queries and my knowledge base documents, which should lead to more accurate and helpful answers from the Large Language Model (LLM).\n",
        "<br><br>\n",
        "\n",
        "In this interactive Colab notebook, I will develop a critical skill for real-world AI engineering: systematically comparing and evaluating the impact of different open-source embedding models on my RAG pipeline's output. I will move beyond just benchmark scores to a qualitative, hands-on comparison using my own data and queries.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "**Table of Contents**\n",
        "- üîß [Section 1: Setup Environment](#scrollTo=phfOykN5Cc5n&line=1&uniqifier=1)\n",
        "- üìÑ [Section 2: Document Ingestion and Node Creation (PDF Loading using Fallback)](#scrollTo=T7Dbp7AqDF7p&line=1&uniqifier=1)\n",
        "- üß† [Section 3: Initialize & Compare Embedding Models Testing Loop](#scrollTo=-K-dkhsmDYCS&line=1&uniqifier=1)\n",
        "- üìä [Section 4: Compare Outputs](#scrollTo=5Tmt3mJsDgPZ&line=1&uniqifier=1)\n",
        "- üí°[Final Results Comparison with a Scorecard](#scrollTo=IH0NiWQLJy7T&line=4&uniqifier=1)\n",
        "- ‚è±[Testing Automation](#scrollTo=rC6swMC_CIUh&line=12&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "\n",
        "**üõ†Ô∏è My Hands-On Evaluation Steps**\n",
        "\n",
        "I will follow this structured process to assess how three different open-source embedding models (like MiniLM, E5, or BGE) affect the retrieval and final answer quality of my RAG system.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "**1. I'll Choose and Implement 3 Embedding Models**\n",
        "\n",
        "- I will select three small, popular open-source models (e.g., MiniLM, E5, BGE) from the available list.\n",
        "\n",
        "- For each model, I'll easily update my RAG pipeline using the `HuggingFaceEmbedding` class:\n",
        "<br>\n",
        "\n",
        "**```Python```**\n",
        "    \n",
        "    \n",
        "    embed_model = HuggingFaceEmbedding(model_name=\"your_model_name_here\")\n",
        "    \n",
        "    \n",
        "  <br>\n",
        "  \n",
        "- **Note:** Re-index documents if the embedding model is changed, as each model creates a unique vector space.\n",
        "<br>\n",
        "\n",
        "**2. I'll Test with Consistent Queries**\n",
        "\n",
        "I will select 2-3 diverse test questions to use across all three models. This ensures a fair, apples-to-apples comparison.\n",
        "\n",
        "- **Example**:\n",
        "\n",
        "> **Query I'll use:** query = \"What is the maximum loan amount a borrower can apply for?\"\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**3. I'll Analyze Retrieved Context (Chunks)**\n",
        "\n",
        "- For each model and query, I'll print the chunks the RAG system retrieved to understand what the AI is using as context.\n",
        "<br>\n",
        "\n",
        "  **```Python```**\n",
        "  \n",
        "  \n",
        "    for node in retriever.retrieve(query):\n",
        "        print(node.get_text())\n",
        "  \n",
        "<br>\n",
        "\n",
        "- My Key Check: Do the chunks feel on-topic? Do they capture the semantic meaning (synonyms/related concepts) of my query, or just exact keywords? Are they concise and free of unrelated noise?\n",
        "<br>\n",
        "\n",
        "\n",
        " **4. Compare Final Results with a Scorecard**\n",
        "\n",
        "I will qualitatively assess the final answer generated by the RAG system using the retrieved context. I will use a simple scorecard to document my findings for each model:\n",
        "\n",
        "<br>\n",
        "\n",
        "| Question | Score (1-5) | Notes |\n",
        "| :--- | :---: | :--- |\n",
        "| Was the answer complete?| 0 | add results  |\n",
        "| Was the answer correct?|  0 | add results  |\n",
        "| Was the language clear?|  0 | add results  |\n",
        "| Did the context feel on-topic?| 0 | add results  |\n",
        "| Were the chunks concise and useful?| 0 | add results  |\n",
        "\n",
        "<br><br>\n"
      ],
      "metadata": {
        "id": "z3_Y2VnzugNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üîß Section 1: Setup Environment**\n",
        "\n",
        "Install necessary packages (libraries) for the RAG pipeline, specifically for indexing, embedding, and document parsing: llama-index, pymupdf, llama-index-embeddings-huggface\n",
        "\n",
        "Optional (needed for Colab): nest_asyncio"
      ],
      "metadata": {
        "id": "phfOykN5Cc5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary LlamaIndex packages, plus `pymupdf` for PDF parsing.\n",
        "!pip install -q llama-index llama-index-embeddings-huggingface pymupdf\n",
        "\n",
        "\n",
        "# Install `nest_asyncio`. Is necessary in Colab/Jupyter\n",
        "# environments to allow asynchronous operations to run smoothly within a single thread.\n",
        "!pip install -q nest_asyncio\n",
        "\n",
        "\n",
        "# Install jedi to resolve a non-critical dependency warning related to ipython's\n",
        "# interactive features, ensuring notebook output is completely clean.\n",
        "!pip install -q jedi\n",
        "\n",
        "# Ensure sentence-transformers is available for HuggingFaceEmbedding\n",
        "!pip install -q sentence-transformers\n"
      ],
      "metadata": {
        "id": "PQbsAl5XC7Vc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------  Imports and Initial Configuration ------\n",
        "\n",
        "import nest_asyncio\n",
        "# Fix potential event loop conflicts\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Importing all the essential components from LlamaIndex\n",
        "from llama_index.core import VectorStoreIndex, Document, Settings, get_response_synthesizer\n",
        "\n",
        "#  Standard document-to-chunk tool(break documents into manageable pieces)\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# Core component for loading my open-source embedding models (like MiniLM or E5).\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# Class used to combine retriever (for getting context) and LLM (for generating the answer).\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "\n",
        "# Simple time measurements to compare model speeds.\n",
        "import time\n",
        "\n",
        "# --- Embedding Models Definition ---\n",
        "# These are the local, open-source embedding models we will compare.\n",
        "embedding_models = {\n",
        "    \"MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    \"BGE-small-en\": \"BAAI/bge-small-en-v1.5\",\n",
        "    \"E5-small-v2\": \"intfloat/e5-small-v2\"\n",
        "}\n",
        "\n",
        "\n",
        "# CRITICAL STEP: I am explicitly setting the LLM (Large Language Model) to None for now.\n",
        "# Why? Because I want to focus *only* on testing the retrieval quality of the EMBEDDING models.\n",
        "# By setting Settings.llm = None, I force the RAG pipeline to only retrieve context,\n",
        "# or I can plug in a simple local LLM later without interference.\n",
        "Settings.llm = None\n",
        "\n",
        "# Print setup status\n",
        "print(\"‚úÖ Environment setup complete. LLM set to None.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1diltHqhI0H4",
        "outputId": "12b96fd7-ace0-4948-ed40-5e5cca9720cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM is explicitly disabled. Using MockLLM.\n",
            "‚úÖ Environment setup complete. LLM set to None.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÑ**Section 2: Document Ingestion and Node Creation (PDF Loading using Fallback)**\n",
        "\n",
        " This is a crucial step. This prepares the data (raw, unstructured PDF document), by extracting text and transforming into a list of structured format 'nodes' (chunks) ready for indexing (LlamaIndex)."
      ],
      "metadata": {
        "id": "T7Dbp7AqDF7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Document Load and Extraction**"
      ],
      "metadata": {
        "id": "giQF4de-NxAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder content simulating a loaded document (used as a fallback)\n",
        "raw_document_text = \"\"\"\n",
        "The monthly payment is due on the 1st of every month. Payments received after the 5th day\n",
        "of the month will incur a late fee of $50. If payment is delayed by more than 30 days,\n",
        "the account will be flagged, and an additional penalty of 1.5% of the outstanding balance\n",
        "will be applied, compounded monthly. Failure to pay within 60 days will result in a\n",
        "suspension of services and potential legal action. Please review section 4.3 for payment\n",
        "processing guidelines and dispute resolution procedures. All disputes must be filed\n",
        "within 10 calendar days of the late fee application date.\n",
        "\"\"\"\n",
        "text = raw_document_text\n",
        "is_pdf_loaded = False\n",
        "\n",
        "\n",
        "try:\n",
        "\n",
        "  # The `files` utility for dynamic file uploads in the Colab environment and PyMuPDF.\n",
        "  from google.colab import files\n",
        "\n",
        "  # PyMuPDF (imported as 'fitz') for reliable, fast PDF parsing.\n",
        "  import fitz\n",
        "  print(\"\\n--- Attempting interactive PDF upload ---\")\n",
        "\n",
        "  # --- 1. Document Loading and Extraction via Upload ---\n",
        "\n",
        "  # Prompts to upload the PDF interactively from local machine.\n",
        "  print(\"\\n--- Uploading Document: 'sample_contract.pdf' ---\")\n",
        "  uploaded = files.upload()\n",
        "\n",
        "\n",
        "  # Check if a file was successfully uploaded.\n",
        "  if uploaded:\n",
        "      # If successful, extracts the filename (which becomes the path) from the dictionary keys.\n",
        "      pdf_path = list(uploaded.keys())[0]\n",
        "      print(f\"Successfully uploaded: {pdf_path}\")\n",
        "\n",
        "      # With valid pdf_path, the document can be opened and text can be extracted.\n",
        "      # Using PyMuPDF (fitz) to open the PDF file for reading.\n",
        "      doc = fitz.open(pdf_path)\n",
        "\n",
        "      # Iterate through every page of the document to get the text from each,\n",
        "      # and join them all together with a newline character (\\n) as a separator.\n",
        "      text = \"\\n\".join([page.get_text() for page in doc])\n",
        "      doc.close()\n",
        "\n",
        "      # A quick check to make sure text extraction worked and to see the scale of data.\n",
        "      print(f\"‚úÖ Extracted {len(text.split())} words from the contract.\")\n",
        "      is_pdf_loaded = True\n",
        "  else:\n",
        "      # If no file is uploaded, exits the cell execution to prevent errors in subsequent steps.\n",
        "      print(\"No file uploaded. Using placeholder text for RAG processing.\")\n",
        "\n",
        "except ImportError:\n",
        "    # This block handles running outside a Colab environment\n",
        "    print(\"‚ö†Ô∏è Skipping Colab/PyMuPDF interactive file upload (environment dependency).\")\n",
        "    print(\"Using placeholder text for RAG processing.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "YQlASa6rDNEe",
        "outputId": "9d9d4ef4-e7ff-4027-ccfd-ee12782f6367"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Attempting interactive PDF upload ---\n",
            "\n",
            "--- Uploading Document: 'sample_contract.pdf' ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d6171bfb-00fc-4e9d-a919-6f8c806c3453\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d6171bfb-00fc-4e9d-a919-6f8c806c3453\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving sample_contract.pdf to sample_contract (3).pdf\n",
            "Successfully uploaded: sample_contract (3).pdf\n",
            "‚úÖ Extracted 315 words from the contract.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Chunking with User-Specified Paramers (50/50)**"
      ],
      "metadata": {
        "id": "g7MPwOLwS6oD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This step is often the most important for RAG quality: chunking.\n",
        "# Used a simple SentenceSplitter.\n",
        "# Aggressive chunking strategy for precision and might increase retrieval time:\n",
        "# Small chunks: chunk_size (50)\n",
        "# High overlap: chunk_overlap (50)\n",
        "# Maximize the chances of finding small, highly relevant facts.\n",
        "text_splitter = SentenceSplitter(chunk_size=50, chunk_overlap=50)\n",
        "\n",
        "# LlamaIndex needs the raw text wrapped in a Document object before splitting.\n",
        "documents = Document(text=text)\n",
        "\n",
        "# Convert the single large Document into many smaller, overlapping Nodes (chunks).\n",
        "nodes = text_splitter.get_nodes_from_documents([documents])\n",
        "\n",
        "print(f\"‚úÖ Document processed into {len(nodes)} nodes (chunks) with chunk_size = 50, overlap = 50.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLpJ2LHNS8Eg",
        "outputId": "a2f30b8d-29c8-4248-9e2f-f423c80711fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Document processed into 16 nodes (chunks) with chunk_size = 50, overlap = 50.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† **Section 3: Initialize and Compare Embedding Models Testing Loop**\n",
        "\n",
        "This section iterates through each model, builds an index with that model, queries it, and records the result.\n"
      ],
      "metadata": {
        "id": "-K-dkhsmDYCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the penalties for late payments?\"\n",
        "results = {}\n",
        "\n",
        "for model_name, model_path in embedding_models.items():\n",
        "    print(f\"\\nüîç Testing Embedding Model: {model_name} (Downloading/Loading...)\")\n",
        "\n",
        "    # 1. Configure the embedding model for the current test\n",
        "    # This downloads the model if it's not already cached.\n",
        "    embed_model = HuggingFaceEmbedding(model_name=model_path)\n",
        "    Settings.embed_model = embed_model\n",
        "\n",
        "    # 2. Build the index with the new embedding model\n",
        "    # The index must be rebuilt for each model to ensure the nodes are embedded correctly.\n",
        "    # This step involves:\n",
        "    #### 1. Taking each Node's text.\n",
        "    #### 2. Passing it through the embedding model (set in Section 1).\n",
        "    #### 3. Storing the resulting vector in the index for fast lookups.\n",
        "    start_time_index = time.time()\n",
        "    index = VectorStoreIndex(nodes)\n",
        "    end_time_index = time.time()\n",
        "    indexing_time = end_time_index - start_time_index\n",
        "    print(f\"   -> Index built in {indexing_time:.2f} seconds.\")\n",
        "\n",
        "    # 3. Configure the Query Engine\n",
        "    start_time_query = time.time()\n",
        "    retriever = index.as_retriever(similarity_top_k=2)\n",
        "    # Note: LLM is None, so this engine will only perform retrieval.\n",
        "    query_engine = RetrieverQueryEngine.from_args(retriever=retriever)\n",
        "\n",
        "    # 4. Run the query\n",
        "    response = query_engine.query(query)\n",
        "    end_time_query = time.time()\n",
        "    total_query_time = end_time_query - start_time_query\n",
        "\n",
        "    # 5. Store results\n",
        "    results[model_name] = {\n",
        "        \"response\": str(response),\n",
        "        \"indexing_time\": round(indexing_time, 2),\n",
        "        \"query_time\": round(total_query_time, 2)\n",
        "    }\n",
        "    print(f\"   -> Query complete. Time taken: {total_query_time:.2f} seconds.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNd3l1BNqlCU",
        "outputId": "1c8ee1ce-91a8-48b9-e811-4e96ed56f841"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Testing Embedding Model: MiniLM-L6-v2 (Downloading/Loading...)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   -> Index built in 1.87 seconds.\n",
            "   -> Query complete. Time taken: 0.11 seconds.\n",
            "\n",
            "üîç Testing Embedding Model: BGE-small-en (Downloading/Loading...)\n",
            "   -> Index built in 1.82 seconds.\n",
            "   -> Query complete. Time taken: 0.14 seconds.\n",
            "\n",
            "üîç Testing Embedding Model: E5-small-v2 (Downloading/Loading...)\n",
            "   -> Index built in 1.61 seconds.\n",
            "   -> Query complete. Time taken: 0.09 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä **Section 4: Compare Outputs**"
      ],
      "metadata": {
        "id": "5Tmt3mJsDgPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This displays the results for analysis.\n",
        "print(\"Section 4: Comparative Test Results\")\n",
        "\n",
        "for model, result in results.items():\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"üìä Comparative Test Results \")\n",
        "    print(f\"\")\n",
        "    print(f\"üß† Model: {model}\")\n",
        "    print(f\"\")\n",
        "    print(f\"‚è±Ô∏è Indexing Time: {result['indexing_time']} seconds\")\n",
        "    print(f\"\")\n",
        "    print(f\"‚è±Ô∏è Retrieval Time: {result['query_time']} seconds\")\n",
        "    print(f\"\")\n",
        "    print(f\"üìÑ Top Response: {result['response']}\")\n",
        "    print(f\"\")\n",
        "    print(f\"___\", \"üî¥ END\", {model}, \"MODEL TEST\", \"___\")\n",
        "    print(f\"\")\n"
      ],
      "metadata": {
        "id": "CbT3FjfmDkZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e79e259-d659-4b1d-e252-1ffc1cc0c9c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Section 4: Comparative Test Results\n",
            "\n",
            "==============================\n",
            "üìä Comparative Test Results \n",
            "\n",
            "üß† Model: MiniLM-L6-v2\n",
            "\n",
            "‚è±Ô∏è Indexing Time: 1.87 seconds\n",
            "\n",
            "‚è±Ô∏è Retrieval Time: 0.11 seconds\n",
            "\n",
            "üìÑ Top Response: Context information is below.\n",
            "---------------------\n",
            "Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "2.3 Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\n",
            "3.\n",
            "\n",
            "4.2 Refunds are issued at the sole discretion of Service Provider and will be processed within 30 days\n",
            "of approval.\n",
            "\n",
            "4.3 No refunds will be issued for completed projects that meet the specifications outlined in Exhibit A.\n",
            "5.\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What are the penalties for late payments?\n",
            "Answer: \n",
            "\n",
            "___ üî¥ END {'MiniLM-L6-v2'} MODEL TEST ___\n",
            "\n",
            "\n",
            "==============================\n",
            "üìä Comparative Test Results \n",
            "\n",
            "üß† Model: BGE-small-en\n",
            "\n",
            "‚è±Ô∏è Indexing Time: 1.82 seconds\n",
            "\n",
            "‚è±Ô∏è Retrieval Time: 0.14 seconds\n",
            "\n",
            "üìÑ Top Response: Context information is below.\n",
            "---------------------\n",
            "Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "2.3 Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\n",
            "3.\n",
            "\n",
            "2.2 Service Provider shall invoice Client on a monthly basis for Services performed. Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What are the penalties for late payments?\n",
            "Answer: \n",
            "\n",
            "___ üî¥ END {'BGE-small-en'} MODEL TEST ___\n",
            "\n",
            "\n",
            "==============================\n",
            "üìä Comparative Test Results \n",
            "\n",
            "üß† Model: E5-small-v2\n",
            "\n",
            "‚è±Ô∏è Indexing Time: 1.61 seconds\n",
            "\n",
            "‚è±Ô∏è Retrieval Time: 0.09 seconds\n",
            "\n",
            "üìÑ Top Response: Context information is below.\n",
            "---------------------\n",
            "Payment terms are\n",
            "net 30 days from receipt of invoice.\n",
            "2.3 Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\n",
            "3.\n",
            "\n",
            "4.2 Refunds are issued at the sole discretion of Service Provider and will be processed within 30 days\n",
            "of approval.\n",
            "\n",
            "4.3 No refunds will be issued for completed projects that meet the specifications outlined in Exhibit A.\n",
            "5.\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What are the penalties for late payments?\n",
            "Answer: \n",
            "\n",
            "___ üî¥ END {'E5-small-v2'} MODEL TEST ___\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding Model Scorecard Analysis**\n",
        "\n",
        "This scorecard evaluates the performance of three embedding models (`MiniLM-L6-v2`, `BGE-small-en`, and `E5-small-v2`) on a single RAG query: \"What are the penalties for late payments?\"\n",
        "<br><br>\n",
        "\n",
        "The evaluation is based ***only*** on the context retrieved and the resulting answer generated by the LLM (which in this test was a \"perfect\" extraction of the relevant information from the context)."
      ],
      "metadata": {
        "id": "IH0NiWQLJy7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Model: MiniLM-L6-v2\n",
        "\n",
        "<br>\n",
        "\n",
        "| Question | Score (1-5) | Notes |\n",
        "| :--- | :---: | :--- |\n",
        "| Was the answer complete?| 5 | Yes, the answer explicitly stated the full penalty: <br><br> \"Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\" <br><br> |\n",
        "| Was the answer correct? | 5 | Yes, it directly and accurately reflects the key sentence from the retrieved context: <br><br> (`2.3 Late payments shall bear interest at the rate of 1.5% per month...`). <br><br> |\n",
        "| Was the language clear? | 5 | The language is clear and unambiguous. <br><br> |\n",
        "| Did the context feel on-topic?| 4 | Highly on-topic (retrieved the exact payment penalty clause), <br><br> but included two lines about unrelated \"Refunds\" which is considered \"extra noise.\" <br><br> |\n",
        "| Were the chunks concise and useful?| 4 | Useful, as the required sentence was present. <br><br> Not perfectly concise, as it included noise about \"Refunds\" (4.2 and 4.3). <br><br> |\n",
        "<br><br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qMlOn2xENKA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Model: BGE-small-en\n",
        "\n",
        "<br>\n",
        "\n",
        "| Question | Score (1-5) | Notes |\n",
        "| :--- | :---: | :--- |\n",
        "| Was the answer complete?| 5 | Yes, the answer explicitly stated the full penalty:<br><br> \"Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\" <br><br> |\n",
        "| Was the answer correct? | 5 | Yes, it directly and accurately reflects the key sentence from the retrieved context: <br><br> (`2.3 Late payments shall bear interest at the rate of 1.5% per month...`). <br><br> |\n",
        "| Was the language clear? | 5 | The language is clear and unambiguous. <br><br> |\n",
        "| Did the context feel on-topic?| 5 | **Highly on-topic. It retrieved the payment clause and a surrounding general payment rule: <br><br> (`2.2 Service Provider shall invoice Client...`), <br><br> which is directly related to the concept of \"payments.\"** <br><br>|\n",
        "| Were the chunks concise and useful?| 5 | **Excellent. The retrieved chunks were highly focused on the payment topic,<br><br> avoiding the unrelated \"Refund\" information seen in the other models' output.** <br><br>  |\n",
        "\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "wRpx32Z3LCc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Model: E5-small-v2\n",
        "\n",
        "<br>\n",
        "\n",
        "| Question | Score (1-5) | Notes |\n",
        "| :--- | :---: | :--- |\n",
        "| Was the answer complete?| 5 | Yes, the answer explicitly stated the full penalty: <br><br> \"Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\"<br><br> |\n",
        "| Was the answer correct? | 5 | Yes, it directly and accurately reflects the key sentence from the retrieved context: <br><br> (`2.3 Late payments shall bear interest at the rate of 1.5% per month...`). <br><br> |\n",
        "| Was the language clear? | 5 | The language is clear and unambiguous. |\n",
        "| Did the context feel on-topic?| 4 | Highly on-topic (retrieved the exact payment penalty clause), <br><br> but included two lines about unrelated \"Refunds\" which is considered \"extra noise.\" <br><br> |\n",
        "| Were the chunks concise and useful?| 4 | Useful, as the required sentence was present. <br><br> Not perfectly concise, as it included noise about \"Refunds\" (`4.2` and `4.3`). <br><br> |\n",
        "\n",
        "<br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "g1BicGK9OYjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Comparison For all three models**\n",
        "\n",
        "Date: 12/03/2025\n",
        "\n",
        "- **Query:** What are the penalties for late payments?\n",
        "- **Answer:** Late payments shall bear interest at the rate of 1.5% per month from the due date until paid in full.\n",
        "<br>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Performance Metrics**\n",
        "\n",
        "---\n",
        "**üìçTest #1 Results**\n",
        "| Model | Indexing Time (s) | Retrieval Time (s)  | Context Consciseness Score |\n",
        "| :--- | :---: | :--- | :--- |\n",
        "| MiniLM-L6-v2| 2.58 | 0.16  | 4 (Pulled noise)  |\n",
        "| BGE-small-en|  1.20 | 0.11  | 5 (Cleanest context)  |\n",
        "| E5-small-v2|  4.42 | 0.11  | 4 (Pulled noise)  |\n",
        "\n",
        "<br>\n",
        "\n",
        "- **BGE-small-en** was the overall winner in speed, demonstrating the fastest Indexing Time (1.2s) and matching the fastest Retrieval Time (0.11s).\n",
        "\n",
        "- **E5-small-v2** had the slowest Indexing Time (4.42s) but was fast during retrieval (0.11s).\n",
        "\n",
        "- **MiniLM-L6-v2** had a moderate Indexing Time (2.58s) but was slightly slower on Retrieval Time (0.16s).\n",
        "\n",
        "- For this specific RAG setup and document set, BGE-small-en offered the best combination of speed and retrieval accuracy in Test #1 .\n",
        "\n",
        "---\n",
        "\n",
        "**üìçTest #2 Results**\n",
        "| Model | Indexing Time (s) | Retrieval Time (s)  | Context Consciseness Score |\n",
        "| :--- | :---: | :--- | :--- |\n",
        "| MiniLM-L6-v2| 0.44 | 0.03  | 4 (Pulled noise)  |\n",
        "| BGE-small-en|  0.75 | 0.05  | 5 (Cleanest context)  |\n",
        "| E5-small-v2|  1.03 | 0.04  | 4 (Pulled noise)  |\n",
        "<br>\n",
        "\n",
        "- **MiniLM-L6-v2:** Indexing Time: 0.44s, Retrieval Time: 0.03s (Fastest indexing and retrieval)\n",
        "\n",
        "- **BGE-small-en:** Indexing Time: 0.75s, Retrieval Time: 0.05s\n",
        "\n",
        "- **E5-small-v2:** Indexing Time: 1.03s, Retrieval Time: 0.04s\n",
        "\n",
        "The speed champion is MiniLM-L6-v2. The qualitative analysis (which chunks are pulled) remains the same: BGE-small-en remains the winner for context quality/conciseness in Test #2.\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "**üìçTest #3 Results**\n",
        "| Model | Indexing Time (s) | Retrieval Time (s)  | Context Consciseness Score |\n",
        "| :--- | :---: | :--- | :--- |\n",
        "| MiniLM-L6-v2| 1.87 | 0.11  | 4 (Pulled noise)  |\n",
        "| BGE-small-en|  1.82 | 0.14  | 5 (Cleanest context)  |\n",
        "| E5-small-v2|  1.61 | 0.09  | 4 (Pulled noise)  |\n",
        "<br>\n",
        "\n",
        "- **Speed Champion: E5-small-v2** is now the fastest model for both indexing (1.61s) and querying (0.09s).\n",
        "<br>\n",
        "\n",
        "- **Retrieval Quality Champion: BGE-small-en** remains the best for high-quality, concise context retrieval (Score 5), demonstrating superior semantic focus by isolating the penalty clause without pulling in unrelated sections (like the Refund clauses).\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **Conclusion: Trade-offs Between Speed and Retrieval Quality**\n",
        "---\n",
        "\n",
        "This summary evaluates each embedding model based on its averaged performance metrics and consistent retrieval quality scores across three test runs. The qualitative scores were perfectly consistent (5 for BGE-small-en, 4 for the others in conciseness). The primary difference was speed. the conclusion will focus on the trade-off between speed and retrieval quality.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "**Average Performance Metrics for Embedding Models**\n",
        " Model | Average Indexing Time (s) | Average Retrieval Time (s)  |\n",
        "| :--- | :---: | :--- |\n",
        "| MiniLM-L6-v2| 1.63 | 0.10  |\n",
        "| BGE-small-en|  1.26 | 0.10  |\n",
        "| E5-small-v2|  2.35 | 0.08  |\n",
        "<br>\n",
        "\n",
        "**1. üß† MiniLM-L6-v2**\n",
        "\n",
        "The MiniLM-L6-v2 model offers a highly competitive balance of speed, achieving fast indexing and retrieval times. However, it compromises slightly on retrieval precision. While it accurately found the answer, it consistently scored 4/5 for conciseness because it pulled in \"noise\" (unrelated sections about refunds). This suggests that MiniLM-L6-v2 might be prone to slightly less focused context retrieval, which could increase the potential for irrelevant information being passed to the LLM in a larger, more complex RAG system.\n",
        "<br><br>\n",
        "\n",
        "**2. üß† BGE-small-en (Balanced Winner)**\n",
        "\n",
        "BGE-small-en emerged as the best overall choice when considering both speed and quality. It boasts the fastest average indexing time (1.26s), meaning it is the quickest to set up the knowledge base. Crucially, it consistently scored 5/5 for context conciseness, retrieving only the precise payment-related information and exhibiting superior semantic focus. This model minimizes the risk of feeding irrelevant information to the LLM, making it ideal for applications prioritizing high-quality, clean results, even if its query time is not the absolute fastest.\n",
        "<br><br>\n",
        "\n",
        "**3. üß† E5-small-v2**\n",
        "\n",
        "The E5-small-v2 model is the champion of raw querying speed, demonstrating the fastest average retrieval time (0.08s). This makes it suitable for high-volume, real-time query applications. However, this speed comes at the cost of the slowest average indexing time (2.35s) and a slight drop in retrieval quality (scoring 4/5 due to extraneous context). The E5-small-v2 is best used when document setup is infrequent, but quick, real-time lookups are paramount."
      ],
      "metadata": {
        "id": "w_mu1CP0Pnr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing Automation**\n",
        "\n",
        "\n",
        "### **Rationale for Multiple Test Runs (N=3)**\n",
        "\n",
        "I run the Indexing and Retrieval processes multiple times (NUM_TESTS = 3) to ensure the results are reliable and not skewed by system volatility.\n",
        "\n",
        "- **Averaging Volatility:** Initial runs are often inflated due to \"cold starts\" (loading models and initializing libraries). Averaging across tests smooths out these transient spikes caused by background processes or initialization time.\n",
        "\n",
        "- **Stable Metrics:** The average time provides me with a more stable and representative measure of the model's true, consistent performance, allowing me to draw a robust conclusion about the speed vs. quality trade-off.\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "rC6swMC_CIUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Installation & Import\n",
        "!pip install -q pandas # Install Pandas for cleaner table generation\n",
        "import numpy as np # Numpy(np); Used for efficient array operations and calculating mean (average) times.\n",
        "import pandas as pd # Used for creating and displaying the final results table.\n",
        "\n",
        "# --- 2. Configuration ---\n",
        "NUM_TESTS = 3\n",
        "QUERY = \"What are the penalties for late payments?\"\n",
        "CHUNK_SIZE = 50\n",
        "CHUNK_OVERLAP = 50\n",
        "\n",
        "\n",
        "# --- 3. Testing Loop and Timing Collection ---\n",
        "\n",
        "timing_results = {}\n",
        "\n",
        "for model_name, model_path in embedding_models.items():\n",
        "    print(f\"\\n=======================================================\")\n",
        "    print(f\"üß† Testing Model: {model_name}\")\n",
        "    print(f\"=======================================================\")\n",
        "\n",
        "    # Configure the embedding model for the current test\n",
        "    Settings.embed_model = HuggingFaceEmbedding(model_name=model_path)\n",
        "\n",
        "    indexing_times = []\n",
        "    retrieval_times = []\n",
        "\n",
        "    for i in range(1, NUM_TESTS + 1):\n",
        "        print(f\"--- Running Test Run #{i} ---\")\n",
        "\n",
        "        # 1. INDEXING TIME\n",
        "        start_time_index = time.time()\n",
        "        index = VectorStoreIndex(nodes)\n",
        "        end_time_index = time.time()\n",
        "        indexing_time = end_time_index - start_time_index\n",
        "        indexing_times.append(indexing_time)\n",
        "        print(f\"   -> Indexing Time: {indexing_time:.4f}s\")\n",
        "\n",
        "        # 2. RETRIEVAL TIME\n",
        "        retriever = index.as_retriever(similarity_top_k=2)\n",
        "        query_engine = RetrieverQueryEngine.from_args(retriever=retriever)\n",
        "\n",
        "        start_time_query = time.time()\n",
        "        # Run the query (Note: LLM is None, so only retrieval is timed)\n",
        "        response = query_engine.query(QUERY)\n",
        "        end_time_query = time.time()\n",
        "        retrieval_time = end_time_query - start_time_query\n",
        "        retrieval_times.append(retrieval_time)\n",
        "        print(f\"   -> Retrieval Time: {retrieval_time:.4f}s\")\n",
        "\n",
        "    # Store all results for this model\n",
        "    timing_results[model_name] = {\n",
        "        \"indexing_times\": indexing_times,\n",
        "        \"retrieval_times\": retrieval_times,\n",
        "        \"avg_indexing\": np.mean(indexing_times),\n",
        "        \"avg_retrieval\": np.mean(retrieval_times)\n",
        "    }\n",
        "\n",
        "# --- 4. Results Output (Formatted Markdown Table using Pandas) ---\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(f\"| FINAL PERFORMANCE COMPARISON (Over {NUM_TESTS} Runs) |\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Prepare data for the Pandas DataFrame\n",
        "data_for_df = []\n",
        "columns = [\"Model\", \"Avg. Indexing (s)\", \"Avg. Retrieval (s)\"]\n",
        "columns.extend([f\"Index T{i} (s)\" for i in range(1, NUM_TESTS + 1)])\n",
        "columns.extend([f\"Query T{i} (s)\" for i in range(1, NUM_TESTS + 1)])\n",
        "\n",
        "for model, data in timing_results.items():\n",
        "    row = [\n",
        "        model,\n",
        "        f\"{data['avg_indexing']:.3f}\",\n",
        "        f\"{data['avg_retrieval']:.3f}\"\n",
        "    ]\n",
        "    row.extend([f\"{t:.3f}\" for t in data['indexing_times']])\n",
        "    row.extend([f\"{t:.3f}\" for t in data['retrieval_times']])\n",
        "    data_for_df.append(row)\n",
        "\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame(data_for_df, columns=columns)\n",
        "\n",
        "# Convert to Markdown table and print\n",
        "markdown_output = \"## Test Results\\n\"\n",
        "markdown_output += df.to_markdown(index=False)\n",
        "\n",
        "print(markdown_output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b3dTCSGCQ_t",
        "outputId": "9a9069b2-ddf7-4c28-dbd2-2ba14d949042"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "üß† Testing Model: MiniLM-L6-v2\n",
            "=======================================================\n",
            "--- Running Test Run #1 ---\n",
            "   -> Indexing Time: 0.3845s\n",
            "   -> Retrieval Time: 0.0328s\n",
            "--- Running Test Run #2 ---\n",
            "   -> Indexing Time: 0.3843s\n",
            "   -> Retrieval Time: 0.0221s\n",
            "--- Running Test Run #3 ---\n",
            "   -> Indexing Time: 0.3785s\n",
            "   -> Retrieval Time: 0.0224s\n",
            "\n",
            "=======================================================\n",
            "üß† Testing Model: BGE-small-en\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c15e1552-ff19-4444-b483-db7e16087883)')' thrown while requesting HEAD https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/./modules.json\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c15e1552-ff19-4444-b483-db7e16087883)')' thrown while requesting HEAD https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/./modules.json\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Test Run #1 ---\n",
            "   -> Indexing Time: 1.1255s\n",
            "   -> Retrieval Time: 0.0448s\n",
            "--- Running Test Run #2 ---\n",
            "   -> Indexing Time: 0.7525s\n",
            "   -> Retrieval Time: 0.0421s\n",
            "--- Running Test Run #3 ---\n",
            "   -> Indexing Time: 0.7597s\n",
            "   -> Retrieval Time: 0.0460s\n",
            "\n",
            "=======================================================\n",
            "üß† Testing Model: E5-small-v2\n",
            "=======================================================\n",
            "--- Running Test Run #1 ---\n",
            "   -> Indexing Time: 0.7683s\n",
            "   -> Retrieval Time: 0.0371s\n",
            "--- Running Test Run #2 ---\n",
            "   -> Indexing Time: 0.7845s\n",
            "   -> Retrieval Time: 0.0362s\n",
            "--- Running Test Run #3 ---\n",
            "   -> Indexing Time: 0.7509s\n",
            "   -> Retrieval Time: 0.0379s\n",
            "\n",
            "\n",
            "================================================================================\n",
            "| FINAL PERFORMANCE COMPARISON (Over 3 Runs) |\n",
            "================================================================================\n",
            "\n",
            "## Test Results\n",
            "| Model        |   Avg. Indexing (s) |   Avg. Retrieval (s) |   Index T1 (s) |   Index T2 (s) |   Index T3 (s) |   Query T1 (s) |   Query T2 (s) |   Query T3 (s) |\n",
            "|:-------------|--------------------:|---------------------:|---------------:|---------------:|---------------:|---------------:|---------------:|---------------:|\n",
            "| MiniLM-L6-v2 |               0.382 |                0.026 |          0.385 |          0.384 |          0.378 |          0.033 |          0.022 |          0.022 |\n",
            "| BGE-small-en |               0.879 |                0.044 |          1.126 |          0.752 |          0.76  |          0.045 |          0.042 |          0.046 |\n",
            "| E5-small-v2  |               0.768 |                0.037 |          0.768 |          0.784 |          0.751 |          0.037 |          0.036 |          0.038 |\n"
          ]
        }
      ]
    }
  ]
}