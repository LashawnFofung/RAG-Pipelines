{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqjrkVtR82i4LTM1qA1OCc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LashawnFofung/RAG-Pipelines/blob/main/src/Optimized_RAG_Pipeline_with_Simple_Chatbot_for_Document_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üìë Build And Optimize A RAG Pipeline For Document Retrieval**\n",
        "\n",
        "**Data:** *LenderFeesWorksheetNew.pdf*\n",
        "<br><br>\n",
        "\n",
        "\n",
        "This Colab notebook sets up a Retrieval-Augmented Generation (RAG) system designed to answer questions about the uploaded LenderFeesWorksheetNew.pdf.\n",
        "<br><br>\n",
        "\n",
        "It works by first using PyMuPDF to accurately extract (parse) the text from the PDF. This text is then broken down into smaller, meaningful chunks (‚úÇÔ∏è) and converted into numerical vectors (üî¢) using the efficient `all-MiniLM-L6-v2` HuggingFace embedding model. These vectors are stored in a Vector Index. When you ask a question, the system searches the index to retrieve (üîç) the most relevant text chunks (Vector Retrieval) and feeds both the question and those chunks to the Gemini 2.5 Flash LLM. The LLM then reads the retrieved context and generates a precise answer, such as calculating the total monthly payment or identifying specific fees.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "**Table of Contents**\n",
        "* [1. RAG Pipeline](#scrollTo=Bfr8FNjnSvW8&line=1&uniqifier=1)\n",
        "  * [1.1 Installation](#scrollTo=CVLftbBrTtJD&line=1&uniqifier=1)\n",
        "  * [1.2 Setup Environment and Imports](#scrollTo=A-7Tha4LT_Wp&line=1&uniqifier=1)\n",
        "  * [1.3 API Key Setup](#scrollTo=p43QyjfpUZ-k&line=1&uniqifier=1)\n",
        "  * [1.4 Document Upload](#scrollTo=zF41iRd_UmH7&line=1&uniqifier=1)\n",
        "  * [1.5 Custom PyMuPDF Loader Function (PDF Parsing)](#scrollTo=bh0AOQB7nvmp&line=1&uniqifier=1)\n",
        "  * [1.6 Configure RAG Pipeline (LLM, Embedding, Chunking)](#scrollTo=YqYMeoCroEGh&line=1&uniqifier=1)\n",
        "  * [1.7 Indexing and Index Creation](#scrollTo=7F7-so82oU_z&line=1&uniqifier=1)\n",
        "* [2. Reasons for Methods: Embedding, Chunking Strategy, & Retrieval Method](#scrollTo=2lo9ffUJSzJE&line=1&uniqifier=1)\n",
        "* [3. Simple Chatbot](#scrollTo=KZt_12xPS5hx)\n",
        "  * [3.1 Create Chat Engine](#scrollTo=B_9DrGkNxFQr&line=1&uniqifier=1)\n",
        "  * [3.2 Interactive Chat Loop](#scrollTo=zxAruhrJyeHa&line=1&uniqifier=1)\n",
        "  * [3.3 Run the Chatbot](#scrollTo=wIfn7bGazZ7k&line=1&uniqifier=1)\n",
        "  "
      ],
      "metadata": {
        "id": "6PmIjtzqPkDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. RAG Pipeline**"
      ],
      "metadata": {
        "id": "Bfr8FNjnSvW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Installation"
      ],
      "metadata": {
        "id": "CVLftbBrTtJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install LlamaIndex core, Gemini LLM connector, PyMuPDF, and HuggingFace Embedding integration\n",
        "!pip install -q llama-index llama-index-llms-gemini pymupdf llama-index-embeddings-huggingface nest_asyncio sentence-transformers"
      ],
      "metadata": {
        "id": "H8BUJNkyT0LC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Setup Environment and Imports"
      ],
      "metadata": {
        "id": "A-7Tha4LT_Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz # PyMuPDF\n",
        "from google.colab import files, userdata\n",
        "from llama_index.core import VectorStoreIndex, Document, Settings\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from typing import List\n",
        "import nest_asyncio\n",
        "\n",
        "# Fix event loop conflicts in Colab\n",
        "nest_asyncio.apply()\n"
      ],
      "metadata": {
        "id": "9M0_WDziUJjO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 API Key Setup"
      ],
      "metadata": {
        "id": "p43QyjfpUZ-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gemini API Key setup in Colab secret\n",
        "try:\n",
        "    API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    if not API_KEY:\n",
        "        raise ValueError(\"GEMINI_API_KEY not found in Colab Secrets. Please set it.\")\n",
        "    # Set the official environment variable name required by the Google GenAI SDK\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
        "    print(\"‚úÖ API Key successfully loaded and set as GOOGLE_API_KEY.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Warning: Could not load API Key from Colab Secrets: {e}\")\n",
        "    print(\"Please ensure your API Key is set as a Colab secret named 'GEMINI_API_KEY' or set the environment variable manually.\")\n",
        "    # Fallback/Manual setting (Uncomment and replace if Colab Secrets is not used)\n",
        "    # os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_MANUAL_API_KEY\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0InllCONUggj",
        "outputId": "81e67f6f-e213-4fcd-e826-380244425c13"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ API Key successfully loaded and set as GOOGLE_API_KEY.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Document Upload"
      ],
      "metadata": {
        "id": "zF41iRd_UmH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Uploading Document: 'LenderFeesWorksheetNew.pdf' ---\")\n",
        "uploaded = files.upload()\n",
        "pdf_path = None\n",
        "if uploaded:\n",
        "    pdf_path = list(uploaded.keys())[0]\n",
        "    print(f\"Successfully uploaded: {pdf_path}\")\n",
        "else:\n",
        "    print(\"No file was uploaded. Exiting.\")\n",
        "    exit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "BjWKUUkIUp6W",
        "outputId": "1d3227fd-455e-4505-cf24-ca9eef48288a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Uploading Document: 'LenderFeesWorksheetNew.pdf' ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ad92c5ca-c198-494c-b527-788ebe98d76d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ad92c5ca-c198-494c-b527-788ebe98d76d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving LenderFeesWorksheetNew.pdf to LenderFeesWorksheetNew (1).pdf\n",
            "Successfully uploaded: LenderFeesWorksheetNew (1).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Custom PyMuPDF Loader Function (PDF Parsing)"
      ],
      "metadata": {
        "id": "bh0AOQB7nvmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf_with_pymupdf(pdf_path: str) -> List[Document]:\n",
        "    \"\"\"Load a PDF and convert it to LlamaIndex Document format using PyMuPDF.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    documents = []\n",
        "    for i, page in enumerate(doc):\n",
        "        text = page.get_text()\n",
        "        if not text.strip(): continue # Skip empty pages\n",
        "        documents.append(\n",
        "            Document(text=text, metadata={\"file_name\": os.path.basename(pdf_path), \"page_number\": i + 1})\n",
        "        )\n",
        "    doc.close()\n",
        "    print(f\"Processed {pdf_path}: Extracted {len(documents)} pages with content.\")\n",
        "    return documents"
      ],
      "metadata": {
        "id": "H0kFuz4Rn7Q3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Configure RAG Pipeline (LLM, Embeddings, Chunking)"
      ],
      "metadata": {
        "id": "YqYMeoCroEGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Configuring LlamaIndex Settings ---\")\n",
        "# LLM: Gemini 2.5 Flash\n",
        "llm = Gemini(model=\"models/gemini-2.5-flash\")\n",
        "Settings.llm = llm\n",
        "\n",
        "# Embedding Model: HuggingFace all-MiniLM-L6-v2 (Efficient and Local)\n",
        "# Hugging Face API Key setup in Colab Secret\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "# Chunking Strategy: SentenceSplitter with optimal settings\n",
        "Settings.text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
        "Settings.chunk_size = 512\n",
        "Settings.chunk_overlap = 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "5mtgoZ_aoNqP",
        "outputId": "64c4dec3-3000-47c3-c99d-015c45b3dc93"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Configuring LlamaIndex Settings ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1179820267.py:3: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
            "  llm = Gemini(model=\"models/gemini-2.5-flash\")\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 Indexing and Index Creation"
      ],
      "metadata": {
        "id": "7F7-so82oU_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = load_pdf_with_pymupdf(pdf_path)\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "print(\"‚úÖ Indexing complete. RAG VectorStoreIndex created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvoRfyFRog9X",
        "outputId": "3ff9ea54-8b90-4dfc-cead-dfb6597da00c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed LenderFeesWorksheetNew (1).pdf: Extracted 1 pages with content.\n",
            "‚úÖ Indexing complete. RAG VectorStoreIndex created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Reasons for Methods: Embedding, Chunking Strategy, & Retrieval Method**"
      ],
      "metadata": {
        "id": "2lo9ffUJSzJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üî¢ Embedding Model**\n",
        "\n",
        "**Model:** `sentence-transformers/all-MiniLM-L6-v2`\n",
        "<br><br>\n",
        "\n",
        "**Justification:** This model is an extremely efficient, small, and fast open-source embedding model. It provides a good balance of accuracy and speed, making it highly suitable for rapid RAG development in a Colab environment. Running a local HuggingFace model also helps to reduce API costs and latency compared to calling a remote embedding service.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ggWcI5nj0vzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **‚úÇÔ∏è Chunking Strategy**\n",
        "\n",
        "**Strategy:** `SentenceSplitter` with `chunk_size = 512` and `chunk_overlap = 20`.\n",
        "<br><br>\n",
        "\n",
        "**Justification:** The `SentenceSplitter` breaks text primarily at logical sentence boundaries, which is ideal for a semi-structured document like a financial worksheet.\n",
        "\n",
        "  * **Chunk Size (512 tokens):** Provides enough context for the LLM to perform calculations or detailed analysis from the retrieved text.\n",
        "  \n",
        "  * **Overlap (20 tokens):** A small overlap ensures that the context is maintained across the split points, preventing critical information from being cut off.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZoIn097P19eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üîç Retrieval Method**\n",
        "\n",
        "**Method:** **Vector Retrieval** (Semantic Search)\n",
        "<br><br>\n",
        "\n",
        "**Justification:** Vector retrieval is used because it finds relevant document segments based on the semantic mening of the query. This is essential for documents where specific financial terms might be used (e.g., \"lender's title insurance\") that a user might query using a different phrase (e.g., \"fee to protect the lender's interest\").\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kFxgX4_l3Zwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.Create Query Engine and Execute Prompts**"
      ],
      "metadata": {
        "id": "KZt_12xPS5hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Create Chat Engine"
      ],
      "metadata": {
        "id": "B_9DrGkNxFQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the ChatEngine to maintain conversation history while retrieving context\n",
        "# from your index for each turn.\n",
        "chat_engine = index.as_chat_engine(\n",
        "    chat_mode=\"condense_plus_context\", # A common mode for RAG chat\n",
        "    similarity_top_k=3\n",
        ")"
      ],
      "metadata": {
        "id": "DpwmuqyAxL9k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Interactive Chat Loop"
      ],
      "metadata": {
        "id": "zxAruhrJyeHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_rag_chatbot():\n",
        "    \"\"\"\n",
        "    An interactive chatbot that grounds its answers using the RAG pipeline.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- üí¨ Interactive RAG Chatbot üí¨ ---\")\n",
        "    print(f\"Document: {pdf_path}\")\n",
        "    print(\"Ask me questions about the document (e.g., 'What is the total estimated monthly payment?').\")\n",
        "    print(\"Type 'exit' to end the conversation.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            # Use input() within a try/except to handle unexpected Colab issues\n",
        "            user_input = input(\"You: \")\n",
        "        except EOFError:\n",
        "            # This often catches issues when input is expected but not provided interactively\n",
        "            print(\"\\nExiting due to non-interactive environment.\")\n",
        "            break\n",
        "\n",
        "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "            print(\"\\nChatbot: Goodbye! Feel free to upload a new document next time.\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            # Use the chat_engine which handles history and retrieval simultaneously\n",
        "            response = chat_engine.chat(user_input)\n",
        "\n",
        "            # Print the response\n",
        "            print(f\"\\nChatbot: {response.response}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError processing query: {e}\")\n",
        "            print(\"Please try again.\")"
      ],
      "metadata": {
        "id": "reFbq0GZyc9K"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Run the chatbot"
      ],
      "metadata": {
        "id": "wIfn7bGazZ7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interactive_rag_chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "b6xhwdwTzgo0",
        "outputId": "6a59ec00-e102-4628-c5d5-aeb5aac34be3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- üí¨ Interactive RAG Chatbot üí¨ ---\n",
            "Document: LenderFeesWorksheetNew (1).pdf\n",
            "Ask me questions about the document (e.g., 'What is the total estimated monthly payment?').\n",
            "Type 'exit' to end the conversation.\n",
            "------------------------------------------------------------\n",
            "You: What is the total estimated monthly payment\n",
            "\n",
            "Chatbot: The total estimated monthly payment is **$2,308.95**.\n",
            "\n",
            "This amount is broken down into several components, as detailed in the Fees Worksheet:\n",
            "*   **Principal & Interest:** $1,869.37\n",
            "*   **Hazard Insurance:** $39.58\n",
            "*   **Real Estate Taxes:** $400.00\n",
            "*   **Mortgage Insurance:** $0.00 (This item is listed but shows $0.00 for this specific loan)\n",
            "*   **Homeowner Assn. Dues:** $0.00 (This item is listed but shows $0.00 for this specific loan)\n",
            "*   **Other Financing (P & I):** $0.00 (This item is listed but shows $0.00 for this specific loan)\n",
            "*   **Other:** $0.00 (This item is listed but shows $0.00 for this specific loan)\n",
            "\n",
            "The sum of these components ($1,869.37 + $39.58 + $400.00) equals the total estimated monthly payment of $2,308.95.\n",
            "You: exit\n",
            "\n",
            "Chatbot: Goodbye! Feel free to upload a new document next time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q2v1OebJ3KZ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}