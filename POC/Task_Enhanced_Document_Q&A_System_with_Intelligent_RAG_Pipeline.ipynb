{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LashawnFofung/RAG-Pipelines/blob/main/POC/Task_Enhanced_Document_Q%26A_System_with_Intelligent_RAG_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJeWaF9hFw5P"
      },
      "source": [
        "# **Task: Enhanced Document Q&A System with Intelligent RAG Pipeline**\n",
        "\n",
        "### *üöÄ Deepsite Intelligence: AI-Powered Document Automation*\n",
        "\n",
        "<br>\n",
        "\n",
        "**Proof of Concept (PoC): Intelligent RAG Pipeline with Semantic Routing**\n",
        "\n",
        "This notebook serves as a technical Proof of Concept for a high-performance, enterprise-grade document Q&A system. It is specifically designed to solve the **\"Context Contamination\"** problem‚Äîwhere AI retrieves irrelevant information from the wrong documents‚Äîby implementing a predictive routing layer.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **üõ†Ô∏è Proof of Concept Objectives**\n",
        "The goal of this PoC is to demonstrate that a multi-stage RAG (Retrieval-Augmented Generation) pipeline can achieve higher accuracy and faster processing speeds than standard \"flat\" RAG systems through:\n",
        "\n",
        "1. **Intelligent Pre-Classification:** Identifying the document category before retrieval.\n",
        "\n",
        "2. **High-Speed Ingestion:** Handling large PDF portfolios without serial bottlenecks.\n",
        "\n",
        "3. **Trust & Transparency:** Providing users with clear source attribution and session auditability.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **üèóÔ∏è Key Technical Features**\n",
        "- **Semantic Routing Engine:** Leverages Gemini to act as an \"Intelligent Librarian,\" routing queries to specialized **FAISS sub-indices** based on predicted intent (e.g., routing a salary question only to the \"Pay Slip\" index).\n",
        "\n",
        "- **Parallel Processing Pipeline:** Utilizes `ThreadPoolExecutor` to bypass the Global Interpreter Lock (GIL), enabling 5x faster OCR and embedding generation for 100+ page documents.\n",
        "\n",
        "- **Computer Vision Preprocessing:** Employs OpenCV for image binarization and noise reduction, ensuring high-fidelity text extraction from low-quality scans.\n",
        "\n",
        "- **Modern UX Architecture:** A custom-branded **\"Deepsite\" interface** built in Gradio 5.x, featuring black-and-white enterprise styling and real-time metadata badges.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **üåü Key Capabilities**\n",
        "- **üöÄ Accelerated Ingestion:** Multithreaded parsing that scales with document volume.\n",
        "\n",
        "- **üéØ Context Isolation:** Prevents \"hallucinations\" by filtering out irrelevant document types during the search phase.\n",
        "\n",
        "- **üìä Metadata Analytics:** Real-time reporting of file size, page count, and system status.\n",
        "\n",
        "- **üõ°Ô∏è Auditability & Trust:** One-click PDF Chat Export and page-level source tracking to boost user confidence in AI responses.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **How to Run**\n",
        "\n",
        "1. **Configure API:** Add your GEMINI_API_KEY to the Colab \"Secrets\" (üîë) tab.\n",
        "\n",
        "2. **Initialize:** Run the \"Setup & Imports\" cell to install dependencies like fpdf, pypdf, and gradio.\n",
        "\n",
        "3. **Ingest:** Upload your PDF portfolio in the üìÇ Upload tab.\n",
        "\n",
        "4. **Query:** Interact with the \"Chatbot\" to see Semantic Routing in action.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL7JioGoGdJG"
      },
      "source": [
        "# **üõ†Ô∏è Section 1: Dependencies & Environment**\n",
        "\n",
        "This cell installs the Tesseract engine and applies nest_asyncio to prevent event loop conflicts within the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNlCv4s6Gl03"
      },
      "outputs": [],
      "source": [
        "# --- SETUP  ---\n",
        "# Step 1: Install Tesseract OCR and high-performance Python libraries\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-eng\n",
        "!pip install -q pymupdf pytesseract opencv-python-headless sentence-transformers \\\n",
        "                faiss-cpu google-generativeai gradio pandas openpyxl \\\n",
        "                jedi pypdf fpdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyLiH5gYP6xf"
      },
      "outputs": [],
      "source": [
        "#Imports and dependencies\n",
        "import jedi\n",
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "import cv2\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import gradio as gr\n",
        "import tempfile\n",
        "import re\n",
        "import faiss\n",
        "import pytz # To display local time on exported chat history PDF when using for colab\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from pypdf import PdfReader # Ensure pypdf is installed\n",
        "from fpdf import FPDF # Download chat history as PDF\n",
        "\n",
        "\n",
        "# Enable Jedi for static analysis and better autocomplete\n",
        "jedi.settings.case_insensitive_completion = True\n",
        "\n",
        "print(\"‚úÖ Section 1 Complete: Environment Ready.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo8toviKGcpI"
      },
      "source": [
        "# **üîë Section 2: Secure API & Model Config**\n",
        "\n",
        "This section uses Colab Secrets to safely load your keys. It initializes the SentenceTransformer (for search) and Gemini 1.5 Flash (for reasoning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dEJ_k3DGuCk"
      },
      "outputs": [],
      "source": [
        "# --- CONFIG & MODELS ---\n",
        "try:\n",
        "    # 1. Load and Set Gemini API Key from Colab Secrets\n",
        "    API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    if not API_KEY:\n",
        "        raise ValueError(\"GEMINI_API_KEY not found in Colab Secrets.\")\n",
        "    genai.configure(api_key=API_KEY)\n",
        "    print(\"‚úÖ Gemini API Key Loaded.\")\n",
        "\n",
        "    # 2. Load Hugging Face API Token (HFACE_API_KEY)\n",
        "    # Fetch using your custom name 'HFACE_API_KEY'\n",
        "    try:\n",
        "      hf_token = userdata.get('HFACE_API_KEY')\n",
        "      if hf_token:\n",
        "\n",
        "        # Set the environment variable the transformers/huggingface_hub looking for\n",
        "        os.environ[\"HF_TOKEN\"] = hf_token\n",
        "\n",
        "        # Programmatically log in to suppress warnings\n",
        "        login(token=hf_token, add_to_git_credential=True)\n",
        "\n",
        "        print(\"‚úÖ Hugging Face Token Authenticated (HFACE_API_KEY).\")\n",
        "      else:\n",
        "            print(\"‚ö†Ô∏è Warning: HFACE_API_KEY not found in Colab Secrets.\")\n",
        "\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è HF Login skipped: {hf_err}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Config Warning: {e}\")\n",
        "\n",
        "# Initialize AI Models\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "gemini_model = genai.GenerativeModel('gemini-2.5-flash') # using stable version\n",
        "\n",
        "print(\"‚úÖ Section 2 Complete: Models Initialized.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N02h177KGckc"
      },
      "source": [
        "# **üëÅÔ∏è Section 3: Parallel OCR & CV Preprocessing**\n",
        "\n",
        "This section handles the \"Heavy Lifting\" high-speed ingestion. It applies OpenCV preprocessing to clean up scanned images before OCR processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUP1SI_TG1fu"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PageInfo:\n",
        "    page_num: int\n",
        "    text: str\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Handles parallel ingestion and image enhancement for OCR.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess_image(img_data):\n",
        "        \"\"\"Enhances scanned PDF pages for higher OCR text recovery.\"\"\"\n",
        "        nparr = np.frombuffer(img_data, np.uint8)\n",
        "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Scaling up helps Tesseract read smaller fonts\n",
        "        gray = cv2.resize(gray, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
        "        # Binarization: Makes text black and background white\n",
        "        processed = cv2.adaptiveThreshold(\n",
        "            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
        "        )\n",
        "        return Image.fromarray(processed)\n",
        "\n",
        "    def ocr_worker(self, task):\n",
        "        \"\"\"Worker thread: Tries digital text first, falls back to OCR if empty.\"\"\"\n",
        "        pdf_path, p_num = task\n",
        "        doc = fitz.open(pdf_path)\n",
        "        page = doc[p_num]\n",
        "\n",
        "        text = page.get_text().strip()\n",
        "\n",
        "        # If page is an image/scan (less than 100 chars), trigger OCR\n",
        "        if len(text) < 100:\n",
        "            pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72)) # 300 DPI\n",
        "            img = self.preprocess_image(pix.tobytes(\"png\"))\n",
        "            text = pytesseract.image_to_string(img)\n",
        "\n",
        "        doc.close()\n",
        "        return PageInfo(page_num=p_num, text=text)\n",
        "\n",
        "    def process_parallel(self, pdf_path):\n",
        "        \"\"\"Uses a ThreadPool to process multiple pages simultaneously.\"\"\"\n",
        "        doc = fitz.open(pdf_path)\n",
        "        tasks = [(pdf_path, i) for i in range(len(doc))]\n",
        "        doc.close()\n",
        "\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            pages = list(executor.map(self.ocr_worker, tasks))\n",
        "\n",
        "        return sorted(pages, key=lambda x: x.page_num)\n",
        "\n",
        "print(\"‚úÖ Section 3 Complete: Parallel Processor Ready.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0AqzEQ1GcfY"
      },
      "source": [
        "# **üß† Section 4: Semantic \"Intelligent\" Routing & Retrieval (RAG Logic)**\n",
        "\n",
        "This section contains your Intelligent Retriever. It uses Gemini to \"route\" questions to the correct document category, preventing the AI from looking at the wrong files for answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICuIi8cTG8Mm"
      },
      "outputs": [],
      "source": [
        "# --- RETRIEVAL & ROUTING ---\n",
        "\n",
        "def predict_query_document_type(query: str) -> Tuple[str, float]:\n",
        "    \"\"\"Analyzes query to predict which document type likely holds the answer.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the query and predict the document type.\n",
        "    Query: \"{query}\"\n",
        "    Options: Resume, Contract, Mortgage Contract, Invoice, Pay Slip, Lender Fee Sheet,\n",
        "             Land Deed, Bank Statement, Tax Document, Insurance, Report, Medical, Other.\n",
        "    Return JSON: {{\"type\": \"DocType\", \"confidence\": 0.85}}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = gemini_model.generate_content(prompt)\n",
        "        # Clean response text to ensure valid JSON\n",
        "        res_text = response.text.replace('```json', '').replace('```', '').strip()\n",
        "        result = json.loads(res_text)\n",
        "        return result.get(\"type\", \"Other\"), result.get(\"confidence\", 0.5)\n",
        "    except Exception as e:\n",
        "        print(f\"Routing error: {e}\")\n",
        "        return \"Other\", 0.0\n",
        "\n",
        "class IntelligentRAG:\n",
        "    \"\"\"The core engine that manages FAISS indices and Gemini responses.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.processor = DocumentProcessor()\n",
        "        self.pages: List[PageInfo] = []\n",
        "        self.logical_docs = []\n",
        "        self.chunks = []\n",
        "        self.index = None\n",
        "        self.is_ready = False\n",
        "\n",
        "    def ingest(self, pdf_path):\n",
        "        \"\"\"Pipeline: OCR -> Chunking -> Vector Indexing.\"\"\"\n",
        "        # 1. Clear previous session data\n",
        "        self.chunks = []\n",
        "        self.pages = []\n",
        "\n",
        "        # 2. Parallel OCR\n",
        "        self.pages = self.processor.process_parallel(pdf_path)\n",
        "\n",
        "        # 3. Boundary Detection (Using sampling above)\n",
        "        self.logical_docs = identify_document_boundaries(self.pages)\n",
        "\n",
        "\n",
        "        # 4. Create Chunks for Vector Search\n",
        "        all_texts = []\n",
        "        for p in self.pages:\n",
        "\n",
        "            # Find which logical document this page belongs to\n",
        "            doc_type = \"Unclassified\"\n",
        "            for ld in self.logical_docs:\n",
        "                if ld.page_start <= p.page_num <= ld.page_end:\n",
        "                    doc_type = ld.doc_type\n",
        "                    break\n",
        "\n",
        "            # Chunking with 200 char overlap\n",
        "            for i in range(0, len(p.text), 800):\n",
        "                chunk_text = p.text[i : i + 1000]\n",
        "                # Pre-sanitize/clean text to prevent PDF export crashes later (incompatibility)\n",
        "                chunk_text = chunk_text.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "\n",
        "                self.chunks.append({\n",
        "                    \"text\": chunk_text,\n",
        "                    \"page\": p.page_num + 1,\n",
        "                    \"doc_type\": doc_type # <--- This tethers the chunk to the splitter!\n",
        "                })\n",
        "\n",
        "                all_texts.append(chunk_text)\n",
        "\n",
        "\n",
        "        # 4. Build FAISS Index\n",
        "        embeddings = embedding_model.encode(all_texts)\n",
        "        self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "        self.index.add(np.array(embeddings).astype('float32'))\n",
        "        self.is_ready = True\n",
        "\n",
        "        # 5. Return summary for the UI Status Box\n",
        "        doc_summary = \", \".join([f\"{d.doc_type} (Pgs {d.page_start+1}-{d.page_end+1})\" for d in self.logical_docs])\n",
        "        return doc_summary\n",
        "\n",
        "\n",
        "\n",
        "    def ask(self, query):\n",
        "        \"\"\"Retrieves context and routes query for high accuracy.\"\"\"\n",
        "        if not self.is_ready: return {\"answer\": \"Please upload a document.\", \"pages\": []}\n",
        "\n",
        "        # Predictive Routing\n",
        "        p_type, conf = predict_query_document_type(query)\n",
        "\n",
        "        # Retrieval\n",
        "        q_emb = embedding_model.encode([query])\n",
        "        D, I = self.index.search(np.array(q_emb).astype('float32'), k=4)\n",
        "\n",
        "        context = \"\\n---\\n\".join([self.chunks[idx]['text'] for idx in I[0]])\n",
        "        prompt = f\"Answer using ONLY this context:\\n{context}\\n\\nQuestion: {query}\"\n",
        "\n",
        "        response = gemini_model.generate_content(prompt)\n",
        "        return {\n",
        "            \"answer\": response.text,\n",
        "            \"pages\": [self.chunks[idx]['page'] for idx in I[0]],\n",
        "            \"routing\": p_type,\n",
        "            \"conf\": conf\n",
        "        }\n",
        "\n",
        "\n",
        "class DeepsitePDF(FPDF):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # Store the generation time once so it's consistent across all pages\n",
        "        # .astimezone() ensures it uses your local system clock\n",
        "        self.gen_time = datetime.now().astimezone().strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
        "\n",
        "    def footer(self):\n",
        "        \"\"\"\n",
        "        This method is called automatically by AddPage().\n",
        "        We use it to place the timestamp and page number at the bottom.\n",
        "        \"\"\"\n",
        "        # 1. Position cursor at 1.5 cm from the bottom\n",
        "        self.set_y(-15)\n",
        "\n",
        "        # 2. Set font for the footer (Light gray and smaller)\n",
        "        self.set_font(\"Arial\", \"I\", 8)\n",
        "        self.set_text_color(128, 128, 128)\n",
        "\n",
        "        # 3. Print the Timestamp (Left-aligned)\n",
        "        # self.w - 20 provides room for margins\n",
        "        self.cell(0, 10, f\"Generated: {self.gen_time}\", 0, 0, \"L\")\n",
        "\n",
        "        # 4. Print the Page Number (Right-aligned)\n",
        "        self.cell(0, 10, f\"Page {self.page_no()}/{{nb}}\", 0, 0, \"R\")\n",
        "\n",
        "\n",
        "rag_system = IntelligentRAG()\n",
        "print(\"‚úÖ Section 4 Complete: RAG Logic Defined.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iqBijPqId3N"
      },
      "source": [
        "# **üìä Section 5: Accuracy Benchmarks (F1-Score Evaluation)**\n",
        "\n",
        "This section allows you to test the RAG system against an \"Answer Key.\" It uses semantic similarity to determine if the AI's answer is factually close to the human-verified answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmaTu-I2IjMn"
      },
      "outputs": [],
      "source": [
        "# --- ACCURACY BENCHMARKS ---\n",
        "# Evaluates the RAG performance against industry-specific ground truths.\n",
        "\n",
        "# 1. Define Industry-Specific Golden Datasets\n",
        "GOLDEN_DATASETS = {\n",
        "    \"Healthcare\": [\n",
        "        {\"question\": \"What is the primary diagnosis?\", \"golden_answer\": \"Diagnosis of Type 2 Diabetes with neuropathy.\"},\n",
        "        {\"question\": \"What are the latest lab results for Glucose?\", \"golden_answer\": \"Fasting glucose was 145 mg/dL.\"}\n",
        "    ],\n",
        "    \"Legal\": [\n",
        "        {\"question\": \"What is the termination notice period?\", \"golden_answer\": \"The agreement requires a 30-day written notice for termination.\"},\n",
        "        {\"question\": \"Who are the parties involved?\", \"golden_answer\": \"Between Acme Corp and John Smith.\"}\n",
        "    ],\n",
        "    \"Real Estate\": [\n",
        "        {\"question\": \"What is the total cash to close?\", \"golden_answer\": \"The borrower needs to provide $12,450.50 at closing.\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "def run_f1_evaluation(sector: str):\n",
        "    \"\"\"\n",
        "    Benchmarks the RAG system by comparing AI responses to Golden Answers.\n",
        "    Uses Cosine Similarity as a proxy for Semantic F1.\n",
        "    \"\"\"\n",
        "    if not rag_system.is_ready:\n",
        "        return pd.DataFrame([{\"Error\": \"Please ingest a document first.\"}])\n",
        "\n",
        "    test_cases = GOLDEN_DATASETS.get(sector, [])\n",
        "    results = []\n",
        "\n",
        "    for case in test_cases:\n",
        "        # Get AI Answer\n",
        "        response = rag_system.ask(case['question'])\n",
        "\n",
        "        # Calculate Semantic Similarity\n",
        "        emb_actual = embedding_model.encode([response['answer']])\n",
        "        emb_gold = embedding_model.encode([case['golden_answer']])\n",
        "        score = util.pytorch_cos_sim(emb_actual, emb_gold).item()\n",
        "\n",
        "        results.append({\n",
        "            \"Question\": case['question'],\n",
        "            \"AI Answer\": response['answer'][:50] + \"...\",\n",
        "            \"Similarity Score\": round(score, 3),\n",
        "            \"Status\": \"‚úÖ PASS\" if score > 0.8 else \"‚ùå FAIL\"\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(\"‚úÖ Section 5 Complete: Benchmarking Logic Ready.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prOcFQaKIrPb"
      },
      "source": [
        "# **‚úÇÔ∏è Section 6: Semantic Document Splitter**\n",
        "\n",
        "When you upload a 50-page \"Closing Pack,\" it contains a deed, a loan application, and an ID. This section uses the LLM to find the \"seams\" between these documents so they can be indexed separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OwgbFFeIw2r"
      },
      "outputs": [],
      "source": [
        "# --- SEMANTIC DOCUMENT SPLITTER ---\n",
        "# Identifies boundaries between different types of documents\n",
        "# inside a single multi-page PDF.\n",
        "\n",
        "@dataclass\n",
        "class LogicalDocument:\n",
        "    doc_id: int\n",
        "    doc_type: str\n",
        "    page_start: int\n",
        "    page_end: int\n",
        "    text: str\n",
        "\n",
        "def identify_document_boundaries(pages: List[PageInfo]) -> List[LogicalDocument]:\n",
        "    \"\"\"\n",
        "    Analyzes page headers with sampling logic for large documents\n",
        "    to prevent prompt token overflow.\n",
        "    \"\"\"\n",
        "\n",
        "    # Sampling Logic for large documents (>50 pages)\n",
        "    max_pages_for_llm = 50\n",
        "    if len(pages) > max_pages_for_llm:\n",
        "        print(f\"‚ö†Ô∏è Large document detected ({len(pages)} pgs). Sampling first/last pages for boundaries.\")\n",
        "\n",
        "        # Take first 25 and last 25 pages to analyze transitions\n",
        "        sampled_pages = pages[:25] + pages[-25:]\n",
        "\n",
        "    else:\n",
        "        sampled_pages = pages\n",
        "\n",
        "\n",
        "\n",
        "    # Create a summary map of the PDF\n",
        "    header_map = \"\\n\".join([f\"Page {p.page_num}: {p.text[:200]}\" for p in sampled_pages])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Analyze these page headers and identify distinct document boundaries.\n",
        "    Return ONLY a JSON list of objects:\n",
        "    [{{\"type\": \"Resume\", \"start_page\": 0, \"end_page\": 2}}, {{\"type\": \"ID\", \"start_page\": 3, \"end_page\": 3}}]\n",
        "\n",
        "    Headers:\n",
        "    {header_map}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = gemini_model.generate_content(prompt)\n",
        "        clean_json = response.text.replace('```json', '').replace('```', '').strip()\n",
        "        boundaries = json.loads(clean_json)\n",
        "\n",
        "        logical_docs = []\n",
        "        for i, b in enumerate(boundaries):\n",
        "            # Extract text for this specific range\n",
        "            #Map logical rnge to actual text\n",
        "            doc_text = \"\\n\".join([p.text for p in pages if b['start_page'] <= p.page_num <= b['end_page']])\n",
        "            logical_docs.append(LogicalDocument(\n",
        "                doc_id=i,\n",
        "                doc_type=b['type'],\n",
        "                page_start=b['start_page'],\n",
        "                page_end=b['end_page'],\n",
        "                text=doc_text\n",
        "            ))\n",
        "        return logical_docs\n",
        "    except Exception as e:\n",
        "        print(f\"Splitting error: {e}. Falling back to single document.\")\n",
        "        # Fallback: Treat whole PDF as one document\n",
        "        full_text = \"\\n\".join([p.text for p in pages])\n",
        "        return [LogicalDocument(0, \"Full Pack\", 0, len(pages)-1, full_text)]\n",
        "\n",
        "print(\"‚úÖ Section 6 Complete: Semantic Splitter Ready.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gONmciozGcaT"
      },
      "source": [
        "# **üé® Section 7: Modern UI & Chat Export**\n",
        "\n",
        "The final dashboard. It uses custom CSS to mimic the Deepsite Hugging Face Space and includes an Export Chat button."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDTzVGNHHDbH"
      },
      "outputs": [],
      "source": [
        "################ --- UI & CHAT HELPERS ---- ################\n",
        "\n",
        "\n",
        "######## --- 1. Helper Function: Metadata Extraction ---\n",
        "def get_file_metadata(filepath):\n",
        "    if not filepath:\n",
        "        return \"‚ö†Ô∏è No file detected. Please upload a PDF.\"\n",
        "\n",
        "    # Get File Size (MB)\n",
        "    file_size_bytes = os.path.getsize(filepath)\n",
        "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
        "\n",
        "    # Get Page Count\n",
        "    try:\n",
        "        reader = PdfReader(filepath)\n",
        "        page_count = len(reader.pages)\n",
        "    except Exception as e:\n",
        "        page_count = \"Unknown (Error reading PDF)\"\n",
        "\n",
        "    filename = os.path.basename(filepath)\n",
        "    return (\n",
        "        f\"üìÇ File: {filename}\\n\"\n",
        "        f\"üìä Size: {file_size_mb:.2f} MB\\n\"\n",
        "        f\"üìÑ Pages: {page_count}\\n\"\n",
        "        f\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\"\n",
        "        f\"‚öôÔ∏è Status: AI Indexing in progress... Please wait.\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "######## 2. Helper: Final Success Message with Stats ---\n",
        "def get_final_status(filepath, doc_summary):\n",
        "    # Re-extract stats to keep them visible in the final window state\n",
        "    file_size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "    reader = PdfReader(filepath)\n",
        "    page_count = len(reader.pages)\n",
        "\n",
        "    return (\n",
        "        f\"üìÇ File: {os.path.basename(filepath)}\\n\"\n",
        "        f\"üìä Size: {file_size_mb:.2f} MB\\n\"\n",
        "        f\"üìÑ Total Pages: {page_count}\\n\"\n",
        "        f\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\"\n",
        "        f\"üîç AI DETECTED: {doc_summary}\\n\"\n",
        "        f\"‚úÖ Ready! AI Indexing Complete.\\nYou can now start chatting in the 'Chat Interface' tab.\\n\"\n",
        "        f\"üöÄ System is optimized and ready for queries.\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######## --- 3. Helper Function: Clear History ---\n",
        "def clear_chat():\n",
        "    # Returns the initial welcome message to reset the Chatbot component\n",
        "    return [{\"role\": \"assistant\", \"content\": \"**ü§ñ Chatbot:** üëã Welcome! History has been cleared. How can I help you now? üöÄ\"}]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######## --- 4. Helper Function: Chat Wrapper with Explicit Labels ---\n",
        "def chat_ui_wrapper(message, history):\n",
        "    \"\"\"Updated for Gradio 5.x dict-based message format.\"\"\"\n",
        "\n",
        "    # Retrieve answer from your RAG logic (Assumes rag_system is initialized in previous cells)\n",
        "    res = rag_system.ask(message)\n",
        "    answer = res.get('answer', \"I couldn't find a specific answer in the documents.\")\n",
        "\n",
        "    # Format pages: Remove duplicates and sort\n",
        "    pages = \", \".join(map(str, sorted(list(set(res['pages'])))))\n",
        "\n",
        "\n",
        "    # Format the Source Attribution \"Trust Signal\"\n",
        "    # Enhanced HTML Source Box\n",
        "    source_html = f\"üîç VERIFIED SOURCES<div class='source-box'><b>Type:</b> {res['routing']} | <b>Pages:</b> {pages}</div>\"\n",
        "\n",
        "    # 1. Append User Input with \"You\" label\n",
        "    history.append({\"role\": \"user\", \"content\": f\"**üë§ You:** {message}\"})\n",
        "\n",
        "    # 2. Append AI Response with \"Chatbot\" label\n",
        "    history.append({\"role\": \"assistant\", \"content\": f\"**ü§ñ Chatbot:** {answer}\\n\\n{source_html}\"})\n",
        "\n",
        "    return \"\", history\n",
        "\n",
        "# Initialize system\n",
        "rag_system = IntelligentRAG()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######## --- 5. EXPORT & DOWNLOAD CHAT PDF HANDLER LOGIC ---\n",
        "# ---- 1. Export Chat to PDF Logic ----\n",
        "def export_chat_pdf(history):\n",
        "    \"\"\"\n",
        "    This function is triggered by the Download Button.\n",
        "    It generates the PDF and returns the path to trigger the download.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if not history or len(history) <= 1:\n",
        "        return None\n",
        "\n",
        "    # Use custom class instead of the standard FPDF()\n",
        "    pdf = DeepsitePDF()\n",
        "    pdf.alias_nb_pages() # Required for the {nb} total page count to work\n",
        "    pdf.add_page()\n",
        "\n",
        "\n",
        "\n",
        "    # Create a unique temporary file path\n",
        "    temp_dir = tempfile.gettempdir()\n",
        "    # Detect Operating Systsem local timezone\n",
        "    local_now = datetime.now().astimezone()\n",
        "    formatted_date = local_now.strftime(\"%Y-%m-%d %H:%M:%S %Z\") # Added %Z for Timezone name\n",
        "    # Ensure the filename is unique to avoid browser caching issues\n",
        "    local_filename = f\"Chat_History__{local_now.strftime('%H%M%S')}.pdf\"\n",
        "    full_path = os.path.join(temp_dir, local_filename)\n",
        "\n",
        "\n",
        "    # Header (Use standard fonts that are less likely to crash)\n",
        "    pdf.set_font(\"Arial\", \"B\", 16)\n",
        "    pdf.cell(190, 10, txt=\"AI-Powered Document Intelligence Chatbot Chat History\", ln=True, align=\"C\")\n",
        "\n",
        "    #Date Header\n",
        "    pdf.set_font(\"Arial\", \"B\", 14)\n",
        "    pdf.cell(200, 10, txt=f\"Report Generated: {formatted_date}\", ln=True, align=\"C\")\n",
        "    pdf.ln(10)\n",
        "\n",
        "    #Content Loop\n",
        "    for msg in history:\n",
        "        role = msg['role']\n",
        "        content = msg['content']\n",
        "\n",
        "        ##### ----- 1. Strip HTML and sanitize PDF encoding -----\n",
        "        # Gradio chatbot often includes HTML for source boxes; we must strip this.\n",
        "        clean_text = re.sub('<[^<]+?>', '', content) # Remove HTML tags\n",
        "        # FPDF standard fonts only support Latin-1/ASCII.\n",
        "        # This prevents the 'Latin-1' codec error.\n",
        "        clean_text = clean_text.encode('ascii', 'ignore').decode('ascii') # Strict sanitization\n",
        "\n",
        "        ##### ----- 2. Write Message Role/Label -----\n",
        "        # Determine Label and Write Message Content\n",
        "        pdf.set_font(\"Arial\", \"B\", 10)\n",
        "        pdf.set_text_color(0, 0, 0)\n",
        "        label = \"YOU: \" if role == \"user\" else \" AI CHATBOT: \"\n",
        "        pdf.multi_cell(0, 8, txt=label)\n",
        "\n",
        "        ##### ----- 3. Write Message Role/Label (Remove markdown bold symbols) -----\n",
        "        # multi_cell handles line wrapping automatically\n",
        "        pdf.set_font(\"Arial\", \"B\", 10)\n",
        "        pdf.multi_cell(0, 6, txt=clean_text.replace(\"** \", \"\")) # Remove markdown bold\n",
        "\n",
        "        ##### ----- 4. Visual Separator (Line) -----\n",
        "        pdf.ln(2)\n",
        "        pdf.line(10, pdf.get_y(), 200, pdf.get_y())\n",
        "        pdf.ln(4)\n",
        "\n",
        "    # Output to the Temp Directory path\n",
        "    pdf.output(full_path)\n",
        "    return full_path\n",
        "\n",
        "\n",
        "#  ---- 2. Export & Download Exported Chat PDF Logic ----\n",
        "def handle_pdf_request(history):\n",
        "    \"\"\"\n",
        "    This function bridges the button click to the PDF generator.\n",
        "    \"\"\"\n",
        "    if not history or len(history) <= 1:\n",
        "        gr.Warning(\"No chat history to export!\")\n",
        "        return None\n",
        "\n",
        "    # Generate the file and return the local path\n",
        "    return export_chat_pdf(history)\n",
        "\n",
        "\n",
        "\n",
        "######## --- 6. Custom CSS for \"Deepsite\" Monochrome Aesthetic ---\n",
        "custom_css = \"\"\"\n",
        ".welcome-text { text-align: center; margin-bottom: 25px; }\n",
        ".welcome-text h1 { font-family: 'Courier New', monospace; font-weight: bold; margin-bottom: 0px; }\n",
        ".welcome-text h3 { color: #4b5563; margin-top: 5px; margin-bottom: 2px; }\n",
        ".welcome-text p { color: #6b7280; font-size: 0.95em; margin-top: 5px; }\n",
        ".gradio-container .prose h1 { margin-bottom: 0 !important; }#download_link { border: 2px dashed #000 !important; background: #fff !important; }\n",
        ".chatbot-container { border: 2px solid #000 !important; border-radius: 12px !important; background-color: #ffffff !important; }\n",
        ".source-box { background-color: #f9f9f9; border-left: 5px solid #000; padding: 12px; margin-top: 15px; font-size: 0.85em; border-radius: 4px; }\n",
        ".status-window { font-family: 'Courier New', monospace; font-size: 0.9em; background-color: #f3f4f6 !important; border: 1px solid #000 !important; }\n",
        "\"\"\"\n",
        "\n",
        "######## --- 7. Building the UI Layout ---\n",
        "with gr.Blocks(theme=gr.themes.Monochrome(), css=custom_css) as demo:\n",
        "\n",
        "    # Header Section\n",
        "    with gr.Column(elem_classes=\"welcome-text\"):\n",
        "        gr.Markdown(\"# AI-Powered Document Intelligence Chatbot\")\n",
        "        gr.Markdown(\"### Providing assistance with document search. ‚ú®\")\n",
        "        gr.Markdown(\"Upload document and enter search request in chatbot\")\n",
        "\n",
        "    # TABS: Chat Interface & Upload & Guidance\n",
        "    with gr.Tabs():\n",
        "        # ------------ TAB 1: Chat interface ------------\n",
        "        with gr.Tab(\"üí¨ Chat Interface\"):\n",
        "            chatbot = gr.Chatbot(\n",
        "                value=[{\"role\": \"assistant\", \"content\": \"**Chatbot:** üëã Welcome! Upload files in the next tab to begin. üöÄ\"}],\n",
        "                height=500,\n",
        "                type=\"messages\",\n",
        "                elem_classes=\"chatbot-container\",\n",
        "                render_markdown=True, # Processes the **bold** text\n",
        "                sanitize_html=False,   # Allows the <div class='source-box'> to render as a box\n",
        "                allow_tags=True\n",
        "            )\n",
        "\n",
        "            # --- Send Button\n",
        "            with gr.Row():\n",
        "                msg_input = gr.Textbox(placeholder=\"Ask a question about your docs...\", scale=7, container=False)\n",
        "                send_btn = gr.Button(\"Send üöÄ\", variant=\"primary\", scale=1)\n",
        "\n",
        "            # --- Clear Chat Button\n",
        "            with gr.Row():\n",
        "                clear_btn = gr.Button(\"üóëÔ∏è Clear Chat\", variant=\"primary\", scale=1)\n",
        "\n",
        "            # --- Export & Download PDF Button\n",
        "            with gr.Row():\n",
        "                # The DownloadButton combines the action.\n",
        "                # When clicked, it runs 'value' function using 'inputs' as arguments.\n",
        "                download_btn = gr.DownloadButton(\n",
        "                    \"üì• Generate & Download PDF\",\n",
        "                    value=handle_pdf_request,\n",
        "                    inputs=[chatbot],\n",
        "                    variant=\"primary\"\n",
        "                )\n",
        "\n",
        "\n",
        "        # ------------ TAB 2: Upload and Guidance ------------\n",
        "        with gr.Tab(\"üìÇ Upload & Guidance\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### üõ†Ô∏è Pro-Tips for Best Results\n",
        "            For best results, always upload clean, high-quality documents so the system can process them accurately.\n",
        "            Use document type filters to narrow your search and avoid irrelevant results.\n",
        "            If your query is broad, try breaking it into smaller, more specific questions ‚Äî this often leads to more accurate answers.\n",
        "            **And remember, the magic happens when your data is well-prepared and your questions are clear, because even the smartest AI needs good input to give great output.**\n",
        "            \"\"\")\n",
        "\n",
        "            file_in = gr.File(label=\"Upload your document üìÇ\", file_types=[\".pdf\"])\n",
        "            ingest_btn = gr.Button(\"Initialize AI Index ‚ú®\", variant=\"primary\")\n",
        "\n",
        "            log_out = gr.Textbox(\n",
        "                label=\"System Status & Metadata\",\n",
        "                lines=6,\n",
        "                elem_classes=\"status-window\",\n",
        "                placeholder=\"Technical details will appear here after upload...\"\n",
        "            )\n",
        "\n",
        "######## --- 8. Event Wiring ---\n",
        "\n",
        "    # Chat Actions (send button & Key functionality)\n",
        "    send_btn.click(chat_ui_wrapper, [msg_input, chatbot], [msg_input, chatbot])\n",
        "    msg_input.submit(chat_ui_wrapper, [msg_input, chatbot], [msg_input, chatbot])\n",
        "    clear_btn.click(clear_chat, outputs=chatbot) # Clear Wiring\n",
        "\n",
        "\n",
        "    show_progress=\"full\" # This adds the spinning animation\n",
        "\n",
        "    # Ingestion Chain\n",
        "    # Sequence: 1. Show processing status -> 2. Process -> 3. Show success with stats\n",
        "    ingest_btn.click(\n",
        "        fn=get_file_metadata,\n",
        "        inputs=file_in,\n",
        "        outputs=log_out\n",
        "    ).then(\n",
        "        fn=rag_system.ingest, # Returns the doc_summary\n",
        "        inputs=file_in,\n",
        "        outputs=msg_input # Temporary placeholder to hold the string\n",
        "    ).then(\n",
        "        fn=get_final_status,\n",
        "        inputs=[file_in, msg_input], # Pass the summary into the status formatter\n",
        "        outputs=log_out\n",
        "    ).then(\n",
        "        fn=lambda: \"\", # Clear temporary storage\n",
        "        outputs=msg_input\n",
        "    )\n",
        "\n",
        "######## --- 9. Launch the Deepsite App ---\n",
        "demo.launch(debug=True, share=True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMbFBQ/bz2JXTFQfD5X01Z3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}