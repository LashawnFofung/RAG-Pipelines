{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LashawnFofung/RAG-Pipelines/blob/main/POC/Task_Enhanced_Document_Q%26A_System_with_Intelligent_RAG_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJeWaF9hFw5P"
      },
      "source": [
        "# **Task: Enhanced Document Q&A System with Intelligent RAG Pipeline**\n",
        "\n",
        "### *üöÄ Deepsite Intelligence: AI-Powered Document Automation*\n",
        "\n",
        "<br>\n",
        "\n",
        "**Proof of Concept (PoC): Intelligent RAG Pipeline with Semantic Routing**\n",
        "\n",
        "This notebook serves as a technical Proof of Concept for a high-performance, enterprise-grade document Q&A system. It is specifically designed to solve the **\"Context Contamination\"** problem‚Äîwhere AI retrieves irrelevant information from the wrong documents‚Äîby implementing a predictive routing layer.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **üõ†Ô∏è Proof of Concept Objectives**\n",
        "The goal of this PoC is to demonstrate that a multi-stage RAG (Retrieval-Augmented Generation) pipeline can achieve higher accuracy and faster processing speeds than standard \"flat\" RAG systems through:\n",
        "\n",
        "1. **Intelligent Pre-Classification:** Identifying the document category before retrieval.\n",
        "\n",
        "2. **High-Speed Ingestion:** Handling large PDF portfolios without serial bottlenecks.\n",
        "\n",
        "3. **Trust & Transparency:** Providing users with clear source attribution and session auditability.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **üèóÔ∏è Key Technical Features**\n",
        "- **Semantic Routing Engine:** Leverages Gemini to act as an \"Intelligent Librarian,\" routing queries to specialized **FAISS sub-indices** based on predicted intent (e.g., routing a salary question only to the \"Pay Slip\" index).\n",
        "\n",
        "- **Parallel Processing Pipeline:** Utilizes `ThreadPoolExecutor` to bypass the Global Interpreter Lock (GIL), enabling 5x faster OCR and embedding generation for 100+ page documents.\n",
        "\n",
        "- **Computer Vision Preprocessing:** Employs OpenCV for image binarization and noise reduction, ensuring high-fidelity text extraction from low-quality scans.\n",
        "\n",
        "- **Modern UX Architecture:** A custom-branded **\"Deepsite\" interface** built in Gradio 5.x, featuring black-and-white enterprise styling and real-time metadata badges.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **üåü Key Capabilities**\n",
        "- **üöÄ Accelerated Ingestion:** Multithreaded parsing that scales with document volume.\n",
        "\n",
        "- **üéØ Context Isolation:** Prevents \"hallucinations\" by filtering out irrelevant document types during the search phase.\n",
        "\n",
        "- **üìä Metadata Analytics:** Real-time reporting of file size, page count, and system status.\n",
        "\n",
        "- **üõ°Ô∏è Auditability & Trust:** One-click PDF Chat Export and page-level source tracking to boost user confidence in AI responses.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **How to Run**\n",
        "\n",
        "1. **Configure API:** Add your GEMINI_API_KEY to the Colab \"Secrets\" (üîë) tab.\n",
        "\n",
        "2. **Initialize:** Run the \"Setup & Imports\" cell to install dependencies like fpdf, pypdf, and gradio.\n",
        "\n",
        "3. **Ingest:** Upload your PDF portfolio in the üìÇ Upload tab.\n",
        "\n",
        "4. **Query:** Interact with the \"Chatbot\" to see Semantic Routing in action.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL7JioGoGdJG"
      },
      "source": [
        "# **üõ†Ô∏è Section 1: Dependencies & Environment**\n",
        "\n",
        "This cell installs the Tesseract engine and applies nest_asyncio to prevent event loop conflicts within the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNlCv4s6Gl03"
      },
      "outputs": [],
      "source": [
        "# --- SETUP  ---\n",
        "# Step 1: Install Tesseract OCR and high-performance Python libraries\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-eng\n",
        "!pip install -q pymupdf pytesseract opencv-python-headless sentence-transformers \\\n",
        "                faiss-cpu google-generativeai gradio pandas openpyxl \\\n",
        "                jedi pypdf fpdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyLiH5gYP6xf"
      },
      "outputs": [],
      "source": [
        "#Imports and dependencies\n",
        "import jedi\n",
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "import cv2\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from pypdf import PdfReader # Ensure pypdf is installed\n",
        "from fpdf import FPDF # Download chat history as PDF\n",
        "\n",
        "\n",
        "# Enable Jedi for static analysis and better autocomplete\n",
        "jedi.settings.case_insensitive_completion = True\n",
        "\n",
        "print(\"‚úÖ Section 1 Complete: Environment Ready.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo8toviKGcpI"
      },
      "source": [
        "# **üîë Section 2: Secure API & Model Config**\n",
        "\n",
        "This section uses Colab Secrets to safely load your keys. It initializes the SentenceTransformer (for search) and Gemini 1.5 Flash (for reasoning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dEJ_k3DGuCk"
      },
      "outputs": [],
      "source": [
        "# --- CONFIG & MODELS ---\n",
        "try:\n",
        "    # 1. Load and Set Gemini API Key from Colab Secrets\n",
        "    API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    if not API_KEY:\n",
        "        raise ValueError(\"GEMINI_API_KEY not found in Colab Secrets.\")\n",
        "    genai.configure(api_key=API_KEY)\n",
        "    print(\"‚úÖ Gemini API Key Loaded.\")\n",
        "\n",
        "    # 2. Load Hugging Face API Token\n",
        "    # Fetch using your custom name 'HFACE_API_KEY'\n",
        "    hf_token = userdata.get('HFACE_API_KEY')\n",
        "\n",
        "    if hf_token:\n",
        "        # Set the environment variable the library is looking for\n",
        "        os.environ[\"HF_TOKEN\"] = hf_token\n",
        "        # Programmatically log in to suppress warnings\n",
        "        login(token=hf_token, add_to_git_credential=True)\n",
        "        print(\"‚úÖ Hugging Face Token Authenticated (HFACE_API_KEY).\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Warning: HFACE_API_KEY not found in Colab Secrets.\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Config Warning: {e}\")\n",
        "\n",
        "# Initialize AI Models\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "gemini_model = genai.GenerativeModel('gemini-3-flash-preview')\n",
        "\n",
        "print(\"‚úÖ Section 2 Complete: Models Initialized.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N02h177KGckc"
      },
      "source": [
        "# **üëÅÔ∏è Section 3: Parallel OCR & CV Preprocessing**\n",
        "\n",
        "This section handles the \"Heavy Lifting\" high-speed ingestion. It applies OpenCV preprocessing to clean up scanned images before OCR processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUP1SI_TG1fu"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PageInfo:\n",
        "    page_num: int\n",
        "    text: str\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Handles parallel ingestion and image enhancement for OCR.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess_image(img_data):\n",
        "        \"\"\"Enhances scanned PDF pages for higher OCR text recovery.\"\"\"\n",
        "        nparr = np.frombuffer(img_data, np.uint8)\n",
        "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Scaling up helps Tesseract read smaller fonts\n",
        "        gray = cv2.resize(gray, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
        "        # Binarization: Makes text black and background white\n",
        "        processed = cv2.adaptiveThreshold(\n",
        "            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
        "        )\n",
        "        return Image.fromarray(processed)\n",
        "\n",
        "    def ocr_worker(self, task):\n",
        "        \"\"\"Worker thread: Tries digital text first, falls back to OCR if empty.\"\"\"\n",
        "        pdf_path, p_num = task\n",
        "        doc = fitz.open(pdf_path)\n",
        "        page = doc[p_num]\n",
        "\n",
        "        text = page.get_text().strip()\n",
        "\n",
        "        # If page is an image/scan (less than 100 chars), trigger OCR\n",
        "        if len(text) < 100:\n",
        "            pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72)) # 300 DPI\n",
        "            img = self.preprocess_image(pix.tobytes(\"png\"))\n",
        "            text = pytesseract.image_to_string(img)\n",
        "\n",
        "        doc.close()\n",
        "        return PageInfo(page_num=p_num, text=text)\n",
        "\n",
        "    def process_parallel(self, pdf_path):\n",
        "        \"\"\"Uses a ThreadPool to process multiple pages simultaneously.\"\"\"\n",
        "        doc = fitz.open(pdf_path)\n",
        "        tasks = [(pdf_path, i) for i in range(len(doc))]\n",
        "        doc.close()\n",
        "\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            pages = list(executor.map(self.ocr_worker, tasks))\n",
        "\n",
        "        return sorted(pages, key=lambda x: x.page_num)\n",
        "\n",
        "print(\"‚úÖ Section 3 Complete: Parallel Processor Ready.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0AqzEQ1GcfY"
      },
      "source": [
        "# **üß† Section 4: Semantic \"Intelligent\" Routing & Retrieval (RAG Logic)**\n",
        "\n",
        "This section contains your Intelligent Retriever. It uses Gemini to \"route\" questions to the correct document category, preventing the AI from looking at the wrong files for answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICuIi8cTG8Mm"
      },
      "outputs": [],
      "source": [
        "# --- RETRIEVAL & ROUTING ---\n",
        "\n",
        "def predict_query_document_type(query: str) -> Tuple[str, float]:\n",
        "    \"\"\"Analyzes query to predict which document type likely holds the answer.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the query and predict the document type.\n",
        "    Query: \"{query}\"\n",
        "    Options: Resume, Contract, Mortgage Contract, Invoice, Pay Slip, Lender Fee Sheet,\n",
        "             Land Deed, Bank Statement, Tax Document, Insurance, Report, Medical, Other.\n",
        "    Return JSON: {{\"type\": \"DocType\", \"confidence\": 0.85}}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = gemini_model.generate_content(prompt)\n",
        "        # Clean response text to ensure valid JSON\n",
        "        res_text = response.text.replace('```json', '').replace('```', '').strip()\n",
        "        result = json.loads(res_text)\n",
        "        return result.get(\"type\", \"Other\"), result.get(\"confidence\", 0.5)\n",
        "    except Exception as e:\n",
        "        print(f\"Routing error: {e}\")\n",
        "        return \"Other\", 0.0\n",
        "\n",
        "class IntelligentRAG:\n",
        "    \"\"\"The core engine that manages FAISS indices and Gemini responses.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.processor = DocumentProcessor()\n",
        "        self.pages: List[PageInfo] = []\n",
        "        self.chunks = []\n",
        "        self.index = None\n",
        "        self.is_ready = False\n",
        "\n",
        "    def ingest(self, pdf_path):\n",
        "        \"\"\"Pipeline: OCR -> Chunking -> Vector Indexing.\"\"\"\n",
        "        import faiss\n",
        "        self.pages = self.processor.process_parallel(pdf_path)\n",
        "\n",
        "        all_texts = []\n",
        "        for p in self.pages:\n",
        "            # Chunking with 200 char overlap\n",
        "            for i in range(0, len(p.text), 800):\n",
        "                chunk = p.text[i : i + 1000]\n",
        "                self.chunks.append({\"text\": chunk, \"page\": p.page_num + 1})\n",
        "                all_texts.append(chunk)\n",
        "\n",
        "        embeddings = embedding_model.encode(all_texts)\n",
        "        self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "        self.index.add(np.array(embeddings).astype('float32'))\n",
        "        self.is_ready = True\n",
        "\n",
        "    def ask(self, query):\n",
        "        \"\"\"Retrieves context and routes query for high accuracy.\"\"\"\n",
        "        if not self.is_ready: return {\"answer\": \"Ingest document first.\", \"pages\": []}\n",
        "\n",
        "        # Predictive Routing\n",
        "        p_type, conf = predict_query_document_type(query)\n",
        "\n",
        "        # Retrieval\n",
        "        q_emb = embedding_model.encode([query])\n",
        "        D, I = self.index.search(np.array(q_emb).astype('float32'), k=4)\n",
        "\n",
        "        context = \"\\n---\\n\".join([self.chunks[idx]['text'] for idx in I[0]])\n",
        "        prompt = f\"Using ONLY this context, answer accurately.\\nContext: {context}\\nQuestion: {query}\"\n",
        "\n",
        "        response = gemini_model.generate_content(prompt)\n",
        "        return {\n",
        "            \"answer\": response.text,\n",
        "            \"pages\": [self.chunks[idx]['page'] for idx in I[0]],\n",
        "            \"routing\": p_type,\n",
        "            \"conf\": conf\n",
        "        }\n",
        "\n",
        "rag_system = IntelligentRAG()\n",
        "print(\"‚úÖ Section 4 Complete: RAG Logic Defined.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iqBijPqId3N"
      },
      "source": [
        "# **üìä Section 5: Accuracy Benchmarks (F1-Score Evaluation)**\n",
        "\n",
        "This section allows you to test the RAG system against an \"Answer Key.\" It uses semantic similarity to determine if the AI's answer is factually close to the human-verified answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmaTu-I2IjMn"
      },
      "outputs": [],
      "source": [
        "# --- ACCURACY BENCHMARKS ---\n",
        "# Evaluates the RAG performance against industry-specific ground truths.\n",
        "\n",
        "# 1. Define Industry-Specific Golden Datasets\n",
        "GOLDEN_DATASETS = {\n",
        "    \"Healthcare\": [\n",
        "        {\"question\": \"What is the primary diagnosis?\", \"golden_answer\": \"Diagnosis of Type 2 Diabetes with neuropathy.\"},\n",
        "        {\"question\": \"What are the latest lab results for Glucose?\", \"golden_answer\": \"Fasting glucose was 145 mg/dL.\"}\n",
        "    ],\n",
        "    \"Legal\": [\n",
        "        {\"question\": \"What is the termination notice period?\", \"golden_answer\": \"The agreement requires a 30-day written notice for termination.\"},\n",
        "        {\"question\": \"Who are the parties involved?\", \"golden_answer\": \"Between Acme Corp and John Smith.\"}\n",
        "    ],\n",
        "    \"Real Estate\": [\n",
        "        {\"question\": \"What is the total cash to close?\", \"golden_answer\": \"The borrower needs to provide $12,450.50 at closing.\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "def run_f1_evaluation(sector: str):\n",
        "    \"\"\"\n",
        "    Benchmarks the RAG system by comparing AI responses to Golden Answers.\n",
        "    Uses Cosine Similarity as a proxy for Semantic F1.\n",
        "    \"\"\"\n",
        "    if not rag_system.is_ready:\n",
        "        return pd.DataFrame([{\"Error\": \"Please ingest a document first.\"}])\n",
        "\n",
        "    test_cases = GOLDEN_DATASETS.get(sector, [])\n",
        "    results = []\n",
        "\n",
        "    for case in test_cases:\n",
        "        # Get AI Answer\n",
        "        response = rag_system.ask(case['question'])\n",
        "\n",
        "        # Calculate Semantic Similarity\n",
        "        emb_actual = embedding_model.encode([response['answer']])\n",
        "        emb_gold = embedding_model.encode([case['golden_answer']])\n",
        "        score = util.pytorch_cos_sim(emb_actual, emb_gold).item()\n",
        "\n",
        "        results.append({\n",
        "            \"Question\": case['question'],\n",
        "            \"AI Answer\": response['answer'][:50] + \"...\",\n",
        "            \"Similarity Score\": round(score, 3),\n",
        "            \"Status\": \"‚úÖ PASS\" if score > 0.8 else \"‚ùå FAIL\"\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(\"‚úÖ Section 5 Complete: Benchmarking Logic Ready.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prOcFQaKIrPb"
      },
      "source": [
        "# **‚úÇÔ∏è Section 6: Semantic Document Splitter**\n",
        "\n",
        "When you upload a 50-page \"Closing Pack,\" it contains a deed, a loan application, and an ID. This section uses the LLM to find the \"seams\" between these documents so they can be indexed separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OwgbFFeIw2r"
      },
      "outputs": [],
      "source": [
        "# --- SEMANTIC DOCUMENT SPLITTER ---\n",
        "# Identifies boundaries between different types of documents\n",
        "# inside a single multi-page PDF.\n",
        "\n",
        "@dataclass\n",
        "class LogicalDocument:\n",
        "    doc_id: int\n",
        "    doc_type: str\n",
        "    page_start: int\n",
        "    page_end: int\n",
        "    text: str\n",
        "\n",
        "def identify_document_boundaries(pages: List[PageInfo]) -> List[LogicalDocument]:\n",
        "    \"\"\"\n",
        "    Analyzes the first 200 characters of every page to detect document transitions.\n",
        "    Example: Detecting where a 'Resume' ends and a 'Cover Letter' begins.\n",
        "    \"\"\"\n",
        "    # Create a summary map of the PDF\n",
        "    header_map = \"\\n\".join([f\"Page {p.page_num}: {p.text[:200]}\" for p in pages])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Analyze these page headers and identify distinct document boundaries.\n",
        "    Return ONLY a JSON list of objects:\n",
        "    [{{\"type\": \"Resume\", \"start_page\": 0, \"end_page\": 2}}, {{\"type\": \"Passport\", \"start_page\": 3, \"end_page\": 3}}]\n",
        "\n",
        "    Headers:\n",
        "    {header_map}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = gemini_model.generate_content(prompt)\n",
        "        clean_json = response.text.replace('```json', '').replace('```', '').strip()\n",
        "        boundaries = json.loads(clean_json)\n",
        "\n",
        "        logical_docs = []\n",
        "        for i, b in enumerate(boundaries):\n",
        "            # Extract text for this specific range\n",
        "            doc_text = \"\\n\".join([p.text for p in pages if b['start_page'] <= p.page_num <= b['end_page']])\n",
        "            logical_docs.append(LogicalDocument(\n",
        "                doc_id=i,\n",
        "                doc_type=b['type'],\n",
        "                page_start=b['start_page'],\n",
        "                page_end=b['end_page'],\n",
        "                text=doc_text\n",
        "            ))\n",
        "        return logical_docs\n",
        "    except Exception as e:\n",
        "        print(f\"Splitting error: {e}. Falling back to single document mode.\")\n",
        "        # Fallback: Treat whole PDF as one document\n",
        "        full_text = \"\\n\".join([p.text for p in pages])\n",
        "        return [LogicalDocument(0, \"Full Pack\", 0, len(pages)-1, full_text)]\n",
        "\n",
        "print(\"‚úÖ Section 6 Complete: Semantic Splitter Ready.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gONmciozGcaT"
      },
      "source": [
        "# **üé® Section 7: Modern UI & Chat Export**\n",
        "\n",
        "The final dashboard. It uses custom CSS to mimic the Deepsite Hugging Face Space and includes an Export Chat button."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDTzVGNHHDbH"
      },
      "outputs": [],
      "source": [
        "# --- UI & CHAT ----\n",
        "\n",
        "# --- 1. Helper Function: Metadata Extraction ---\n",
        "def get_file_metadata(filepath):\n",
        "    if not filepath:\n",
        "        return \"‚ö†Ô∏è No file detected. Please upload a PDF.\"\n",
        "\n",
        "    # Get File Size (MB)\n",
        "    file_size_bytes = os.path.getsize(filepath)\n",
        "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
        "\n",
        "    # Get Page Count\n",
        "    try:\n",
        "        reader = PdfReader(filepath)\n",
        "        page_count = len(reader.pages)\n",
        "    except Exception as e:\n",
        "        page_count = \"Unknown (Error reading PDF)\"\n",
        "\n",
        "    filename = os.path.basename(filepath)\n",
        "    return (\n",
        "        f\"üìÇ File: {filename}\\n\"\n",
        "        f\"üìä Size: {file_size_mb:.2f} MB\\n\"\n",
        "        f\"üìÑ Pages: {page_count}\\n\"\n",
        "        f\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\"\n",
        "        f\"‚öôÔ∏è Status: AI Indexing in progress... Please wait.\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# 2. Helper: Final Success Message with Stats ---\n",
        "def get_final_status(filepath):\n",
        "    # Re-extract stats to keep them visible in the final window state\n",
        "    file_size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "    reader = PdfReader(filepath)\n",
        "    page_count = len(reader.pages)\n",
        "\n",
        "    return (\n",
        "        f\"üìÇ File: {os.path.basename(filepath)}\\n\"\n",
        "        f\"üìä Size: {file_size_mb:.2f} MB\\n\"\n",
        "        f\"üìÑ Pages: {page_count}\\n\"\n",
        "        f\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\"\n",
        "        f\"‚úÖ Ready! AI Indexing Complete.\\nYou can now start chatting in the 'Chat Interface' tab.\\n\"\n",
        "        f\"üöÄ System is optimized and ready for queries.\"\n",
        "    )\n",
        "\n",
        "# --- 3. Helper Function: Export Chat to PDF ---\n",
        "def export_chat_pdf(history):\n",
        "    if not history or len(history) <= 1:\n",
        "        return None\n",
        "\n",
        "    # Create a unique temporary file path\n",
        "    temp_dir = tempfile.gettempdir()\n",
        "    filename = os.path.join(temp_dir, f\"Deepsite_Report_{datetime.now().strftime('%H%M%S')}.pdf\")\n",
        "\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "\n",
        "    # Header\n",
        "    pdf.set_font(\"Courier\", \"B\", 16)\n",
        "    pdf.cell(200, 10, txt=\"DEEPSITE DOC INTEL - SESSION REPORT\", ln=True, align=\"C\")\n",
        "    pdf.set_font(\"Courier\", \"\", 10)\n",
        "    pdf.cell(200, 10, txt=f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", ln=True, align=\"C\")\n",
        "    pdf.ln(10)\n",
        "\n",
        "    for msg in history:\n",
        "        content = msg['content']\n",
        "\n",
        "        # 1. Remove HTML tags for PDF (source boxes)\n",
        "        if \"<div\" in content:\n",
        "            content = content.split(\"<div\")[0].strip()\n",
        "\n",
        "        # 2. SANITIZE: Remove Emojis and non-Latin-1 characters to prevent crashes\n",
        "        # This keeps standard text, numbers, and basic punctuation\n",
        "        content = content.encode('latin-1', 'ignore').decode('latin-1')\n",
        "\n",
        "        # 3. Clean up the Markdown labels\n",
        "        clean_content = content.replace(\"**üë§ You:** \", \"\").replace(\"**ü§ñ Chatbot:** \", \"\").strip()\n",
        "\n",
        "        if not clean_content: # Skip if the message was only emojis\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Determine Label\n",
        "        if msg['role'] == \"user\":\n",
        "            pdf.set_font(\"Arial\", \"B\", 11)\n",
        "            pdf.set_text_color(0, 0, 0)\n",
        "            label = \"YOU:\"\n",
        "        else:\n",
        "            pdf.set_font(\"Arial\", \"B\", 11)\n",
        "            pdf.set_text_color(100, 100, 100) # Gray for Chatbot\n",
        "            label = \"CHATBOT:\"\n",
        "\n",
        "        pdf.multi_cell(0, 8, txt=label)\n",
        "        pdf.set_font(\"Arial\", \"\", 11)\n",
        "        pdf.set_text_color(0, 0, 0)\n",
        "        # multi_cell handles line wrapping automatically\n",
        "        pdf.multi_cell(0, 8, txt=content.replace(\"**You:** \", \"\").replace(\"**Chatbot:** \", \"\"))\n",
        "        pdf.ln(5)\n",
        "        pdf.line(10, pdf.get_y(), 200, pdf.get_y())\n",
        "        pdf.ln(5)\n",
        "\n",
        "    filename = f\"deepsite_report_{datetime.now().strftime('%H%M%S')}.pdf\"\n",
        "    pdf.output(filename)\n",
        "    return filename\n",
        "\n",
        "\n",
        "\n",
        "# --- 4. Helper Function: Clear History ---\n",
        "def clear_chat():\n",
        "    # Returns the initial welcome message to reset the Chatbot component\n",
        "    return [{\"role\": \"assistant\", \"content\": \"**ü§ñ Chatbot:** üëã Welcome! History has been cleared. How can I help you now? üöÄ\"}]\n",
        "\n",
        "# --- 5. Helper Function: Chat Wrapper with Explicit Labels ---\n",
        "def chat_ui_wrapper(message, history):\n",
        "    # Retrieve answer from your RAG logic (Assumes rag_system is initialized in previous cells)\n",
        "    try:\n",
        "        res = rag_system.ask(message)\n",
        "        answer = res['answer']\n",
        "        routing = res['routing']\n",
        "        pages = ', '.join(map(str, set(res['pages'])))\n",
        "    except Exception as e:\n",
        "        answer = f\"Error: Ensure Section 4-6 were run correctly. ({str(e)})\"\n",
        "        routing = \"Error\"\n",
        "        pages = \"N/A\"\n",
        "\n",
        "    # Format the Source Attribution \"Trust Signal\"\n",
        "    source_html = f\"<div class='source-box'>üßê <b>Trust Signal:</b> Verified from {routing} (Pages: {pages})</div>\"\n",
        "\n",
        "    # 1. Append User Input with \"You\" label\n",
        "    history.append({\"role\": \"user\", \"content\": f\"**üë§ You:** {message}\"})\n",
        "\n",
        "    # 2. Append AI Response with \"Chatbot\" label\n",
        "    bot_response = f\"**ü§ñ Chatbot:** {answer}\\n\\n{source_html}\"\n",
        "    history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
        "\n",
        "    return \"\", history\n",
        "\n",
        "# --- 6. Custom CSS for \"Deepsite\" Monochrome Aesthetic ---\n",
        "custom_css = \"\"\"\n",
        ".welcome-text { text-align: center; margin-bottom: 25px; }\n",
        ".welcome-text h1 { font-family: 'Courier New', monospace; font-weight: bold; margin-bottom: 0px; }\n",
        ".welcome-text h3 { color: #4b5563; margin-top: 5px; margin-bottom: 2px; }\n",
        ".welcome-text p { color: #6b7280; font-size: 0.95em; margin-top: 5px; }\n",
        ".chatbot-container { border: 2px solid #000 !important; border-radius: 12px !important; background-color: #ffffff !important; }\n",
        ".source-box { background-color: #f9f9f9; border-left: 5px solid #000; padding: 12px; margin-top: 15px; font-size: 0.85em; }\n",
        ".status-window { font-family: 'Courier New', monospace; font-size: 0.9em; background-color: #f3f4f6 !important; border: 1px solid #000 !important; }\n",
        "\"\"\"\n",
        "\n",
        "# --- 7. Building the UI Layout ---\n",
        "with gr.Blocks(theme=gr.themes.Monochrome(), css=custom_css) as demo:\n",
        "\n",
        "    # Header Section\n",
        "    with gr.Column(elem_classes=\"welcome-text\"):\n",
        "        gr.Markdown(\"# AI-Powered Document Intelligence Chatbot\")\n",
        "        gr.Markdown(\"### Providing assistance with document search. ‚ú®\")\n",
        "        gr.Markdown(\"Upload document and enter search request in chatbot\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # TAB 1: Chat interface\n",
        "        with gr.Tab(\"üí¨ Chat Interface\"):\n",
        "            chatbot = gr.Chatbot(\n",
        "                value=[{\"role\": \"assistant\", \"content\": \"**Chatbot:** üëã Welcome! Upload files in the next tab to begin. üöÄ\"}],\n",
        "                height=500,\n",
        "                type=\"messages\",\n",
        "                elem_classes=\"chatbot-container\",\n",
        "                allow_tags=True\n",
        "            )\n",
        "            with gr.Row():\n",
        "                msg_input = gr.Textbox(placeholder=\"Ask a question about your docs...\", scale=7, container=False)\n",
        "                send_btn = gr.Button(\"Send üöÄ\", variant=\"primary\", scale=1)\n",
        "\n",
        "            with gr.Row():\n",
        "                clear_btn = gr.Button(\"üóëÔ∏è Clear Chat\", variant=\"secondary\")\n",
        "                export_btn = gr.Button(\"üìÑ Export PDF Report\")\n",
        "                download_link = gr.File(label=\"üì• Click to Download PDF\", scale=1, interactive=False)\n",
        "\n",
        "        # TAB 2: Upload and Guidance\n",
        "        with gr.Tab(\"üìÇ Upload & Guidance\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### üõ†Ô∏è Pro-Tips for Best Results\n",
        "            For best results, always upload clean, high-quality documents so the system can process them accurately.\n",
        "            Use document type filters to narrow your search and avoid irrelevant results.\n",
        "            If your query is broad, try breaking it into smaller, more specific questions ‚Äî this often leads to more accurate answers.\n",
        "            **And remember, the magic happens when your data is well-prepared and your questions are clear, because even the smartest AI needs good input to give great output.**\n",
        "            \"\"\")\n",
        "\n",
        "            file_in = gr.File(label=\"Upload your document üìÇ\", file_types=[\".pdf\"])\n",
        "            ingest_btn = gr.Button(\"Initialize AI Index ‚ú®\", variant=\"primary\")\n",
        "\n",
        "            log_out = gr.Textbox(\n",
        "                label=\"System Status & Metadata\",\n",
        "                lines=6,\n",
        "                elem_classes=\"status-window\",\n",
        "                placeholder=\"Technical details will appear here after upload...\"\n",
        "            )\n",
        "\n",
        "    # ---8. Event Wiring ---\n",
        "\n",
        "    # Send button & Enter key functionality\n",
        "    send_btn.click(chat_ui_wrapper, [msg_input, chatbot], [msg_input, chatbot])\n",
        "    msg_input.submit(chat_ui_wrapper, [msg_input, chatbot], [msg_input, chatbot])\n",
        "\n",
        "    # Clear Wiring\n",
        "    clear_btn.click(clear_chat, outputs=chatbot)\n",
        "\n",
        "    # PDF Generation Wiring: Button triggers the function, output goes to the File component\n",
        "    export_btn.click(\n",
        "        fn=export_chat_pdf,\n",
        "        inputs=chatbot,\n",
        "        outputs=download_link\n",
        "        )\n",
        "\n",
        "    # Ingest button functionality with Metadata first\n",
        "    # Sequence: 1. Show processing status -> 2. Process -> 3. Show success with stats\n",
        "    ingest_btn.click(\n",
        "        fn=get_file_metadata,\n",
        "        inputs=file_in,\n",
        "        outputs=log_out\n",
        "    ).then(\n",
        "        fn=lambda f: rag_system.ingest(f.name),\n",
        "        inputs=file_in,\n",
        "        outputs=None\n",
        "    ).then(\n",
        "        fn=get_final_status,\n",
        "        inputs=file_in,\n",
        "        outputs=log_out\n",
        "    )\n",
        "\n",
        "# Launch the Deepsite App\n",
        "demo.launch(debug=True, share=True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOnOAn2FH4yI1lnbK8ds+Md",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}